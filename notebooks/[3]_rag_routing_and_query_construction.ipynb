{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4ffa253-e427-4663-bc0b-9b1fcce508e6",
   "metadata": {},
   "source": [
    "# bRAG: Routing and Query Construction\n",
    "\n",
    "![route_query_construction](./image/route_query_construction.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb38512b",
   "metadata": {},
   "source": [
    "## Pre-requisites (optional but recommended)\n",
    "\n",
    "### Only do the first step if you have never created a virtual environment for this repository. Otherwise, make sure that the Python Kernel that you selected is from your `venv/` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1ce5384d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create virtual environment\n",
    "! python3 -m venv ../venv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "11ad4035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activate virtual Python environment\n",
    "! source ../venv/bin/activate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "91b3f035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/taha/Desktop/bRAGAI/code/gh/bRAG-langchain/venv/bin/python\n"
     ]
    }
   ],
   "source": [
    "# If your Python is not from your venv path, ensure that your IDE's kernel selection (on the top right corner) is set to the correct path \n",
    "# (your path output should contain \"...venv/bin/python\")\n",
    "\n",
    "! which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a9a21b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install all packages\n",
    "! pip3 install -r ../requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13282b3d",
   "metadata": {},
   "source": [
    "### * If you choose to skip the pre-requisites and install only the packages specific to this notebook using your global Python path environment, execute the command below; otherwise, proceed to the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5550bb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip3 install --quiet langchain_community tiktoken langchain-openai langchainhub chromadb langchain youtube-transcript-api pytube yt_dlp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6faeceb",
   "metadata": {},
   "source": [
    "## Environment\n",
    "\n",
    "`(1) Packages`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec0f6b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load all environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Access the environment variables\n",
    "langchain_tracing_v2 = os.getenv('LANGCHAIN_TRACING_V2')\n",
    "langchain_endpoint = os.getenv('LANGCHAIN_ENDPOINT')\n",
    "langchain_api_key = os.getenv('LANGCHAIN_API_KEY')\n",
    "\n",
    "## LLM\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9528893c",
   "metadata": {},
   "source": [
    "`(2) LangSmith`\n",
    "\n",
    "https://docs.smith.langchain.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "282076c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['LANGCHAIN_TRACING_V2'] = langchain_tracing_v2\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = langchain_endpoint\n",
    "os.environ['LANGCHAIN_API_KEY'] = langchain_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387d13bd",
   "metadata": {},
   "source": [
    "`(3) API Keys`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4db9a1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['OPENAI_API_KEY'] = openai_api_key\n",
    "openai_model = \"gpt-3.5-turbo\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45fd9558-c53f-4a6c-80f0-c31b3bfd55de",
   "metadata": {},
   "source": [
    "## bRAG: Logical and Semantic routing \n",
    "\n",
    "Use function-calling for classification.\n",
    "\n",
    "Flow: \n",
    "\n",
    "![routing](./image/routing.png)\n",
    "\n",
    "Docs:\n",
    "\n",
    "https://python.langchain.com/docs/how_to/routing/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04c2cf60-d636-4992-a021-1236f7688999",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/taha/Desktop/bRAGAI/code/gh/bRAG-langchain/venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3577: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "/Users/taha/Desktop/bRAGAI/code/gh/bRAG-langchain/venv/lib/python3.11/site-packages/langchain_openai/chat_models/base.py:1354: UserWarning: Received a Pydantic BaseModel V1 schema. This is not supported by method=\"json_schema\". Please use method=\"function_calling\" or specify schema via JSON Schema or Pydantic V2 BaseModel. Overriding to method=\"function_calling\".\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from typing import Literal\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Data model\n",
    "class RouteQuery(BaseModel):\n",
    "    \"\"\"Route a user query to the most relevant datasource.\"\"\"\n",
    "\n",
    "    datasource: Literal[\"python_docs\", \"js_docs\", \"golang_docs\"] = Field(\n",
    "        ...,\n",
    "        description=\"Given a user question choose which datasource would be most relevant for answering their question\",\n",
    "    )\n",
    "\n",
    "# LLM with function call \n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n",
    "structured_llm = llm.with_structured_output(RouteQuery)\n",
    "\n",
    "# Prompt \n",
    "system = \"\"\"You are an expert at routing a user question to the appropriate data source.\n",
    "\n",
    "Based on the programming language the question is referring to, route it to the relevant data source.\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define router \n",
    "router = prompt | structured_llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb5aaa3-7826-471c-9203-390009cbb4d6",
   "metadata": {},
   "source": [
    "Note: we used function calling to produce structured output.\n",
    "\n",
    "![structured_output](./image/structured_output.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cfc6febc-93df-49b4-9920-c93589ba021e",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"\"\"Why doesn't the following code work:\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\"human\", \"speak in {language}\"])\n",
    "prompt.invoke(\"french\")\n",
    "\"\"\"\n",
    "\n",
    "result = router.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "277536df-0904-4d99-92bb-652621afbdec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RouteQuery(datasource='python_docs')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "636a43ae-50f3-43a1-a1b7-93266ea13bcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'python_docs'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.datasource"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba569c12-ba1d-4486-a5be-bc8c31a8448c",
   "metadata": {},
   "source": [
    "Once we have this, it is trivial to define a branch that uses `result.datasource`\n",
    "\n",
    "https://python.langchain.com/docs/how_to/routing/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "01f15722-35c6-4456-ad1b-06463233db25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_route(result):\n",
    "    if \"python_docs\" in result.datasource.lower():\n",
    "        ### Logic here \n",
    "        return \"chain for python_docs\"\n",
    "    elif \"js_docs\" in result.datasource.lower():\n",
    "        ### Logic here \n",
    "        return \"chain for js_docs\"\n",
    "    else:\n",
    "        ### Logic here \n",
    "        return \"golang_docs\"\n",
    "\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "full_chain = router | RunnableLambda(choose_route)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6af07b77-0537-4635-87ec-ad8f59d34e9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'chain for python_docs'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_chain.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0afc28c6-6bb6-4869-8c4e-e81dd28c69fd",
   "metadata": {},
   "source": [
    "Trace:\n",
    "\n",
    "https://smith.langchain.com/public/c2ca61b4-3810-45d0-a156-3d6a73e9ee2a/r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1db843-95f4-4abd-8d4c-4a41f0910949",
   "metadata": {},
   "source": [
    "### Semantic routing\n",
    "\n",
    "Flow:\n",
    "\n",
    "![semantic_routing](./image/semantic_routing.png)\n",
    "\n",
    "Docs:\n",
    "\n",
    "https://python.langchain.com/docs/how_to/routing/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53cbfa72-c35a-4d1d-aa6d-08a570ab2170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PHYSICS\n",
      "A black hole is a region in space where the gravitational pull is so strong that nothing, not even light, can escape from it. It is formed when a massive star collapses in on itself. The center of a black hole is called a singularity, where the mass is concentrated in an infinitely small space. Black holes can be of various sizes, with the supermassive ones found at the centers of galaxies being the largest.\n"
     ]
    }
   ],
   "source": [
    "from langchain.utils.math import cosine_similarity\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "# Two prompts\n",
    "physics_template = \"\"\"You are a very smart physics professor. \\\n",
    "You are great at answering questions about physics in a concise and easy to understand manner. \\\n",
    "When you don't know the answer to a question you admit that you don't know.\n",
    "\n",
    "Here is a question:\n",
    "{query}\"\"\"\n",
    "\n",
    "math_template = \"\"\"You are a very good mathematician. You are great at answering math questions. \\\n",
    "You are so good because you are able to break down hard problems into their component parts, \\\n",
    "answer the component parts, and then put them together to answer the broader question.\n",
    "\n",
    "Here is a question:\n",
    "{query}\"\"\"\n",
    "\n",
    "# Embed prompts\n",
    "embeddings = OpenAIEmbeddings()\n",
    "prompt_templates = [physics_template, math_template]\n",
    "prompt_embeddings = embeddings.embed_documents(prompt_templates)\n",
    "\n",
    "# Route question to prompt \n",
    "def prompt_router(input):\n",
    "    # Embed question\n",
    "    query_embedding = embeddings.embed_query(input[\"query\"])\n",
    "    # Compute similarity\n",
    "    similarity = cosine_similarity([query_embedding], prompt_embeddings)[0]\n",
    "    most_similar = prompt_templates[similarity.argmax()]\n",
    "    # Chosen prompt \n",
    "    print(\"Using MATH\" if most_similar == math_template else \"Using PHYSICS\")\n",
    "    return PromptTemplate.from_template(most_similar)\n",
    "\n",
    "\n",
    "chain = (\n",
    "    {\"query\": RunnablePassthrough()}\n",
    "    | RunnableLambda(prompt_router)\n",
    "    | ChatOpenAI()\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(chain.invoke(\"What's a black hole\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40eb434-97e2-497d-8241-914941594ffe",
   "metadata": {},
   "source": [
    "Trace: \n",
    "\n",
    "https://smith.langchain.com/public/98c25405-2631-4de8-b12a-1891aded3359/r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e93d04a-513e-435b-8e09-8df5d4976690",
   "metadata": {},
   "source": [
    "# bRAG: Query Construction\n",
    "\n",
    "![query_construction](./image/query_construction.png)\n",
    "\n",
    "For graph and SQL, see helpful resources:\n",
    "\n",
    "https://blog.langchain.dev/query-construction/\n",
    "\n",
    "https://blog.langchain.dev/enhancing-rag-based-applications-accuracy-by-constructing-and-leveraging-knowledge-graphs/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93d3516-3f77-4548-b55f-5db4d7c9fbbb",
   "metadata": {},
   "source": [
    "## bRAG: Query structuring for metadata filters\n",
    "\n",
    "Flow:\n",
    "\n",
    "![metadata](./image/metadata.png)\n",
    "\n",
    "Many vectorstores contain metadata fields. \n",
    "\n",
    "This makes it possible to filter for specific chunks based on metadata.\n",
    "\n",
    "Let's look at some example metadata we might see in a database of YouTube transcripts.\n",
    "\n",
    "Docs:\n",
    "\n",
    "https://python.langchain.com/v0.1/docs/use_cases/query_analysis/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b22eb666-a6c2-4b3f-81dd-93ece81f035d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': 'pbAd8O1Lvm4', 'title': 'Self-reflective RAG with LangGraph: Self-RAG and CRAG', 'description': 'Self-reflection can greatly enhance RAG, enabling correction of poor quality retrieval or generations. Several recent RAG papers focus on this theme, but implementing the ideas can be tricky. Here, we show that LangGraph can be easily used for \"flow engineering\" of self-reflective RAG pipelines. We provide cookbooks for implementing ideas from two interesting papers, Self-RAG and C-RAG.\\n\\nCode:\\nhttps://github.com/langchain-ai/langgraph/tree/main/examples/rag', 'uploader': 'LangChain', 'upload_date': '20240207'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import YoutubeLoader\n",
    "import yt_dlp\n",
    "\n",
    "def fetch_video_info(url: str):\n",
    "    \"\"\"Fetch metadata of a YouTube video using yt-dlp.\"\"\"\n",
    "    ydl_opts = {\n",
    "        \"quiet\": True,\n",
    "        \"format\": \"best\",\n",
    "    }\n",
    "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "        info = ydl.extract_info(url, download=False)\n",
    "    return info\n",
    "\n",
    "# Fetch metadata using yt-dlp\n",
    "video_url = \"https://www.youtube.com/watch?v=pbAd8O1Lvm4\"\n",
    "video_info = fetch_video_info(video_url)\n",
    "\n",
    "# Add metadata to the YoutubeLoader\n",
    "loader = YoutubeLoader.from_youtube_url(\n",
    "    video_url, add_video_info=False\n",
    ")\n",
    "\n",
    "# Manually attach metadata from yt-dlp\n",
    "docs = loader.load()\n",
    "for doc in docs:\n",
    "    doc.metadata.update({\n",
    "        \"title\": video_info.get(\"title\", \"Unknown\"),\n",
    "        \"description\": video_info.get(\"description\", \"No description available\"),\n",
    "        \"uploader\": video_info.get(\"uploader\", \"Unknown uploader\"),\n",
    "        \"upload_date\": video_info.get(\"upload_date\", \"Unknown date\"),\n",
    "    })\n",
    "\n",
    "# Print the metadata\n",
    "print(docs[0].metadata)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53767f87-f38d-4f16-9b3b-b83ce4657f03",
   "metadata": {},
   "source": [
    "Let’s assume we’ve built an index that:\n",
    "\n",
    "1. Allows us to perform unstructured search over the `contents` and `title` of each document\n",
    "2. And to use range filtering on `view count`, `publication date`, and `length`.\n",
    "\n",
    "We want to convert natural langugae into structured search queries.\n",
    "\n",
    "We can define a schema for structured search queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7731745b-accc-4cf1-8291-e12d1aa46361",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from typing import Literal, Optional, Tuple\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "class TutorialSearch(BaseModel):\n",
    "    \"\"\"Search over a database of tutorial videos about a software library.\"\"\"\n",
    "\n",
    "    content_search: str = Field(\n",
    "        ...,\n",
    "        description=\"Similarity search query applied to video transcripts.\",\n",
    "    )\n",
    "    title_search: str = Field(\n",
    "        ...,\n",
    "        description=(\n",
    "            \"Alternate version of the content search query to apply to video titles. \"\n",
    "            \"Should be succinct and only include key words that could be in a video \"\n",
    "            \"title.\"\n",
    "        ),\n",
    "    )\n",
    "    min_view_count: Optional[int] = Field(\n",
    "        None,\n",
    "        description=\"Minimum view count filter, inclusive. Only use if explicitly specified.\",\n",
    "    )\n",
    "    max_view_count: Optional[int] = Field(\n",
    "        None,\n",
    "        description=\"Maximum view count filter, exclusive. Only use if explicitly specified.\",\n",
    "    )\n",
    "    earliest_publish_date: Optional[datetime.date] = Field(\n",
    "        None,\n",
    "        description=\"Earliest publish date filter, inclusive. Only use if explicitly specified.\",\n",
    "    )\n",
    "    latest_publish_date: Optional[datetime.date] = Field(\n",
    "        None,\n",
    "        description=\"Latest publish date filter, exclusive. Only use if explicitly specified.\",\n",
    "    )\n",
    "    min_length_sec: Optional[int] = Field(\n",
    "        None,\n",
    "        description=\"Minimum video length in seconds, inclusive. Only use if explicitly specified.\",\n",
    "    )\n",
    "    max_length_sec: Optional[int] = Field(\n",
    "        None,\n",
    "        description=\"Maximum video length in seconds, exclusive. Only use if explicitly specified.\",\n",
    "    )\n",
    "\n",
    "    def pretty_print(self) -> None:\n",
    "        for field in self.__fields__:\n",
    "            if getattr(self, field) is not None and getattr(self, field) != getattr(\n",
    "                self.__fields__[field], \"default\", None\n",
    "            ):\n",
    "                print(f\"{field}: {getattr(self, field)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6054bc98-0ae1-45e6-8f06-22c65ec47180",
   "metadata": {},
   "source": [
    "Now, we prompt the LLM to produce queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f699d9e7-468e-4574-bdba-f4be4a5779de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/taha/Desktop/bRAGAI/code/gh/bRAG-langchain/venv/lib/python3.11/site-packages/langchain_openai/chat_models/base.py:1354: UserWarning: Received a Pydantic BaseModel V1 schema. This is not supported by method=\"json_schema\". Please use method=\"function_calling\" or specify schema via JSON Schema or Pydantic V2 BaseModel. Overriding to method=\"function_calling\".\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "system = \"\"\"You are an expert at converting user questions into database queries. \\\n",
    "You have access to a database of tutorial videos about a software library for building LLM-powered applications. \\\n",
    "Given a question, return a database query optimized to retrieve the most relevant results.\n",
    "\n",
    "If there are acronyms or words you are not familiar with, do not try to rephrase them.\"\"\"\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "structured_llm = llm.with_structured_output(TutorialSearch)\n",
    "query_analyzer = prompt | structured_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1b776858-a589-4fe5-a8a3-19530706075d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content_search: bRAGAI Generative AI Platform\n",
      "title_search: coming soon\n"
     ]
    }
   ],
   "source": [
    "query_analyzer.invoke({\"question\": \"bRAGAI - Generative AI Platform coming soon\"}).pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "65bfad12-1985-433e-a980-eb8c9da53f72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content_search: chat langchain\n",
      "title_search: 2023\n",
      "earliest_publish_date: 2023-01-01\n",
      "latest_publish_date: 2024-01-01\n"
     ]
    }
   ],
   "source": [
    "query_analyzer.invoke(\n",
    "    {\"question\": \"videos on chat langchain published in 2023\"}\n",
    ").pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "99643372-01cc-49cc-a507-bebbed096247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content_search: chat langchain\n",
      "title_search: chat langchain\n",
      "earliest_publish_date: 2024-01-01\n"
     ]
    }
   ],
   "source": [
    "query_analyzer.invoke(\n",
    "    {\"question\": \"videos that are focused on the topic of chat langchain that are published before 2024\"}\n",
    ").pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c26f2329-d091-4a47-995a-822e3f062ea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content_search: multi-modal models agent\n",
      "title_search: multi-modal models agent\n",
      "max_length_sec: 300\n"
     ]
    }
   ],
   "source": [
    "query_analyzer.invoke(\n",
    "    {\n",
    "        \"question\": \"how to use multi-modal models in an agent, only videos under 5 minutes\"\n",
    "    }\n",
    ").pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90111ec1-784f-4b8c-bd91-6122edb7eb25",
   "metadata": {},
   "source": [
    "To then connect this to various vectorstores, you can follow [here](https://python.langchain.com/docs/how_to/self_query)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "This notebook explored two key components of building advanced RAG systems:\n",
    "\n",
    "1. **Routing Strategies**:\n",
    "   - **Logical Routing**: Using function calling to classify and route queries to appropriate data sources\n",
    "   - **Semantic Routing**: Leveraging embeddings and cosine similarity to match queries with relevant prompt templates\n",
    "\n",
    "2. **Query Construction**:\n",
    "   - **Metadata Filtering**: Building structured queries that combine semantic search with metadata filters\n",
    "   - **Schema Definition**: Using Pydantic models to define structured search parameters\n",
    "   - **Query Analysis**: Converting natural language questions into structured database queries\n",
    "\n",
    "These techniques enable:\n",
    "- More precise and relevant document retrieval\n",
    "- Better handling of diverse data sources\n",
    "- Structured filtering based on metadata\n",
    "- Natural language interface for complex queries\n",
    "\n",
    "The combination of intelligent routing and structured query construction forms the foundation for building more sophisticated and accurate RAG systems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
