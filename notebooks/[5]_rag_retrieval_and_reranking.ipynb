{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "402ae2dc-9c06-4de6-a301-39bcfde04153",
      "metadata": {
        "id": "402ae2dc-9c06-4de6-a301-39bcfde04153"
      },
      "source": [
        "# 检索和重排\n",
        "\n",
        "![retrieval and reranking](https://github.com/tivon-x/bRAG-langchain/blob/main/notebooks/image/retrieval_reranking.png?raw=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "75e299e2",
      "metadata": {
        "id": "75e299e2"
      },
      "outputs": [],
      "source": [
        "! pip3 install --quiet langchain_community tiktoken langchain-openai langchainhub chromadb langchain cohere langgraph python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "nWO5tdJ1WfZ3",
      "metadata": {
        "id": "nWO5tdJ1WfZ3"
      },
      "outputs": [],
      "source": [
        "! pip install --upgrade --quiet  dashscope"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "zrHg2LQdab9I",
      "metadata": {
        "id": "zrHg2LQdab9I"
      },
      "outputs": [],
      "source": [
        "! pip install --quiet langchain-chroma"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2364045",
      "metadata": {
        "id": "f2364045"
      },
      "source": [
        "## Environment\n",
        "\n",
        "`(1) Packages`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad05ae49",
      "metadata": {
        "id": "ad05ae49"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# LangSmith\n",
        "langsmith_tracing = os.getenv('LANGSMITH_TRACING')\n",
        "langsmith_endpoint = os.getenv('LANGSMITH_ENDPOINT')\n",
        "langsmith_api_key = os.getenv('LANGSMITH_API_KEY')\n",
        "\n",
        "## LLM\n",
        "dashscope_api_key = os.getenv('DASHSCOPE_API_KEY')\n",
        "\n",
        "## Cohere API\n",
        "cohere_api_key = os.getenv('COHERE_API_KEY')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "Oe6f9VmNZAxm",
      "metadata": {
        "id": "Oe6f9VmNZAxm"
      },
      "outputs": [],
      "source": [
        "# Colab环境\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "langsmith_tracing = userdata.get('LANGSMITH_TRACING')\n",
        "langsmith_endpoint = userdata.get('LANGSMITH_ENDPOINT')\n",
        "langsmith_api_key = userdata.get('LANGSMITH_API_KEY')\n",
        "\n",
        "dashscope_api_key = userdata.get(\"DASHSCOPE_API_KEY\")\n",
        "\n",
        "## Cohere API\n",
        "cohere_api_key = userdata.get('COHERE_API_KEY')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef377957",
      "metadata": {
        "id": "ef377957"
      },
      "source": [
        "`(2) LangSmith`\n",
        "\n",
        "https://docs.smith.langchain.com/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "f213ce96",
      "metadata": {
        "id": "f213ce96"
      },
      "outputs": [],
      "source": [
        "os.environ['LANGSMITH_TRACING'] = langsmith_tracing\n",
        "os.environ['LANGSMITH_ENDPOINT'] = langsmith_endpoint\n",
        "os.environ['LANGSMITH_API_KEY'] = langsmith_api_key"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "744ec5ec",
      "metadata": {
        "id": "744ec5ec"
      },
      "source": [
        "`(3) API Keys`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "024a38cd",
      "metadata": {
        "id": "024a38cd"
      },
      "outputs": [],
      "source": [
        "os.environ['DASHSCOPE_API_KEY'] = dashscope_api_key\n",
        "dashscope_model = \"qwen-plus-latest\"\n",
        "\n",
        "os.environ['COHERE_API_KEY'] = cohere_api_key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "hrJiWNMSZef7",
      "metadata": {
        "id": "hrJiWNMSZef7"
      },
      "outputs": [],
      "source": [
        "# langchain的webbaseloader需要\n",
        "os.environ[\"USER_AGENT\"] = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a0ae5a7f-7c07-4825-b53b-4ac632b8eac6",
      "metadata": {
        "id": "a0ae5a7f-7c07-4825-b53b-4ac632b8eac6"
      },
      "source": [
        "## 重排 Re-ranking\n",
        "\n",
        "之前的RAG-Fusion架构已经执行了重排操作：\n",
        "\n",
        "![reranking](https://github.com/tivon-x/bRAG-langchain/blob/main/notebooks/image/reranking.png?raw=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "f201eec1-6ed0-4236-9594-286894574779",
      "metadata": {
        "id": "f201eec1-6ed0-4236-9594-286894574779"
      },
      "outputs": [],
      "source": [
        "#### INDEXING ####\n",
        "\n",
        "# Load blog\n",
        "import bs4\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "loader = WebBaseLoader(\n",
        "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
        "    bs_kwargs=dict(\n",
        "        parse_only=bs4.SoupStrainer(\n",
        "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
        "        )\n",
        "    ),\n",
        ")\n",
        "blog_docs = loader.load()\n",
        "\n",
        "# Split\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "    chunk_size=300,\n",
        "    chunk_overlap=50)\n",
        "\n",
        "# Make splits\n",
        "splits = text_splitter.split_documents(blog_docs)\n",
        "\n",
        "# Index\n",
        "from langchain_community.embeddings import DashScopeEmbeddings\n",
        "\n",
        "embeddings = DashScopeEmbeddings(model=\"text-embedding-v4\")\n",
        "\n",
        "def batch_read(lst, batch_size=10):\n",
        "    for i in range(0, len(lst), batch_size):\n",
        "        yield lst[i:i + batch_size]\n",
        "\n",
        "# from langchain_cohere import CohereEmbeddings\n",
        "from langchain_chroma import Chroma\n",
        "\n",
        "vectorstore = Chroma(embedding_function=embeddings)\n",
        "\n",
        "for batch in batch_read(splits):\n",
        "  vectorstore.add_documents(batch)\n",
        "\n",
        "retriever = vectorstore.as_retriever()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b733a230-d217-4ee0-8482-ab4380e7be4f",
      "metadata": {
        "id": "b733a230-d217-4ee0-8482-ab4380e7be4f"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "# RAG-Fusion\n",
        "template = \"\"\"You are a helpful assistant that generates multiple search queries based on a single input query. \\n\n",
        "Generate multiple search queries related to: {question} \\n\n",
        "Output (4 queries):\"\"\"\n",
        "prompt_rag_fusion = ChatPromptTemplate.from_template(template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e37bc5d-93fc-4b34-8a4d-208394e89709",
      "metadata": {
        "id": "6e37bc5d-93fc-4b34-8a4d-208394e89709"
      },
      "outputs": [],
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_community.chat_models.tongyi import ChatTongyi\n",
        "\n",
        "llm = ChatTongyi(model=dashscope_model, temperature=0.1)\n",
        "\n",
        "\n",
        "generate_queries = (\n",
        "    prompt_rag_fusion\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        "    | (lambda x: x.split(\"\\n\"))\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "728fedac-7f1d-49aa-9d02-47243c0f2bb4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "728fedac-7f1d-49aa-9d02-47243c0f2bb4",
        "outputId": "dd1fb95a-ea35-4571-87ca-ec9408fc01d0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain.load import dumps, loads\n",
        "\n",
        "def reciprocal_rank_fusion(results: list[list], k=60, n=7):\n",
        "    \"\"\" Reciprocal_rank_fusion that takes multiple lists of ranked documents\n",
        "        and an optional parameter k used in the RRF formula\n",
        "        接受多个排名文档列表、RRF公式中使用的可选参数k、返回的文档数量\n",
        "    \"\"\"\n",
        "\n",
        "    # 初始化字典以保存每个唯一文档的融合分数\n",
        "    fused_scores = {}\n",
        "\n",
        "    # 遍历每个排名文档列表\n",
        "    for docs in results:\n",
        "        # 遍历列表中的每个文档及其排名（列表中的位置）\n",
        "        for rank, doc in enumerate(docs):\n",
        "            # 将文档转换为字符串格式以用作key（假设文档可以序列化为JSON）\n",
        "            doc_str = dumps(doc)\n",
        "            # 如果文档尚未在fused_scores字典中，请将其初始分数添加为0\n",
        "            if doc_str not in fused_scores:\n",
        "                fused_scores[doc_str] = 0\n",
        "            # 检索文档的当前分数（如果有的话）\n",
        "            previous_score = fused_scores[doc_str]\n",
        "            # 使用RRF公式更新文档的分数：1/（rank+k）\n",
        "            fused_scores[doc_str] += 1 / (rank + k)\n",
        "\n",
        "    # 根据fusion分数按降序对文档进行排序，以获得最终的重新排序结果\n",
        "    reranked_results = [\n",
        "        (loads(doc), score)\n",
        "        for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
        "    ]\n",
        "\n",
        "    # 将重新排序的结果作为元组列表返回，每个元组包含文档及其fusion分数\n",
        "    return reranked_results[:n]\n",
        "\n",
        "question = \"What is task decomposition for LLM agents?\"\n",
        "retrieval_chain_rag_fusion = generate_queries | retriever.map() | reciprocal_rank_fusion\n",
        "docs = retrieval_chain_rag_fusion.invoke({\"question\": question})\n",
        "len(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee43bb94-613d-46a3-b688-02c2d2aadfba",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "ee43bb94-613d-46a3-b688-02c2d2aadfba",
        "outputId": "a810a6b3-c991-497f-b8bd-6175abfc25e6"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Task decomposition for LLM (Large Language Model) agents refers to the process of breaking down complex tasks into smaller, more manageable subgoals or steps. This enables efficient handling of intricate problems by transforming them into multiple simpler tasks. \\n\\nKey methods and insights about task decomposition from the context include:\\n\\n1. **Chain of Thought (CoT)**: A prompting technique where the model is instructed to \"think step by step,\" decomposing a hard task into smaller steps to improve performance on complex tasks.\\n\\n2. **Tree of Thoughts (ToT)**: An extension of CoT that explores multiple reasoning possibilities at each step. It generates multiple thoughts per step, creating a tree structure, which can be explored using BFS (breadth-first search) or DFS (depth-first search).\\n\\n3. **Implementation Approaches**:\\n   - Prompting the LLM directly with instructions like \"Steps for XYZ\" or \"What are the subgoals for achieving XYZ?\"\\n   - Using task-specific instructions, such as \"Write a story outline.\"\\n   - Incorporating human inputs for guidance.\\n\\nBy effectively decomposing tasks, LLM-powered autonomous agents can plan ahead, handle multi-step reasoning, and improve their problem-solving capabilities.'"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from operator import itemgetter\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "# RAG\n",
        "template = \"\"\"Answer the following question based on this context:\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "final_rag_chain = (\n",
        "    {\"context\": retrieval_chain_rag_fusion,\n",
        "     \"question\": itemgetter(\"question\")}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "final_rag_chain.invoke({\"question\":question})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a70968aa-52e8-41b9-96f1-7d811351512e",
      "metadata": {
        "id": "a70968aa-52e8-41b9-96f1-7d811351512e"
      },
      "source": [
        "我们也可以使用 [Cohere Re-Rank](https://python.langchain.com/docs/integrations/retrievers/cohere-reranker#doing-reranking-with-coherererank). 进行重排操作\n",
        "\n",
        "[cohere 相关博客](https://txt.cohere.com/rerank/):\n",
        "\n",
        "![cohere-re-rank](https://github.com/tivon-x/bRAG-langchain/blob/main/notebooks/image/cohere-re-rank.png?raw=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9bdd18dd-2e6d-428d-aacb-a539a94064ba",
      "metadata": {
        "id": "9bdd18dd-2e6d-428d-aacb-a539a94064ba"
      },
      "outputs": [],
      "source": [
        "from langchain_community.llms import Cohere\n",
        "from langchain.retrievers import  ContextualCompressionRetriever\n",
        "from langchain.retrievers.document_compressors import CohereRerank"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b88e1ad8-57eb-40d4-8475-a50775a692b0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b88e1ad8-57eb-40d4-8475-a50775a692b0",
        "outputId": "19d80d24-2c30-40a2-f73f-363702c9fc6a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'relevance_score': 0.998844}, page_content='Component One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.'),\n",
              " Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'relevance_score': 0.998844}, page_content='Component One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.'),\n",
              " Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'relevance_score': 0.99293363}, page_content='LLM Powered Autonomous Agents\\n    \\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\\n\\n\\nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview#\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory')]"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 返回前10个\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 10})\n",
        "\n",
        "# Re-rank\n",
        "compressor = CohereRerank(model=\"rerank-english-v3.0\")\n",
        "compression_retriever = ContextualCompressionRetriever(\n",
        "    base_compressor=compressor, base_retriever=retriever\n",
        ")\n",
        "\n",
        "compressed_docs = compression_retriever.invoke(question)\n",
        "compressed_docs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "W16d90fbf2Hc",
      "metadata": {
        "id": "W16d90fbf2Hc"
      },
      "source": [
        "整合到RAG链中"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VJQ5-sjBf00V",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "VJQ5-sjBf00V",
        "outputId": "c654b4c2-3353-40ca-cc26-8caa4bd02c75"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'**Answer:**  \\nTask decomposition for LLM (large language model) agents refers to the process of breaking down complex tasks into smaller, more manageable subgoals or steps. This enables the agent to handle complicated problems more effectively by focusing on solving each individual part sequentially. Techniques like Chain of Thought (CoT) and Tree of Thoughts (ToT) are used to facilitate task decomposition:\\n\\n- **Chain of Thought (CoT)** involves instructing the model to \"think step-by-step,\" decomposing a task into a sequence of intermediate reasoning steps.\\n- **Tree of Thoughts (ToT)** extends CoT by exploring multiple possible reasoning paths at each step, organizing them in a tree structure, and using search strategies like BFS or DFS to find the optimal solution.\\n\\nTask decomposition can be performed:\\n1. By the LLM itself through simple prompting (e.g., “Steps for XYZ.”),\\n2. Through task-specific instructions (e.g., “Write a story outline.”),\\n3. With assistance from human inputs.\\n\\nThis planning component is essential for enabling autonomous LLM-powered agents to tackle complex problems efficiently.'"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Post-processing\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "cohere_rag_chain = (\n",
        "    {\"context\": compression_retriever | format_docs,\n",
        "     \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "cohere_rag_chain.invoke(question)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "97b2c3ff-60df-4a68-9516-f4577ed320a4",
      "metadata": {
        "id": "97b2c3ff-60df-4a68-9516-f4577ed320a4"
      },
      "source": [
        "## Corrective RAG (CRAG)\n",
        "\n",
        "CorrectiveRAG（CRAG）是RAG的一种策略，包括对检索到的文档进行自我反思/自我评分。\n",
        "\n",
        "在[论文](https://arxiv.org/pdf/2401.15884.pdf)中，采取了几个步骤：\n",
        "\n",
        "- 如果至少有一个文档超过了相关性阈值，则继续生成\n",
        "- 在生成之前，它执行知识精炼\n",
        "- 这将文档划分为“knowledge strips”\n",
        "- 它对每个 knowledge strip 进行分级，并过滤掉不相关的条带\n",
        "- 如果所有文档都低于相关性阈值，或者评分者不确定，那么框架会寻找额外的数据源\n",
        "- 它将使用网络搜索来补充检索\n",
        "\n",
        "动机：\n",
        "- self-reflection可以增强RAG\n",
        "- Self-reflection的想法是：基于检索得到的文档与问题的相关性、生成内容相对于问题的质量或者生成内容相对于检索得到的文档的质量，进行一些推理、反馈和重试各种步骤。\n",
        "\n",
        "我们将使用LangGraph从头开始实现其中的一些想法：\n",
        "\n",
        "- 让我们跳过知识精炼阶段作为第一步。如果需要，可以将其作为节点添加回来。\n",
        "- 如果有任何文档是不相关的，让我们选择用网络搜索来补充检索。\n",
        "- 我们将使用Tavily Search进行网络搜索。\n",
        "- 让我们使用查询重写来优化网络搜索的查询。\n",
        "\n",
        "![CRAG](https://github.com/tivon-x/bRAG-langchain/blob/main/notebooks/image/crag.png?raw=1)\n",
        "\n",
        "`视频教程`\n",
        "\n",
        "https://www.youtube.com/watch?v=E2shqsYwxck\n",
        "\n",
        "`Notebooks`\n",
        "\n",
        "https://github.com/langchain-ai/langgraph/blob/main/examples/rag/langgraph_crag.ipynb\n",
        "\n",
        "https://github.com/langchain-ai/langgraph/blob/main/examples/rag/langgraph_crag_local.ipynb\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cab810fe",
      "metadata": {
        "id": "cab810fe"
      },
      "source": [
        "我们先创建文档评分的链，用于对检索到的文档进行评分。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5_dTQ8fNyTcx",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5_dTQ8fNyTcx",
        "outputId": "c9dbdfeb-98aa-47d2-fe14-3c064afa6645"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "binary_score='yes'\n"
          ]
        }
      ],
      "source": [
        "from pydantic import BaseModel, Field\n",
        "\n",
        "# Data model\n",
        "class GradeDocuments(BaseModel):\n",
        "    \"\"\"Binary score for relevance check on retrieved documents.\"\"\"\n",
        "\n",
        "    binary_score: str = Field(\n",
        "        description=\"Documents are relevant to the question, 'yes' or 'no'\"\n",
        "    )\n",
        "\n",
        "structured_llm_grader = llm.with_structured_output(GradeDocuments)\n",
        "\n",
        "# Prompt\n",
        "system = \"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n\n",
        "    If the document contains keyword(s) or semantic meaning related to the question, grade it as relevant. \\n\n",
        "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\"\n",
        "grade_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system),\n",
        "        (\"human\", \"Retrieved document: \\n\\n {document} \\n\\n User question: {question}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "retrieval_grader = grade_prompt | structured_llm_grader\n",
        "question = \"agent memory\"\n",
        "docs = retriever.get_relevant_documents(question)\n",
        "doc_txt = docs[1].page_content\n",
        "print(retrieval_grader.invoke({\"question\": question, \"document\": doc_txt}))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "957de80f",
      "metadata": {
        "id": "957de80f"
      },
      "source": [
        "这里定义一个rag的链"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "g4o5bLZa2qIH",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g4o5bLZa2qIH",
        "outputId": "13fcdb3a-4060-4016-f38c-8175913dd76a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "In the context of LLM-powered autonomous agents, memory is categorized into short-term and long-term memory. Short-term memory involves in-context learning, constrained by the model's context window, while long-term memory utilizes external vector stores for retaining and retrieving vast information over time. These components enable agents to effectively manage knowledge and improve decision-making through reflection and refinement.\n"
          ]
        }
      ],
      "source": [
        "### Generate\n",
        "\n",
        "from langchain import hub\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# Prompt\n",
        "prompt = hub.pull(\"rlm/rag-prompt\")\n",
        "\n",
        "\n",
        "# Post-processing\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "\n",
        "# Chain\n",
        "rag_chain = prompt | llm | StrOutputParser()\n",
        "\n",
        "# Run\n",
        "generation = rag_chain.invoke({\"context\": docs, \"question\": question})\n",
        "print(generation)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "abe7e003",
      "metadata": {
        "id": "abe7e003"
      },
      "source": [
        "接下来是问题改写的链，用于改写原始问题，以便更好地进行网络搜索。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vOOZI5-g8vf4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "vOOZI5-g8vf4",
        "outputId": "a9f79c7f-5f56-48f1-fda6-dd4bd8aeba4f"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Sure! Here\\'s an improved version of the question optimized for web search:\\n\\n**\"What is agent memory in artificial intelligence, and how does it function in AI systems?\"**\\n\\nThis version clarifies the topic and specifies the intent, making it easier to find relevant and detailed information.'"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Prompt\n",
        "system = \"\"\"You a question re-writer that converts an input question to a better version that is optimized \\n\n",
        "     for web search. Look at the input and try to reason about the underlying semantic intent / meaning.\"\"\"\n",
        "re_write_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system),\n",
        "        (\n",
        "            \"human\",\n",
        "            \"Here is the initial question: \\n\\n {question} \\n Formulate an improved question.\",\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "\n",
        "question_rewriter = re_write_prompt | llm | StrOutputParser()\n",
        "question_rewriter.invoke({\"question\": question})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MWFNDrwu2vtT",
      "metadata": {
        "id": "MWFNDrwu2vtT"
      },
      "outputs": [],
      "source": [
        "%pip install -U langchain-tavily"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VLodpbWw4qUT",
      "metadata": {
        "id": "VLodpbWw4qUT"
      },
      "outputs": [],
      "source": [
        "os.environ[\"TAVILY_API_KEY\"] = userdata.get(\"TAVILY_API_KEY\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6NZRxBpj3qEK",
      "metadata": {
        "id": "6NZRxBpj3qEK"
      },
      "outputs": [],
      "source": [
        "from langchain_tavily import TavilySearch\n",
        "\n",
        "tool = TavilySearch(max_results=3)\n",
        "result = tool.invoke({\"query\": \"What's a 'node' in LangGraph?\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4lEBj7zNCiw7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4lEBj7zNCiw7",
        "outputId": "f50df1ea-e45f-4ec1-d396-16f40d7bca11"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'title': 'What is LangGraph? - GeeksforGeeks',\n",
              " 'url': 'https://www.geeksforgeeks.org/machine-learning/what-is-langgraph/',\n",
              " 'content': 'LangGraph is a Python library that helps you build applications like chatbots or AI agents by organizing their logic step-by-step using state machine model. This step configures your Gemini API key and then we create a simple function `ask_gemini` that takes user input, sends it to the Gemini model and returns the AI-generated response. Creates a state structure with three fields: `question`, `classification` and `response` which flows through the LangGraph. import matplotlib.pyplot as plt from langgraph.graph import StateGraph\\u200bbuilder = StateGraph(GraphState)builder.add_node(\"classify\", classify)builder.add_node(\"respond\", respond)builder.set_entry_point(\"classify\")builder.add_edge(\"classify\", \"respond\")builder.set_finish_point(\"respond\")app = builder.compile()\\u200bdef visualize_workflow(builder): G = nx.DiGraph()\\u200b for node in builder.nodes: G.add_node(node) for edge in builder.edges: G.add_edge(edge[0], edge[1])\\u200b pos = nx.spring_layout(G) nx.draw(G, pos, with_labels=True, node_size=3000, node_color=\"skyblue\", font_size=12, font_weight=\"bold\", arrows=True)  plt.title(\"Langchain Workflow Visualization\") plt.show()\\u200bvisualize_workflow(builder)',\n",
              " 'score': 0.69249696,\n",
              " 'raw_content': None}"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result[\"results\"][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bhHRp25N7AhI",
      "metadata": {
        "id": "bhHRp25N7AhI"
      },
      "outputs": [],
      "source": [
        "from typing_extensions import TypedDict\n",
        "from langchain.schema import Document\n",
        "\n",
        "\n",
        "class GraphState(TypedDict):\n",
        "    \"\"\"\n",
        "    图的状态，用于存储问题、生成的答案、是否需要网络搜索以及相关文档列表。\n",
        "\n",
        "    Attributes:\n",
        "        question: question\n",
        "        generation: LLM generation\n",
        "        web_search: whether to add search\n",
        "        documents: list of documents\n",
        "    \"\"\"\n",
        "\n",
        "    question: str\n",
        "    generation: str\n",
        "    web_search: str\n",
        "    documents: list[Document]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QDjhLGjS7GP5",
      "metadata": {
        "id": "QDjhLGjS7GP5"
      },
      "outputs": [],
      "source": [
        "def retrieve(state):\n",
        "    \"\"\"\n",
        "    检索相关文档\n",
        "\n",
        "    Args:\n",
        "        state (dict): 当前图的状态\n",
        "\n",
        "    Returns:\n",
        "        state (dict): 更新状态，更新documents和question键，包含检索到的文档和问题\n",
        "    \"\"\"\n",
        "    print(\"---RETRIEVE---\")\n",
        "    question = state[\"question\"]\n",
        "\n",
        "    # Retrieval\n",
        "    documents = retriever.get_relevant_documents(question)\n",
        "    return {\"documents\": documents, \"question\": question}\n",
        "\n",
        "\n",
        "def generate(state):\n",
        "    \"\"\"\n",
        "    生成答案\n",
        "\n",
        "    Args:\n",
        "        state (dict): 当前图的状态\n",
        "\n",
        "    Returns:\n",
        "        state (dict): 更新状态，包含生成的答案和相关文档\n",
        "    \"\"\"\n",
        "    print(\"---GENERATE---\")\n",
        "    question = state[\"question\"]\n",
        "    documents = state[\"documents\"]\n",
        "\n",
        "    # RAG generation\n",
        "    generation = rag_chain.invoke({\"context\": documents, \"question\": question})\n",
        "    return {\"documents\": documents, \"question\": question, \"generation\": generation}\n",
        "\n",
        "\n",
        "def grade_documents(state):\n",
        "    \"\"\"\n",
        "    判断检索到的文档是否与问题相关。\n",
        "\n",
        "    Args:\n",
        "        state (dict): 当前图的状态\n",
        "\n",
        "    Returns:\n",
        "        state (dict): 更新状态，包含过滤后的文档列表、问题和是否需要网络搜索的标志\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n",
        "    question = state[\"question\"]\n",
        "    documents = state[\"documents\"]\n",
        "\n",
        "    # Score each doc\n",
        "    filtered_docs = []\n",
        "    web_search = \"No\"\n",
        "    for d in documents:\n",
        "        score = retrieval_grader.invoke(\n",
        "            {\"question\": question, \"document\": d.page_content}\n",
        "        )\n",
        "        grade = score.binary_score\n",
        "        if grade == \"yes\":\n",
        "            print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
        "            filtered_docs.append(d)\n",
        "        else:\n",
        "            print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
        "            web_search = \"Yes\"\n",
        "            continue\n",
        "    return {\"documents\": filtered_docs, \"question\": question, \"web_search\": web_search}\n",
        "\n",
        "\n",
        "def transform_query(state):\n",
        "    \"\"\"\n",
        "    改写问题以优化网络搜索。\n",
        "\n",
        "    Args:\n",
        "        state (dict): 当前图的状态\n",
        "\n",
        "    Returns:\n",
        "        state (dict): 更新状态，包含改写后的问题和相关文档列表\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"---TRANSFORM QUERY---\")\n",
        "    question = state[\"question\"]\n",
        "    documents = state[\"documents\"]\n",
        "\n",
        "    # Re-write question\n",
        "    better_question = question_rewriter.invoke({\"question\": question})\n",
        "    print(better_question)\n",
        "    return {\"documents\": documents, \"question\": better_question}\n",
        "\n",
        "\n",
        "def web_search(state):\n",
        "    \"\"\"\n",
        "    基于改写后的问题进行网络搜索，并将结果添加到文档列表中。\n",
        "\n",
        "    Args:\n",
        "        state (dict): 当前图的状态\n",
        "\n",
        "    Returns:\n",
        "        state (dict): 更新状态，包含网络搜索结果和相关文档列表\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"---WEB SEARCH---\")\n",
        "    question = state[\"question\"]\n",
        "    documents = state[\"documents\"]\n",
        "\n",
        "    # Web search\n",
        "    tool_result = tool.invoke({\"query\": question})\n",
        "    print(tool_result)\n",
        "    web_results = \"\\n\".join([d[\"content\"] for d in tool_result[\"results\"]])\n",
        "    web_results = Document(page_content=web_results)\n",
        "    documents.append(web_results)\n",
        "\n",
        "    return {\"documents\": documents, \"question\": question}\n",
        "\n",
        "\n",
        "### Edges\n",
        "\n",
        "\n",
        "def decide_to_generate(state):\n",
        "    \"\"\"\n",
        "    判断是否生成答案或重新生成问题。\n",
        "\n",
        "    Args:\n",
        "        state (dict): 当前图的状态，包含问题、网络搜索标志和相关文档列表\n",
        "\n",
        "    Returns:\n",
        "        str: 决定是生成答案还是改写问题的操作名称\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"---ASSESS GRADED DOCUMENTS---\")\n",
        "    state[\"question\"]\n",
        "    web_search = state[\"web_search\"]\n",
        "    state[\"documents\"]\n",
        "\n",
        "    if web_search == \"Yes\":\n",
        "        # All documents have been filtered check_relevance\n",
        "        # We will re-generate a new query\n",
        "        print(\n",
        "            \"---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY---\"\n",
        "        )\n",
        "        return \"transform_query\"\n",
        "    else:\n",
        "        # We have relevant documents, so generate answer\n",
        "        print(\"---DECISION: GENERATE---\")\n",
        "        return \"generate\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MlBBWzpX8Ns3",
      "metadata": {
        "id": "MlBBWzpX8Ns3"
      },
      "outputs": [],
      "source": [
        "from langgraph.graph import END, StateGraph, START\n",
        "\n",
        "workflow = StateGraph(GraphState)\n",
        "\n",
        "# 定义工作流节点\n",
        "workflow.add_node(\"retrieve\", retrieve)  # retrieve\n",
        "workflow.add_node(\"grade_documents\", grade_documents)  # grade documents\n",
        "workflow.add_node(\"generate\", generate)  # generate\n",
        "workflow.add_node(\"transform_query\", transform_query)  # transform_query\n",
        "workflow.add_node(\"web_search_node\", web_search)  # web search\n",
        "\n",
        "# 构建工作流\n",
        "workflow.add_edge(START, \"retrieve\")\n",
        "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
        "workflow.add_conditional_edges(\n",
        "    \"grade_documents\",\n",
        "    decide_to_generate,\n",
        "    {\n",
        "        \"transform_query\": \"transform_query\",\n",
        "        \"generate\": \"generate\",\n",
        "    },\n",
        ")\n",
        "workflow.add_edge(\"transform_query\", \"web_search_node\")\n",
        "workflow.add_edge(\"web_search_node\", \"generate\")\n",
        "workflow.add_edge(\"generate\", END)\n",
        "\n",
        "# Compile\n",
        "app = workflow.compile()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "oJwBHXL28RPZ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oJwBHXL28RPZ",
        "outputId": "bde8917f-3dcb-4c35-e1a7-f1690b1f95c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---RETRIEVE---\n",
            "\"Node 'retrieve':\"\n",
            "'\\n---\\n'\n",
            "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
            "---GRADE: DOCUMENT RELEVANT---\n",
            "---GRADE: DOCUMENT RELEVANT---\n",
            "---GRADE: DOCUMENT RELEVANT---\n",
            "---GRADE: DOCUMENT RELEVANT---\n",
            "---GRADE: DOCUMENT RELEVANT---\n",
            "---GRADE: DOCUMENT RELEVANT---\n",
            "---GRADE: DOCUMENT RELEVANT---\n",
            "---GRADE: DOCUMENT RELEVANT---\n",
            "---GRADE: DOCUMENT RELEVANT---\n",
            "---GRADE: DOCUMENT RELEVANT---\n",
            "---ASSESS GRADED DOCUMENTS---\n",
            "---DECISION: GENERATE---\n",
            "\"Node 'grade_documents':\"\n",
            "'\\n---\\n'\n",
            "---GENERATE---\n",
            "\"Node 'generate':\"\n",
            "'\\n---\\n'\n",
            "('The types of agent memory include Sensory Memory, Short-Term Memory (STM) or '\n",
            " 'Working Memory, and Long-Term Memory (LTM). Sensory memory retains '\n",
            " 'impressions of sensory information briefly, STM holds a limited amount of '\n",
            " 'information actively for a short period, and LTM stores vast amounts of '\n",
            " 'information for long durations. LTM is further divided into '\n",
            " 'Explicit/Declarative (episodic and semantic) and Implicit/Procedural memory.')\n"
          ]
        }
      ],
      "source": [
        "from pprint import pprint\n",
        "\n",
        "# 询问与文档相关的问题\n",
        "inputs = {\"question\": \"What are the types of agent memory?\"}\n",
        "for output in app.stream(inputs):\n",
        "    for key, value in output.items():\n",
        "        # Node\n",
        "        pprint(f\"Node '{key}':\")\n",
        "        # Optional: print full state at each node\n",
        "        # pprint.pprint(value[\"keys\"], indent=2, width=80, depth=None)\n",
        "    pprint(\"\\n---\\n\")\n",
        "\n",
        "# Final generation\n",
        "pprint(value[\"generation\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wgky8_2C8hdh",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wgky8_2C8hdh",
        "outputId": "c12e2389-3ab1-4c2e-c491-66483b8b357d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---RETRIEVE---\n",
            "\"Node 'retrieve':\"\n",
            "'\\n---\\n'\n",
            "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
            "---GRADE: DOCUMENT NOT RELEVANT---\n",
            "---GRADE: DOCUMENT NOT RELEVANT---\n",
            "---GRADE: DOCUMENT NOT RELEVANT---\n",
            "---GRADE: DOCUMENT NOT RELEVANT---\n",
            "---GRADE: DOCUMENT NOT RELEVANT---\n",
            "---GRADE: DOCUMENT NOT RELEVANT---\n",
            "---GRADE: DOCUMENT NOT RELEVANT---\n",
            "---GRADE: DOCUMENT NOT RELEVANT---\n",
            "---GRADE: DOCUMENT NOT RELEVANT---\n",
            "---GRADE: DOCUMENT NOT RELEVANT---\n",
            "---ASSESS GRADED DOCUMENTS---\n",
            "---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY---\n",
            "\"Node 'grade_documents':\"\n",
            "'\\n---\\n'\n",
            "---TRANSFORM QUERY---\n",
            "\"Node 'transform_query':\"\n",
            "'\\n---\\n'\n",
            "---WEB SEARCH---\n",
            "{'query': 'Improved question:  \\n**What is the methodology and key contribution of the AlphaCodium paper?**\\n\\nAlternate version for broader search results:  \\n**How does the AlphaCodium approach function, and what are its main innovations?**', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'title': 'Code Generation with AlphaCodium: From Prompt Engineering to Flow ...', 'url': 'https://arxiv.org/abs/2401.08500', 'content': 'Donate > cs > arXiv:2401.08500 Help | Advanced Search Search GO quick links Login Help Pages About Computer Science > Machine Learning arXiv:2401.08500 (cs) [Submitted on 16 Jan 2024] Title:Code Generation with AlphaCodium: From Prompt Engineering to Flow Engineering Authors:Tal Ridnik, Dedy Kredo, Itamar Friedman View a PDF of the paper titled Code Generation with AlphaCodium: From Prompt Engineering to Flow Engineering, by Tal Ridnik and 2 other authors View PDF Abstract:Code generation problems differ from common natural language problems - they require matching the exact syntax of the target language, identifying happy paths and edge cases, paying attention to numerous small details in the problem spec, and addressing other code-specific issues and requirements. Hence, many of the optimizations and tricks that have been successful in natural language generation may not be effective for code tasks. In this work, we propose a new approach to code generation by LLMs, which we call AlphaCodium - a test-based, multi-stage, code-oriented iterative flow, that improves the performances of LLMs on code problems. The proposed flow consistently and significantly improves results. Many of the principles and best practices acquired in this work, we believe, are broadly applicable to general code generation tasks.', 'score': 0.35595453, 'raw_content': None}, {'title': 'AlphaCodium/README.md at main · Codium-ai/AlphaCodium - GitHub', 'url': 'https://github.com/Codium-ai/AlphaCodium/blob/main/README.md', 'content': 'Official implementation for the paper: \"Code Generation with AlphaCodium: From Prompt Engineering to Flow Engineering\"\" - AlphaCodium/README.md at main · Codium-ai/AlphaCodium', 'score': 0.25838256, 'raw_content': None}, {'title': 'GitHub - Codium-ai/AlphaCodium: Official implementation for the paper ...', 'url': 'https://github.com/Codium-ai/AlphaCodium', 'content': 'The split_name can be either valid or test.; database_solution_path is the path to the directory where the solutions will be saved.; The dataset section in the configuration file contains the configuration for the running and evaluation of a dataset.; Note that this is a long process, and it may take a few days to complete with large models (e.g. GPT-4) and several iterations per problem.', 'score': 0.2493517, 'raw_content': None}], 'response_time': 2.24}\n",
            "\"Node 'web_search_node':\"\n",
            "'\\n---\\n'\n",
            "---GENERATE---\n",
            "\"Node 'generate':\"\n",
            "'\\n---\\n'\n",
            "('AlphaCodium introduces a test-based, multi-stage, code-oriented iterative '\n",
            " 'flow to improve code generation by LLMs. Its methodology involves '\n",
            " 'iteratively refining code through stages like problem understanding, '\n",
            " 'test-case generation, and solution synthesis. The key contribution is '\n",
            " 'shifting from prompt engineering to flow engineering, offering a structured '\n",
            " 'approach that enhances performance on code-specific challenges.')\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 询问与文档不相关的问题\n",
        "inputs = {\"question\": \"How does the AlphaCodium paper work?\"}\n",
        "for output in app.stream(inputs):\n",
        "    for key, value in output.items():\n",
        "        # Node\n",
        "        pprint(f\"Node '{key}':\")\n",
        "        # Optional: print full state at each node\n",
        "        # pprint.pprint(value[\"keys\"], indent=2, width=80, depth=None)\n",
        "    pprint(\"\\n---\\n\")\n",
        "\n",
        "# Final generation\n",
        "pprint(value[\"generation\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d5bfc7f-5f21-43a3-a149-14256860d0f1",
      "metadata": {
        "id": "0d5bfc7f-5f21-43a3-a149-14256860d0f1"
      },
      "source": [
        "# Generation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "85kBjHh6TOX3",
      "metadata": {
        "id": "85kBjHh6TOX3"
      },
      "source": [
        "## Self-RAG\n",
        "自我反思式检索增强生成（Self-RAG）是一种在检索到的文档和生成内容中加入自我反思/自我评估的 RAG 策略。\n",
        "\n",
        "在[论文](https://arxiv.org/abs/2310.11511)中，做出了以下几点决策：\n",
        "\n",
        "1. 是否应从检索器`R`中检索内容\n",
        "  - 输入为问题`x`，或问题`x`和生成内容`y`。\n",
        "  - 判断是否从`R`中检索分块`D`（可能有多个）\n",
        "  - 输出为“是”、“否”或“继续”。\n",
        "\n",
        "2. 检索到的分块`D`是否与问题`x`相关。\n",
        "\n",
        "  - 输入为问题`x`和分块`d`，\n",
        "  - 判断分块是否有助于解决问题`x`\n",
        "  - 输出为“相关”或“不相关”。\n",
        "3. 来自分块`D`的 LLM 生成内容是否与段落相关（如是否存在幻觉等）。\n",
        "  - 输入为问题`x`、分块`D`和生成内容`y`，\n",
        "  - 判断生成的内容是否得到分块`D`的支持\n",
        "  - 输出为“完全支持”、“部分支持”或“无支持”。\n",
        "4. 来自分块`D`的 LLM 生成内容是否对问题`x`有用。\n",
        "  - 输入为问题`x`和生成内容`y`，\n",
        "  - 判断`y`对于`x`是否有用\n",
        "  - 输出为 1 到 5 分\n",
        "\n",
        "![self-rag](./image/self-rag.png)\n",
        "\n",
        "\n",
        "`Notebooks`\n",
        "\n",
        "https://github.com/langchain-ai/langgraph/blob/main/examples/rag/langgraph_self_rag.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "JpUZoVr7U6f2",
      "metadata": {
        "id": "JpUZoVr7U6f2"
      },
      "source": [
        "#### 检索器\n",
        "索引三个文档"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "kWmTTR0AVAeC",
      "metadata": {
        "id": "kWmTTR0AVAeC"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_community.embeddings import DashScopeEmbeddings\n",
        "\n",
        "embeddings = DashScopeEmbeddings(model=\"text-embedding-v4\")\n",
        "\n",
        "urls = [\n",
        "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
        "    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
        "    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
        "]\n",
        "\n",
        "docs = [WebBaseLoader(url).load() for url in urls]\n",
        "docs_list = [item for sublist in docs for item in sublist]\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "    chunk_size=250, chunk_overlap=0\n",
        ")\n",
        "doc_splits = text_splitter.split_documents(docs_list)\n",
        "\n",
        "def batch_read(lst, batch_size=10):\n",
        "    for i in range(0, len(lst), batch_size):\n",
        "        yield lst[i:i + batch_size]\n",
        "\n",
        "vectorstore = Chroma(embedding_function=embeddings)\n",
        "for batch in batch_read(doc_splits):\n",
        "  vectorstore.add_documents(batch)\n",
        "\n",
        "retriever = vectorstore.as_retriever()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "fGLPneczVWZS",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fGLPneczVWZS",
        "outputId": "547e40ce-6cae-464f-e29f-4a8c2b9cc1e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "binary_score='yes'\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from pydantic import BaseModel, Field\n",
        "from langchain_community.chat_models.tongyi import ChatTongyi\n",
        "\n",
        "llm = ChatTongyi(model=dashscope_model, temperature=0)\n",
        "\n",
        "# Data model\n",
        "class GradeDocuments(BaseModel):\n",
        "    \"\"\"Binary score for relevance check on retrieved documents.\"\"\"\n",
        "\n",
        "    binary_score: str = Field(\n",
        "        description=\"Documents are relevant to the question, 'yes' or 'no'\"\n",
        "    )\n",
        "\n",
        "structured_llm_grader = llm.with_structured_output(GradeDocuments)\n",
        "\n",
        "# Prompt\n",
        "system = \"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n\n",
        "    It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n",
        "    If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\n",
        "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\"\n",
        "grade_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system),\n",
        "        (\"human\", \"Retrieved document: \\n\\n {document} \\n\\n User question: {question}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "retrieval_grader = grade_prompt | structured_llm_grader\n",
        "question = \"agent memory\"\n",
        "docs = retriever.invoke(question)\n",
        "doc_txt = docs[1].page_content\n",
        "print(retrieval_grader.invoke({\"question\": question, \"document\": doc_txt}))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "S2uT6jCuV1RU",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S2uT6jCuV1RU",
        "outputId": "ab3c12a8-470a-47ca-a452-744b55b9b953"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "In LLM-powered autonomous agents, memory is divided into short-term and long-term components. Short-term memory involves in-context learning, constrained by the model's context window, while long-term memory uses external vector stores for retaining and retrieving large amounts of information over time. These memory systems enable agents to handle complex tasks by leveraging both immediate and historical data efficiently.\n"
          ]
        }
      ],
      "source": [
        "### Generate\n",
        "\n",
        "from langchain import hub\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# Prompt\n",
        "prompt = hub.pull(\"rlm/rag-prompt\")\n",
        "\n",
        "# Post-processing\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "\n",
        "# Chain\n",
        "rag_chain = prompt | llm | StrOutputParser()\n",
        "\n",
        "# Run\n",
        "generation = rag_chain.invoke({\"context\": docs, \"question\": question})\n",
        "print(generation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "PKrv5IjjV8KG",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PKrv5IjjV8KG",
        "outputId": "3696fba2-404f-4150-a82f-e6fa51c951b6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "GradeHallucinations(binary_score='yes')"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "### Hallucination Grader\n",
        "\n",
        "\n",
        "# Data model\n",
        "class GradeHallucinations(BaseModel):\n",
        "    \"\"\"Binary score for hallucination present in generation answer.\"\"\"\n",
        "\n",
        "    binary_score: str = Field(\n",
        "        description=\"Answer is grounded in the facts, 'yes' or 'no'\"\n",
        "    )\n",
        "\n",
        "\n",
        "# LLM with function call\n",
        "structured_llm_grader = llm.with_structured_output(GradeHallucinations)\n",
        "\n",
        "# Prompt\n",
        "system = \"\"\"You are a grader assessing whether an LLM generation is grounded in / supported by a set of retrieved facts. \\n\n",
        "     Give a binary score 'yes' or 'no'. 'Yes' means that the answer is grounded in / supported by the set of facts.\"\"\"\n",
        "hallucination_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system),\n",
        "        (\"human\", \"Set of facts: \\n\\n {documents} \\n\\n LLM generation: {generation}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "hallucination_grader = hallucination_prompt | structured_llm_grader\n",
        "hallucination_grader.invoke({\"documents\": docs, \"generation\": generation})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "itXryt0YWA91",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "itXryt0YWA91",
        "outputId": "5d9e9a79-6536-4c8c-f912-5181ba324ed7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "GradeAnswer(binary_score='yes')"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "### Answer Grader\n",
        "\n",
        "\n",
        "# Data model\n",
        "class GradeAnswer(BaseModel):\n",
        "    \"\"\"Binary score to assess answer addresses question.\"\"\"\n",
        "\n",
        "    binary_score: str = Field(\n",
        "        description=\"Answer addresses the question, 'yes' or 'no'\"\n",
        "    )\n",
        "\n",
        "\n",
        "# LLM with function call\n",
        "structured_llm_grader = llm.with_structured_output(GradeAnswer)\n",
        "\n",
        "# Prompt\n",
        "system = \"\"\"You are a grader assessing whether an answer addresses / resolves a question \\n\n",
        "     Give a binary score 'yes' or 'no'. Yes' means that the answer resolves the question.\"\"\"\n",
        "answer_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system),\n",
        "        (\"human\", \"User question: \\n\\n {question} \\n\\n LLM generation: {generation}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "answer_grader = answer_prompt | structured_llm_grader\n",
        "answer_grader.invoke({\"question\": question, \"generation\": generation})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "3adKdbYyWHXe",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "3adKdbYyWHXe",
        "outputId": "6001c9c6-a74b-4bfe-8b4b-2a7493572b47"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'What are the key components and functionalities of agent memory in AI systems, and how do they contribute to intelligent behavior?'"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "### Question Re-writer\n",
        "\n",
        "# Prompt\n",
        "system = \"\"\"You a question re-writer that converts an input question to a better version that is optimized \\n\n",
        "     for vectorstore retrieval. Look at the input and try to reason about the underlying semantic intent / meaning.\"\"\"\n",
        "re_write_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system),\n",
        "        (\n",
        "            \"human\",\n",
        "            \"Here is the initial question: \\n\\n {question} \\n Formulate an improved question.\",\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "\n",
        "question_rewriter = re_write_prompt | llm | StrOutputParser()\n",
        "question_rewriter.invoke({\"question\": question})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "YLgeea8nWKWq",
      "metadata": {
        "id": "YLgeea8nWKWq"
      },
      "source": [
        "#### Graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "dOkAtC64WJox",
      "metadata": {
        "id": "dOkAtC64WJox"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "\n",
        "from typing_extensions import TypedDict\n",
        "from langchain.schema import Document\n",
        "\n",
        "\n",
        "\n",
        "class GraphState(TypedDict):\n",
        "    \"\"\"\n",
        "    Represents the state of our graph.\n",
        "\n",
        "    Attributes:\n",
        "        question: question\n",
        "        generation: LLM generation\n",
        "        documents: list of documents\n",
        "    \"\"\"\n",
        "\n",
        "    question: str\n",
        "    generation: str\n",
        "    documents: List[Document]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "y6P0BipeWfBf",
      "metadata": {
        "id": "y6P0BipeWfBf"
      },
      "outputs": [],
      "source": [
        "### Nodes\n",
        "\n",
        "\n",
        "def retrieve(state):\n",
        "    \"\"\"\n",
        "    Retrieve documents\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        state (dict): New key added to state, documents, that contains retrieved documents\n",
        "    \"\"\"\n",
        "    print(\"---RETRIEVE---\")\n",
        "    question = state[\"question\"]\n",
        "\n",
        "    # Retrieval\n",
        "    documents = retriever.get_relevant_documents(question)\n",
        "    return {\"documents\": documents, \"question\": question}\n",
        "\n",
        "\n",
        "def generate(state):\n",
        "    \"\"\"\n",
        "    Generate answer\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        state (dict): New key added to state, generation, that contains LLM generation\n",
        "    \"\"\"\n",
        "    print(\"---GENERATE---\")\n",
        "    question = state[\"question\"]\n",
        "    documents = state[\"documents\"]\n",
        "\n",
        "    # RAG generation\n",
        "    generation = rag_chain.invoke({\"context\": documents, \"question\": question})\n",
        "    return {\"documents\": documents, \"question\": question, \"generation\": generation}\n",
        "\n",
        "\n",
        "def grade_documents(state):\n",
        "    \"\"\"\n",
        "    Determines whether the retrieved documents are relevant to the question.\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        state (dict): Updates documents key with only filtered relevant documents\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n",
        "    question = state[\"question\"]\n",
        "    documents = state[\"documents\"]\n",
        "\n",
        "    # Score each doc\n",
        "    filtered_docs = []\n",
        "    for d in documents:\n",
        "        score = retrieval_grader.invoke(\n",
        "            {\"question\": question, \"document\": d.page_content}\n",
        "        )\n",
        "        grade = score.binary_score\n",
        "        if grade == \"yes\":\n",
        "            print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
        "            filtered_docs.append(d)\n",
        "        else:\n",
        "            print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
        "            continue\n",
        "    return {\"documents\": filtered_docs, \"question\": question}\n",
        "\n",
        "\n",
        "def transform_query(state):\n",
        "    \"\"\"\n",
        "    Transform the query to produce a better question.\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        state (dict): Updates question key with a re-phrased question\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"---TRANSFORM QUERY---\")\n",
        "    question = state[\"question\"]\n",
        "    documents = state[\"documents\"]\n",
        "\n",
        "    # Re-write question\n",
        "    better_question = question_rewriter.invoke({\"question\": question})\n",
        "    return {\"documents\": documents, \"question\": better_question}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "cUKhqP7bWfgp",
      "metadata": {
        "id": "cUKhqP7bWfgp"
      },
      "outputs": [],
      "source": [
        "### Edges\n",
        "\n",
        "\n",
        "def decide_to_generate(state):\n",
        "    \"\"\"\n",
        "    Determines whether to generate an answer, or re-generate a question.\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        str: Binary decision for next node to call\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"---ASSESS GRADED DOCUMENTS---\")\n",
        "    state[\"question\"]\n",
        "    filtered_documents = state[\"documents\"]\n",
        "\n",
        "    if not filtered_documents:\n",
        "        # All documents have been filtered check_relevance\n",
        "        # We will re-generate a new query\n",
        "        print(\n",
        "            \"---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY---\"\n",
        "        )\n",
        "        return \"transform_query\"\n",
        "    else:\n",
        "        # We have relevant documents, so generate answer\n",
        "        print(\"---DECISION: GENERATE---\")\n",
        "        return \"generate\"\n",
        "\n",
        "\n",
        "def grade_generation_v_documents_and_question(state):\n",
        "    \"\"\"\n",
        "    Determines whether the generation is grounded in the document and answers question.\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        str: Decision for next node to call\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"---CHECK HALLUCINATIONS---\")\n",
        "    question = state[\"question\"]\n",
        "    documents = state[\"documents\"]\n",
        "    generation = state[\"generation\"]\n",
        "\n",
        "    score = hallucination_grader.invoke(\n",
        "        {\"documents\": documents, \"generation\": generation}\n",
        "    )\n",
        "    grade = score.binary_score\n",
        "\n",
        "    # Check hallucination\n",
        "    if grade == \"yes\":\n",
        "        print(\"---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\")\n",
        "        # Check question-answering\n",
        "        print(\"---GRADE GENERATION vs QUESTION---\")\n",
        "        score = answer_grader.invoke({\"question\": question, \"generation\": generation})\n",
        "        grade = score.binary_score\n",
        "        if grade == \"yes\":\n",
        "            print(\"---DECISION: GENERATION ADDRESSES QUESTION---\")\n",
        "            return \"useful\"\n",
        "        else:\n",
        "            print(\"---DECISION: GENERATION DOES NOT ADDRESS QUESTION---\")\n",
        "            return \"not useful\"\n",
        "    else:\n",
        "        pprint(\"---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\")\n",
        "        return \"not supported\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "utpZLraHeunO",
      "metadata": {
        "id": "utpZLraHeunO"
      },
      "outputs": [],
      "source": [
        "from langgraph.graph import END, StateGraph, START\n",
        "\n",
        "workflow = StateGraph(GraphState)\n",
        "\n",
        "# Define the nodes\n",
        "workflow.add_node(\"retrieve\", retrieve)  # retrieve\n",
        "workflow.add_node(\"grade_documents\", grade_documents)  # grade documents\n",
        "workflow.add_node(\"generate\", generate)  # generate\n",
        "workflow.add_node(\"transform_query\", transform_query)  # transform_query\n",
        "\n",
        "# Build graph\n",
        "workflow.add_edge(START, \"retrieve\")\n",
        "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
        "workflow.add_conditional_edges(\n",
        "    \"grade_documents\",\n",
        "    decide_to_generate,\n",
        "    {\n",
        "        \"transform_query\": \"transform_query\",\n",
        "        \"generate\": \"generate\",\n",
        "    },\n",
        ")\n",
        "workflow.add_edge(\"transform_query\", \"retrieve\")\n",
        "workflow.add_conditional_edges(\n",
        "    \"generate\",\n",
        "    grade_generation_v_documents_and_question,\n",
        "    {\n",
        "        \"not supported\": \"generate\",\n",
        "        \"useful\": END,\n",
        "        \"not useful\": \"transform_query\",\n",
        "    },\n",
        ")\n",
        "\n",
        "# Compile\n",
        "app = workflow.compile()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "WwO-He0TWrT5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WwO-He0TWrT5",
        "outputId": "a7372fb0-5ec5-4e05-9c80-2b598bd4b587"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---RETRIEVE---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-16-2367850148>:18: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  documents = retriever.get_relevant_documents(question)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\"Node 'retrieve':\"\n",
            "'\\n---\\n'\n",
            "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
            "---GRADE: DOCUMENT RELEVANT---\n",
            "---GRADE: DOCUMENT RELEVANT---\n",
            "---GRADE: DOCUMENT RELEVANT---\n",
            "---GRADE: DOCUMENT RELEVANT---\n",
            "---ASSESS GRADED DOCUMENTS---\n",
            "---DECISION: GENERATE---\n",
            "\"Node 'grade_documents':\"\n",
            "'\\n---\\n'\n",
            "---GENERATE---\n",
            "---CHECK HALLUCINATIONS---\n",
            "---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\n",
            "---GRADE GENERATION vs QUESTION---\n",
            "---DECISION: GENERATION ADDRESSES QUESTION---\n",
            "\"Node 'generate':\"\n",
            "'\\n---\\n'\n",
            "('In LLM-powered autonomous agents, memory is divided into short-term and '\n",
            " 'long-term types. Short-term memory relies on in-context learning, where the '\n",
            " 'model retains information temporarily within its context window to perform '\n",
            " 'tasks. Long-term memory involves external storage, such as vector databases, '\n",
            " 'allowing the agent to store and efficiently retrieve vast amounts of '\n",
            " 'information over extended periods using methods like approximate nearest '\n",
            " 'neighbors search.')\n"
          ]
        }
      ],
      "source": [
        "from pprint import pprint\n",
        "\n",
        "# Run\n",
        "inputs = {\"question\": \"Explain how the different types of agent memory work?\"}\n",
        "for output in app.stream(inputs):\n",
        "    for key, value in output.items():\n",
        "        # Node\n",
        "        pprint(f\"Node '{key}':\")\n",
        "        # Optional: print full state at each node\n",
        "        # pprint.pprint(value[\"keys\"], indent=2, width=80, depth=None)\n",
        "    pprint(\"\\n---\\n\")\n",
        "\n",
        "# Final generation\n",
        "pprint(value[\"generation\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "RhYvez5lWulg",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RhYvez5lWulg",
        "outputId": "5a710bf2-c4fa-4623-c2e7-f92872feacbe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---RETRIEVE---\n",
            "\"Node 'retrieve':\"\n",
            "'\\n---\\n'\n",
            "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
            "---GRADE: DOCUMENT RELEVANT---\n",
            "---GRADE: DOCUMENT RELEVANT---\n",
            "---GRADE: DOCUMENT RELEVANT---\n",
            "---GRADE: DOCUMENT RELEVANT---\n",
            "---ASSESS GRADED DOCUMENTS---\n",
            "---DECISION: GENERATE---\n",
            "\"Node 'grade_documents':\"\n",
            "'\\n---\\n'\n",
            "---GENERATE---\n",
            "---CHECK HALLUCINATIONS---\n",
            "---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\n",
            "---GRADE GENERATION vs QUESTION---\n",
            "---DECISION: GENERATION ADDRESSES QUESTION---\n",
            "\"Node 'generate':\"\n",
            "'\\n---\\n'\n",
            "('In LLM-powered autonomous agents, memory is categorized into **short-term** '\n",
            " 'and **long-term** types. Short-term memory corresponds to in-context '\n",
            " 'learning, where the agent uses the limited context window of the model to '\n",
            " 'retain and process recent information. Long-term memory allows the agent to '\n",
            " 'store and retrieve vast amounts of information over extended periods, '\n",
            " 'typically using external vector stores with fast retrieval methods like '\n",
            " 'approximate nearest neighbors (ANN). Some frameworks also include **sensory '\n",
            " \"memory**, which handles raw input embeddings, further supporting the agent's \"\n",
            " 'ability to process diverse modalities.')\n"
          ]
        }
      ],
      "source": [
        "from pprint import pprint\n",
        "\n",
        "# Run\n",
        "inputs = {\"question\": \"Explain how the different types of agent memory work?\"}\n",
        "for output in app.stream(inputs):\n",
        "    for key, value in output.items():\n",
        "        # Node\n",
        "        pprint(f\"Node '{key}':\")\n",
        "        # Optional: print full state at each node\n",
        "        # pprint.pprint(value[\"keys\"], indent=2, width=80, depth=None)\n",
        "    pprint(\"\\n---\\n\")\n",
        "\n",
        "# Final generation\n",
        "pprint(value[\"generation\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48f233df-b2a3-438c-8d65-5e83bf6ace64",
      "metadata": {
        "id": "48f233df-b2a3-438c-8d65-5e83bf6ace64"
      },
      "source": [
        "## 18 - Impact of long context  \n",
        "\n",
        "`Deep dive`\n",
        "\n",
        "https://www.youtube.com/watch?v=SsHUNfhF32s\n",
        "\n",
        "`Slides`\n",
        "\n",
        "https://docs.google.com/presentation/d/1mJUiPBdtf58NfuSEQ7pVSEQ2Oqmek7F1i4gBwR6JDss/edit#slide=id.g26c0cb8dc66_0_0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aVs8qaGXWFGe",
      "metadata": {
        "id": "aVs8qaGXWFGe"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
