{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "402ae2dc-9c06-4de6-a301-39bcfde04153",
      "metadata": {
        "id": "402ae2dc-9c06-4de6-a301-39bcfde04153"
      },
      "source": [
        "# 检索和重排\n",
        "\n",
        "![retrieval and reranking](https://github.com/tivon-x/bRAG-langchain/blob/main/notebooks/image/retrieval_reranking.png?raw=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "75e299e2",
      "metadata": {
        "id": "75e299e2"
      },
      "outputs": [],
      "source": [
        "! pip3 install --quiet langchain_community tiktoken langchain-openai langchainhub chromadb langchain cohere langgraph python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install --upgrade --quiet  dashscope"
      ],
      "metadata": {
        "id": "nWO5tdJ1WfZ3",
        "outputId": "da6f485d-bd11-4e77-a4e1-e401eeb221db",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "nWO5tdJ1WfZ3",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.3 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.1/1.3 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.7/1.3 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install --quiet langchain-chroma"
      ],
      "metadata": {
        "id": "zrHg2LQdab9I"
      },
      "id": "zrHg2LQdab9I",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "f2364045",
      "metadata": {
        "id": "f2364045"
      },
      "source": [
        "## Environment\n",
        "\n",
        "`(1) Packages`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad05ae49",
      "metadata": {
        "id": "ad05ae49"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# LangSmith\n",
        "langsmith_tracing = os.getenv('LANGSMITH_TRACING')\n",
        "langsmith_endpoint = os.getenv('LANGSMITH_ENDPOINT')\n",
        "langsmith_api_key = os.getenv('LANGSMITH_API_KEY')\n",
        "\n",
        "## LLM\n",
        "dashscope_api_key = os.getenv('DASHSCOPE_API_KEY')\n",
        "\n",
        "## Cohere API\n",
        "cohere_api_key = os.getenv('COHERE_API_KEY')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Colab环境\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "langsmith_tracing = userdata.get('LANGSMITH_TRACING')\n",
        "langsmith_endpoint = userdata.get('LANGSMITH_ENDPOINT')\n",
        "langsmith_api_key = userdata.get('LANGSMITH_API_KEY')\n",
        "\n",
        "dashscope_api_key = userdata.get(\"DASHSCOPE_API_KEY\")\n",
        "\n",
        "## Cohere API\n",
        "cohere_api_key = userdata.get('COHERE_API_KEY')"
      ],
      "metadata": {
        "id": "Oe6f9VmNZAxm"
      },
      "id": "Oe6f9VmNZAxm",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "ef377957",
      "metadata": {
        "id": "ef377957"
      },
      "source": [
        "`(2) LangSmith`\n",
        "\n",
        "https://docs.smith.langchain.com/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "f213ce96",
      "metadata": {
        "id": "f213ce96"
      },
      "outputs": [],
      "source": [
        "os.environ['LANGSMITH_TRACING'] = langsmith_tracing\n",
        "os.environ['LANGSMITH_ENDPOINT'] = langsmith_endpoint\n",
        "os.environ['LANGSMITH_API_KEY'] = langsmith_api_key"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "744ec5ec",
      "metadata": {
        "id": "744ec5ec"
      },
      "source": [
        "`(3) API Keys`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "024a38cd",
      "metadata": {
        "id": "024a38cd"
      },
      "outputs": [],
      "source": [
        "os.environ['DASHSCOPE_API_KEY'] = dashscope_api_key\n",
        "dashscope_model = \"qwen-plus-latest\"\n",
        "\n",
        "os.environ['COHERE_API_KEY'] = cohere_api_key"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# langchain的webbaseloader需要\n",
        "os.environ[\"USER_AGENT\"] = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\""
      ],
      "metadata": {
        "id": "hrJiWNMSZef7"
      },
      "id": "hrJiWNMSZef7",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "a0ae5a7f-7c07-4825-b53b-4ac632b8eac6",
      "metadata": {
        "id": "a0ae5a7f-7c07-4825-b53b-4ac632b8eac6"
      },
      "source": [
        "## 重排 Re-ranking\n",
        "\n",
        "之前的RAG-Fusion架构已经执行了重排操作：\n",
        "\n",
        "![reranking](https://github.com/tivon-x/bRAG-langchain/blob/main/notebooks/image/reranking.png?raw=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "f201eec1-6ed0-4236-9594-286894574779",
      "metadata": {
        "id": "f201eec1-6ed0-4236-9594-286894574779"
      },
      "outputs": [],
      "source": [
        "#### INDEXING ####\n",
        "\n",
        "# Load blog\n",
        "import bs4\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "loader = WebBaseLoader(\n",
        "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
        "    bs_kwargs=dict(\n",
        "        parse_only=bs4.SoupStrainer(\n",
        "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
        "        )\n",
        "    ),\n",
        ")\n",
        "blog_docs = loader.load()\n",
        "\n",
        "# Split\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "    chunk_size=300,\n",
        "    chunk_overlap=50)\n",
        "\n",
        "# Make splits\n",
        "splits = text_splitter.split_documents(blog_docs)\n",
        "\n",
        "# Index\n",
        "from langchain_community.embeddings import DashScopeEmbeddings\n",
        "\n",
        "embeddings = DashScopeEmbeddings(model=\"text-embedding-v4\")\n",
        "\n",
        "def batch_read(lst, batch_size=10):\n",
        "    for i in range(0, len(lst), batch_size):\n",
        "        yield lst[i:i + batch_size]\n",
        "\n",
        "# from langchain_cohere import CohereEmbeddings\n",
        "from langchain_chroma import Chroma\n",
        "\n",
        "vectorstore = Chroma(embedding_function=embeddings)\n",
        "\n",
        "for batch in batch_read(splits):\n",
        "  vectorstore.add_documents(batch)\n",
        "\n",
        "retriever = vectorstore.as_retriever()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "b733a230-d217-4ee0-8482-ab4380e7be4f",
      "metadata": {
        "id": "b733a230-d217-4ee0-8482-ab4380e7be4f"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "# RAG-Fusion\n",
        "template = \"\"\"You are a helpful assistant that generates multiple search queries based on a single input query. \\n\n",
        "Generate multiple search queries related to: {question} \\n\n",
        "Output (4 queries):\"\"\"\n",
        "prompt_rag_fusion = ChatPromptTemplate.from_template(template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "6e37bc5d-93fc-4b34-8a4d-208394e89709",
      "metadata": {
        "id": "6e37bc5d-93fc-4b34-8a4d-208394e89709"
      },
      "outputs": [],
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_community.chat_models.tongyi import ChatTongyi\n",
        "\n",
        "llm = ChatTongyi(model=dashscope_model, temperature=0.1)\n",
        "\n",
        "\n",
        "generate_queries = (\n",
        "    prompt_rag_fusion\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        "    | (lambda x: x.split(\"\\n\"))\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "728fedac-7f1d-49aa-9d02-47243c0f2bb4",
      "metadata": {
        "id": "728fedac-7f1d-49aa-9d02-47243c0f2bb4",
        "outputId": "dd1fb95a-ea35-4571-87ca-ec9408fc01d0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "from langchain.load import dumps, loads\n",
        "\n",
        "def reciprocal_rank_fusion(results: list[list], k=60, n=7):\n",
        "    \"\"\" Reciprocal_rank_fusion that takes multiple lists of ranked documents\n",
        "        and an optional parameter k used in the RRF formula\n",
        "        接受多个排名文档列表、RRF公式中使用的可选参数k、返回的文档数量\n",
        "    \"\"\"\n",
        "\n",
        "    # 初始化字典以保存每个唯一文档的融合分数\n",
        "    fused_scores = {}\n",
        "\n",
        "    # 遍历每个排名文档列表\n",
        "    for docs in results:\n",
        "        # 遍历列表中的每个文档及其排名（列表中的位置）\n",
        "        for rank, doc in enumerate(docs):\n",
        "            # 将文档转换为字符串格式以用作key（假设文档可以序列化为JSON）\n",
        "            doc_str = dumps(doc)\n",
        "            # 如果文档尚未在fused_scores字典中，请将其初始分数添加为0\n",
        "            if doc_str not in fused_scores:\n",
        "                fused_scores[doc_str] = 0\n",
        "            # 检索文档的当前分数（如果有的话）\n",
        "            previous_score = fused_scores[doc_str]\n",
        "            # 使用RRF公式更新文档的分数：1/（rank+k）\n",
        "            fused_scores[doc_str] += 1 / (rank + k)\n",
        "\n",
        "    # 根据fusion分数按降序对文档进行排序，以获得最终的重新排序结果\n",
        "    reranked_results = [\n",
        "        (loads(doc), score)\n",
        "        for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
        "    ]\n",
        "\n",
        "    # 将重新排序的结果作为元组列表返回，每个元组包含文档及其fusion分数\n",
        "    return reranked_results[:n]\n",
        "\n",
        "question = \"What is task decomposition for LLM agents?\"\n",
        "retrieval_chain_rag_fusion = generate_queries | retriever.map() | reciprocal_rank_fusion\n",
        "docs = retrieval_chain_rag_fusion.invoke({\"question\": question})\n",
        "len(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "ee43bb94-613d-46a3-b688-02c2d2aadfba",
      "metadata": {
        "id": "ee43bb94-613d-46a3-b688-02c2d2aadfba",
        "outputId": "a810a6b3-c991-497f-b8bd-6175abfc25e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Task decomposition for LLM (Large Language Model) agents refers to the process of breaking down complex tasks into smaller, more manageable subgoals or steps. This enables efficient handling of intricate problems by transforming them into multiple simpler tasks. \\n\\nKey methods and insights about task decomposition from the context include:\\n\\n1. **Chain of Thought (CoT)**: A prompting technique where the model is instructed to \"think step by step,\" decomposing a hard task into smaller steps to improve performance on complex tasks.\\n\\n2. **Tree of Thoughts (ToT)**: An extension of CoT that explores multiple reasoning possibilities at each step. It generates multiple thoughts per step, creating a tree structure, which can be explored using BFS (breadth-first search) or DFS (depth-first search).\\n\\n3. **Implementation Approaches**:\\n   - Prompting the LLM directly with instructions like \"Steps for XYZ\" or \"What are the subgoals for achieving XYZ?\"\\n   - Using task-specific instructions, such as \"Write a story outline.\"\\n   - Incorporating human inputs for guidance.\\n\\nBy effectively decomposing tasks, LLM-powered autonomous agents can plan ahead, handle multi-step reasoning, and improve their problem-solving capabilities.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "from operator import itemgetter\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "# RAG\n",
        "template = \"\"\"Answer the following question based on this context:\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "final_rag_chain = (\n",
        "    {\"context\": retrieval_chain_rag_fusion,\n",
        "     \"question\": itemgetter(\"question\")}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "final_rag_chain.invoke({\"question\":question})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a70968aa-52e8-41b9-96f1-7d811351512e",
      "metadata": {
        "id": "a70968aa-52e8-41b9-96f1-7d811351512e"
      },
      "source": [
        "我们也可以使用 [Cohere Re-Rank](https://python.langchain.com/docs/integrations/retrievers/cohere-reranker#doing-reranking-with-coherererank). 进行重排操作\n",
        "\n",
        "[cohere 相关博客](https://txt.cohere.com/rerank/):\n",
        "\n",
        "![cohere-re-rank](https://github.com/tivon-x/bRAG-langchain/blob/main/notebooks/image/cohere-re-rank.png?raw=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "9bdd18dd-2e6d-428d-aacb-a539a94064ba",
      "metadata": {
        "id": "9bdd18dd-2e6d-428d-aacb-a539a94064ba"
      },
      "outputs": [],
      "source": [
        "from langchain_community.llms import Cohere\n",
        "from langchain.retrievers import  ContextualCompressionRetriever\n",
        "from langchain.retrievers.document_compressors import CohereRerank"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "b88e1ad8-57eb-40d4-8475-a50775a692b0",
      "metadata": {
        "id": "b88e1ad8-57eb-40d4-8475-a50775a692b0",
        "outputId": "19d80d24-2c30-40a2-f73f-363702c9fc6a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'relevance_score': 0.998844}, page_content='Component One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.'),\n",
              " Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'relevance_score': 0.998844}, page_content='Component One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.'),\n",
              " Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'relevance_score': 0.99293363}, page_content='LLM Powered Autonomous Agents\\n    \\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\\n\\n\\nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview#\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory')]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "# 返回前10个\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 10})\n",
        "\n",
        "# Re-rank\n",
        "compressor = CohereRerank(model=\"rerank-english-v3.0\")\n",
        "compression_retriever = ContextualCompressionRetriever(\n",
        "    base_compressor=compressor, base_retriever=retriever\n",
        ")\n",
        "\n",
        "compressed_docs = compression_retriever.invoke(question)\n",
        "compressed_docs"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "整合到RAG链中"
      ],
      "metadata": {
        "id": "W16d90fbf2Hc"
      },
      "id": "W16d90fbf2Hc"
    },
    {
      "cell_type": "code",
      "source": [
        "# Post-processing\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "cohere_rag_chain = (\n",
        "    {\"context\": compression_retriever | format_docs,\n",
        "     \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "cohere_rag_chain.invoke(question)"
      ],
      "metadata": {
        "id": "VJQ5-sjBf00V",
        "outputId": "c654b4c2-3353-40ca-cc26-8caa4bd02c75",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        }
      },
      "id": "VJQ5-sjBf00V",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'**Answer:**  \\nTask decomposition for LLM (large language model) agents refers to the process of breaking down complex tasks into smaller, more manageable subgoals or steps. This enables the agent to handle complicated problems more effectively by focusing on solving each individual part sequentially. Techniques like Chain of Thought (CoT) and Tree of Thoughts (ToT) are used to facilitate task decomposition:\\n\\n- **Chain of Thought (CoT)** involves instructing the model to \"think step-by-step,\" decomposing a task into a sequence of intermediate reasoning steps.\\n- **Tree of Thoughts (ToT)** extends CoT by exploring multiple possible reasoning paths at each step, organizing them in a tree structure, and using search strategies like BFS or DFS to find the optimal solution.\\n\\nTask decomposition can be performed:\\n1. By the LLM itself through simple prompting (e.g., “Steps for XYZ.”),\\n2. Through task-specific instructions (e.g., “Write a story outline.”),\\n3. With assistance from human inputs.\\n\\nThis planning component is essential for enabling autonomous LLM-powered agents to tackle complex problems efficiently.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "97b2c3ff-60df-4a68-9516-f4577ed320a4",
      "metadata": {
        "id": "97b2c3ff-60df-4a68-9516-f4577ed320a4"
      },
      "source": [
        "## CRAG\n",
        "\n",
        "`视频教程`\n",
        "\n",
        "https://www.youtube.com/watch?v=E2shqsYwxck\n",
        "\n",
        "`Notebooks`\n",
        "\n",
        "https://github.com/langchain-ai/langgraph/blob/main/examples/rag/langgraph_crag.ipynb\n",
        "\n",
        "https://github.com/langchain-ai/langgraph/blob/main/examples/rag/langgraph_crag_local.ipynb\n",
        "\n",
        "动机：\n",
        "- self-reflection可以增强RAG\n",
        "- Self-reflection的想法是：基于检索得到的文档与问题的相关性、生成内容相对于问题的质量或者生成内容相对于检索得到的文档的质量，进行一些推理、反馈和重试各种步骤。\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic import BaseModel, Field\n",
        "\n",
        "# Data model\n",
        "class GradeDocuments(BaseModel):\n",
        "    \"\"\"Binary score for relevance check on retrieved documents.\"\"\"\n",
        "\n",
        "    binary_score: str = Field(\n",
        "        description=\"Documents are relevant to the question, 'yes' or 'no'\"\n",
        "    )\n",
        "\n",
        "structured_llm_grader = llm.with_structured_output(GradeDocuments)\n",
        "\n",
        "# Prompt\n",
        "system = \"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n\n",
        "    If the document contains keyword(s) or semantic meaning related to the question, grade it as relevant. \\n\n",
        "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\"\n",
        "grade_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system),\n",
        "        (\"human\", \"Retrieved document: \\n\\n {document} \\n\\n User question: {question}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "retrieval_grader = grade_prompt | structured_llm_grader\n",
        "question = \"agent memory\"\n",
        "docs = retriever.get_relevant_documents(question)\n",
        "doc_txt = docs[1].page_content\n",
        "print(retrieval_grader.invoke({\"question\": question, \"document\": doc_txt}))"
      ],
      "metadata": {
        "id": "5_dTQ8fNyTcx",
        "outputId": "c9dbdfeb-98aa-47d2-fe14-3c064afa6645",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "5_dTQ8fNyTcx",
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "binary_score='yes'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Generate\n",
        "\n",
        "from langchain import hub\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# Prompt\n",
        "prompt = hub.pull(\"rlm/rag-prompt\")\n",
        "\n",
        "\n",
        "# Post-processing\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "\n",
        "# Chain\n",
        "rag_chain = prompt | llm | StrOutputParser()\n",
        "\n",
        "# Run\n",
        "generation = rag_chain.invoke({\"context\": docs, \"question\": question})\n",
        "print(generation)"
      ],
      "metadata": {
        "id": "g4o5bLZa2qIH",
        "outputId": "13fcdb3a-4060-4016-f38c-8175913dd76a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "g4o5bLZa2qIH",
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In the context of LLM-powered autonomous agents, memory is categorized into short-term and long-term memory. Short-term memory involves in-context learning, constrained by the model's context window, while long-term memory utilizes external vector stores for retaining and retrieving vast information over time. These components enable agents to effectively manage knowledge and improve decision-making through reflection and refinement.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt\n",
        "system = \"\"\"You a question re-writer that converts an input question to a better version that is optimized \\n\n",
        "     for web search. Look at the input and try to reason about the underlying semantic intent / meaning.\"\"\"\n",
        "re_write_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system),\n",
        "        (\n",
        "            \"human\",\n",
        "            \"Here is the initial question: \\n\\n {question} \\n Formulate an improved question.\",\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "\n",
        "question_rewriter = re_write_prompt | llm | StrOutputParser()\n",
        "question_rewriter.invoke({\"question\": question})"
      ],
      "metadata": {
        "id": "vOOZI5-g8vf4",
        "outputId": "a9f79c7f-5f56-48f1-fda6-dd4bd8aeba4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "id": "vOOZI5-g8vf4",
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Sure! Here\\'s an improved version of the question optimized for web search:\\n\\n**\"What is agent memory in artificial intelligence, and how does it function in AI systems?\"**\\n\\nThis version clarifies the topic and specifies the intent, making it easier to find relevant and detailed information.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langchain-tavily"
      ],
      "metadata": {
        "id": "MWFNDrwu2vtT"
      },
      "id": "MWFNDrwu2vtT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"TAVILY_API_KEY\"] = userdata.get(\"TAVILY_API_KEY\")"
      ],
      "metadata": {
        "id": "VLodpbWw4qUT"
      },
      "id": "VLodpbWw4qUT",
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_tavily import TavilySearch\n",
        "\n",
        "tool = TavilySearch(max_results=3)\n",
        "result = tool.invoke({\"query\": \"What's a 'node' in LangGraph?\"})"
      ],
      "metadata": {
        "id": "6NZRxBpj3qEK"
      },
      "id": "6NZRxBpj3qEK",
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result[\"results\"][0]"
      ],
      "metadata": {
        "id": "4lEBj7zNCiw7",
        "outputId": "f50df1ea-e45f-4ec1-d396-16f40d7bca11",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "4lEBj7zNCiw7",
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'title': 'What is LangGraph? - GeeksforGeeks',\n",
              " 'url': 'https://www.geeksforgeeks.org/machine-learning/what-is-langgraph/',\n",
              " 'content': 'LangGraph is a Python library that helps you build applications like chatbots or AI agents by organizing their logic step-by-step using state machine model. This step configures your Gemini API key and then we create a simple function `ask_gemini` that takes user input, sends it to the Gemini model and returns the AI-generated response. Creates a state structure with three fields: `question`, `classification` and `response` which flows through the LangGraph. import matplotlib.pyplot as plt from langgraph.graph import StateGraph\\u200bbuilder = StateGraph(GraphState)builder.add_node(\"classify\", classify)builder.add_node(\"respond\", respond)builder.set_entry_point(\"classify\")builder.add_edge(\"classify\", \"respond\")builder.set_finish_point(\"respond\")app = builder.compile()\\u200bdef visualize_workflow(builder): G = nx.DiGraph()\\u200b for node in builder.nodes: G.add_node(node) for edge in builder.edges: G.add_edge(edge[0], edge[1])\\u200b pos = nx.spring_layout(G) nx.draw(G, pos, with_labels=True, node_size=3000, node_color=\"skyblue\", font_size=12, font_weight=\"bold\", arrows=True)  plt.title(\"Langchain Workflow Visualization\") plt.show()\\u200bvisualize_workflow(builder)',\n",
              " 'score': 0.69249696,\n",
              " 'raw_content': None}"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "\n",
        "from typing_extensions import TypedDict\n",
        "from langchain.schema import Document\n",
        "\n",
        "\n",
        "class GraphState(TypedDict):\n",
        "    \"\"\"\n",
        "    Represents the state of our graph.\n",
        "\n",
        "    Attributes:\n",
        "        question: question\n",
        "        generation: LLM generation\n",
        "        web_search: whether to add search\n",
        "        documents: list of documents\n",
        "    \"\"\"\n",
        "\n",
        "    question: str\n",
        "    generation: str\n",
        "    web_search: str\n",
        "    documents: List[Document]"
      ],
      "metadata": {
        "id": "bhHRp25N7AhI"
      },
      "id": "bhHRp25N7AhI",
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve(state):\n",
        "    \"\"\"\n",
        "    Retrieve documents\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        state (dict): New key added to state, documents, that contains retrieved documents\n",
        "    \"\"\"\n",
        "    print(\"---RETRIEVE---\")\n",
        "    question = state[\"question\"]\n",
        "\n",
        "    # Retrieval\n",
        "    documents = retriever.get_relevant_documents(question)\n",
        "    return {\"documents\": documents, \"question\": question}\n",
        "\n",
        "\n",
        "def generate(state):\n",
        "    \"\"\"\n",
        "    Generate answer\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        state (dict): New key added to state, generation, that contains LLM generation\n",
        "    \"\"\"\n",
        "    print(\"---GENERATE---\")\n",
        "    question = state[\"question\"]\n",
        "    documents = state[\"documents\"]\n",
        "\n",
        "    # RAG generation\n",
        "    generation = rag_chain.invoke({\"context\": documents, \"question\": question})\n",
        "    return {\"documents\": documents, \"question\": question, \"generation\": generation}\n",
        "\n",
        "\n",
        "def grade_documents(state):\n",
        "    \"\"\"\n",
        "    Determines whether the retrieved documents are relevant to the question.\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        state (dict): Updates documents key with only filtered relevant documents\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n",
        "    question = state[\"question\"]\n",
        "    documents = state[\"documents\"]\n",
        "\n",
        "    # Score each doc\n",
        "    filtered_docs = []\n",
        "    web_search = \"No\"\n",
        "    for d in documents:\n",
        "        score = retrieval_grader.invoke(\n",
        "            {\"question\": question, \"document\": d.page_content}\n",
        "        )\n",
        "        grade = score.binary_score\n",
        "        if grade == \"yes\":\n",
        "            print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
        "            filtered_docs.append(d)\n",
        "        else:\n",
        "            print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
        "            web_search = \"Yes\"\n",
        "            continue\n",
        "    return {\"documents\": filtered_docs, \"question\": question, \"web_search\": web_search}\n",
        "\n",
        "\n",
        "def transform_query(state):\n",
        "    \"\"\"\n",
        "    Transform the query to produce a better question.\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        state (dict): Updates question key with a re-phrased question\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"---TRANSFORM QUERY---\")\n",
        "    question = state[\"question\"]\n",
        "    documents = state[\"documents\"]\n",
        "\n",
        "    # Re-write question\n",
        "    better_question = question_rewriter.invoke({\"question\": question})\n",
        "    print(better_question)\n",
        "    return {\"documents\": documents, \"question\": better_question}\n",
        "\n",
        "\n",
        "def web_search(state):\n",
        "    \"\"\"\n",
        "    Web search based on the re-phrased question.\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        state (dict): Updates documents key with appended web results\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"---WEB SEARCH---\")\n",
        "    question = state[\"question\"]\n",
        "    documents = state[\"documents\"]\n",
        "\n",
        "    # Web search\n",
        "    tool_result = tool.invoke({\"query\": question})\n",
        "    print(tool_result)\n",
        "    web_results = \"\\n\".join([d[\"content\"] for d in tool_result[\"results\"]])\n",
        "    web_results = Document(page_content=web_results)\n",
        "    documents.append(web_results)\n",
        "\n",
        "    return {\"documents\": documents, \"question\": question}\n",
        "\n",
        "\n",
        "### Edges\n",
        "\n",
        "\n",
        "def decide_to_generate(state):\n",
        "    \"\"\"\n",
        "    Determines whether to generate an answer, or re-generate a question.\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        str: Binary decision for next node to call\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"---ASSESS GRADED DOCUMENTS---\")\n",
        "    state[\"question\"]\n",
        "    web_search = state[\"web_search\"]\n",
        "    state[\"documents\"]\n",
        "\n",
        "    if web_search == \"Yes\":\n",
        "        # All documents have been filtered check_relevance\n",
        "        # We will re-generate a new query\n",
        "        print(\n",
        "            \"---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY---\"\n",
        "        )\n",
        "        return \"transform_query\"\n",
        "    else:\n",
        "        # We have relevant documents, so generate answer\n",
        "        print(\"---DECISION: GENERATE---\")\n",
        "        return \"generate\""
      ],
      "metadata": {
        "id": "QDjhLGjS7GP5"
      },
      "id": "QDjhLGjS7GP5",
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import END, StateGraph, START\n",
        "\n",
        "workflow = StateGraph(GraphState)\n",
        "\n",
        "# Define the nodes\n",
        "workflow.add_node(\"retrieve\", retrieve)  # retrieve\n",
        "workflow.add_node(\"grade_documents\", grade_documents)  # grade documents\n",
        "workflow.add_node(\"generate\", generate)  # generate\n",
        "workflow.add_node(\"transform_query\", transform_query)  # transform_query\n",
        "workflow.add_node(\"web_search_node\", web_search)  # web search\n",
        "\n",
        "# Build graph\n",
        "workflow.add_edge(START, \"retrieve\")\n",
        "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
        "workflow.add_conditional_edges(\n",
        "    \"grade_documents\",\n",
        "    decide_to_generate,\n",
        "    {\n",
        "        \"transform_query\": \"transform_query\",\n",
        "        \"generate\": \"generate\",\n",
        "    },\n",
        ")\n",
        "workflow.add_edge(\"transform_query\", \"web_search_node\")\n",
        "workflow.add_edge(\"web_search_node\", \"generate\")\n",
        "workflow.add_edge(\"generate\", END)\n",
        "\n",
        "# Compile\n",
        "app = workflow.compile()"
      ],
      "metadata": {
        "id": "MlBBWzpX8Ns3"
      },
      "id": "MlBBWzpX8Ns3",
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint\n",
        "\n",
        "# Run\n",
        "inputs = {\"question\": \"What are the types of agent memory?\"}\n",
        "for output in app.stream(inputs):\n",
        "    for key, value in output.items():\n",
        "        # Node\n",
        "        pprint(f\"Node '{key}':\")\n",
        "        # Optional: print full state at each node\n",
        "        # pprint.pprint(value[\"keys\"], indent=2, width=80, depth=None)\n",
        "    pprint(\"\\n---\\n\")\n",
        "\n",
        "# Final generation\n",
        "pprint(value[\"generation\"])"
      ],
      "metadata": {
        "id": "oJwBHXL28RPZ",
        "outputId": "bde8917f-3dcb-4c35-e1a7-f1690b1f95c4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "oJwBHXL28RPZ",
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---RETRIEVE---\n",
            "\"Node 'retrieve':\"\n",
            "'\\n---\\n'\n",
            "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
            "---GRADE: DOCUMENT RELEVANT---\n",
            "---GRADE: DOCUMENT RELEVANT---\n",
            "---GRADE: DOCUMENT RELEVANT---\n",
            "---GRADE: DOCUMENT RELEVANT---\n",
            "---GRADE: DOCUMENT RELEVANT---\n",
            "---GRADE: DOCUMENT RELEVANT---\n",
            "---GRADE: DOCUMENT RELEVANT---\n",
            "---GRADE: DOCUMENT RELEVANT---\n",
            "---GRADE: DOCUMENT RELEVANT---\n",
            "---GRADE: DOCUMENT RELEVANT---\n",
            "---ASSESS GRADED DOCUMENTS---\n",
            "---DECISION: GENERATE---\n",
            "\"Node 'grade_documents':\"\n",
            "'\\n---\\n'\n",
            "---GENERATE---\n",
            "\"Node 'generate':\"\n",
            "'\\n---\\n'\n",
            "('The types of agent memory include Sensory Memory, Short-Term Memory (STM) or '\n",
            " 'Working Memory, and Long-Term Memory (LTM). Sensory memory retains '\n",
            " 'impressions of sensory information briefly, STM holds a limited amount of '\n",
            " 'information actively for a short period, and LTM stores vast amounts of '\n",
            " 'information for long durations. LTM is further divided into '\n",
            " 'Explicit/Declarative (episodic and semantic) and Implicit/Procedural memory.')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint\n",
        "\n",
        "# Run\n",
        "inputs = {\"question\": \"How does the AlphaCodium paper work?\"}\n",
        "for output in app.stream(inputs):\n",
        "    for key, value in output.items():\n",
        "        # Node\n",
        "        pprint(f\"Node '{key}':\")\n",
        "        # Optional: print full state at each node\n",
        "        # pprint.pprint(value[\"keys\"], indent=2, width=80, depth=None)\n",
        "    pprint(\"\\n---\\n\")\n",
        "\n",
        "# Final generation\n",
        "pprint(value[\"generation\"])"
      ],
      "metadata": {
        "id": "wgky8_2C8hdh",
        "outputId": "c12e2389-3ab1-4c2e-c491-66483b8b357d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "wgky8_2C8hdh",
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---RETRIEVE---\n",
            "\"Node 'retrieve':\"\n",
            "'\\n---\\n'\n",
            "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
            "---GRADE: DOCUMENT NOT RELEVANT---\n",
            "---GRADE: DOCUMENT NOT RELEVANT---\n",
            "---GRADE: DOCUMENT NOT RELEVANT---\n",
            "---GRADE: DOCUMENT NOT RELEVANT---\n",
            "---GRADE: DOCUMENT NOT RELEVANT---\n",
            "---GRADE: DOCUMENT NOT RELEVANT---\n",
            "---GRADE: DOCUMENT NOT RELEVANT---\n",
            "---GRADE: DOCUMENT NOT RELEVANT---\n",
            "---GRADE: DOCUMENT NOT RELEVANT---\n",
            "---GRADE: DOCUMENT NOT RELEVANT---\n",
            "---ASSESS GRADED DOCUMENTS---\n",
            "---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY---\n",
            "\"Node 'grade_documents':\"\n",
            "'\\n---\\n'\n",
            "---TRANSFORM QUERY---\n",
            "\"Node 'transform_query':\"\n",
            "'\\n---\\n'\n",
            "---WEB SEARCH---\n",
            "{'query': 'Improved question:  \\n**What is the methodology and key contribution of the AlphaCodium paper?**\\n\\nAlternate version for broader search results:  \\n**How does the AlphaCodium approach function, and what are its main innovations?**', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'title': 'Code Generation with AlphaCodium: From Prompt Engineering to Flow ...', 'url': 'https://arxiv.org/abs/2401.08500', 'content': 'Donate > cs > arXiv:2401.08500 Help | Advanced Search Search GO quick links Login Help Pages About Computer Science > Machine Learning arXiv:2401.08500 (cs) [Submitted on 16 Jan 2024] Title:Code Generation with AlphaCodium: From Prompt Engineering to Flow Engineering Authors:Tal Ridnik, Dedy Kredo, Itamar Friedman View a PDF of the paper titled Code Generation with AlphaCodium: From Prompt Engineering to Flow Engineering, by Tal Ridnik and 2 other authors View PDF Abstract:Code generation problems differ from common natural language problems - they require matching the exact syntax of the target language, identifying happy paths and edge cases, paying attention to numerous small details in the problem spec, and addressing other code-specific issues and requirements. Hence, many of the optimizations and tricks that have been successful in natural language generation may not be effective for code tasks. In this work, we propose a new approach to code generation by LLMs, which we call AlphaCodium - a test-based, multi-stage, code-oriented iterative flow, that improves the performances of LLMs on code problems. The proposed flow consistently and significantly improves results. Many of the principles and best practices acquired in this work, we believe, are broadly applicable to general code generation tasks.', 'score': 0.35595453, 'raw_content': None}, {'title': 'AlphaCodium/README.md at main · Codium-ai/AlphaCodium - GitHub', 'url': 'https://github.com/Codium-ai/AlphaCodium/blob/main/README.md', 'content': 'Official implementation for the paper: \"Code Generation with AlphaCodium: From Prompt Engineering to Flow Engineering\"\" - AlphaCodium/README.md at main · Codium-ai/AlphaCodium', 'score': 0.25838256, 'raw_content': None}, {'title': 'GitHub - Codium-ai/AlphaCodium: Official implementation for the paper ...', 'url': 'https://github.com/Codium-ai/AlphaCodium', 'content': 'The split_name can be either valid or test.; database_solution_path is the path to the directory where the solutions will be saved.; The dataset section in the configuration file contains the configuration for the running and evaluation of a dataset.; Note that this is a long process, and it may take a few days to complete with large models (e.g. GPT-4) and several iterations per problem.', 'score': 0.2493517, 'raw_content': None}], 'response_time': 2.24}\n",
            "\"Node 'web_search_node':\"\n",
            "'\\n---\\n'\n",
            "---GENERATE---\n",
            "\"Node 'generate':\"\n",
            "'\\n---\\n'\n",
            "('AlphaCodium introduces a test-based, multi-stage, code-oriented iterative '\n",
            " 'flow to improve code generation by LLMs. Its methodology involves '\n",
            " 'iteratively refining code through stages like problem understanding, '\n",
            " 'test-case generation, and solution synthesis. The key contribution is '\n",
            " 'shifting from prompt engineering to flow engineering, offering a structured '\n",
            " 'approach that enhances performance on code-specific challenges.')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d5bfc7f-5f21-43a3-a149-14256860d0f1",
      "metadata": {
        "id": "0d5bfc7f-5f21-43a3-a149-14256860d0f1"
      },
      "source": [
        "# Generation\n",
        "\n",
        "\n",
        "\n",
        "## 17 - Retrieval (Self-RAG)\n",
        "\n",
        "`Notebooks`\n",
        "\n",
        "https://github.com/langchain-ai/langgraph/tree/main/examples/rag\n",
        "\n",
        "https://github.com/langchain-ai/langgraph/blob/main/examples/rag/langgraph_self_rag.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48f233df-b2a3-438c-8d65-5e83bf6ace64",
      "metadata": {
        "id": "48f233df-b2a3-438c-8d65-5e83bf6ace64"
      },
      "source": [
        "## 18 - Impact of long context  \n",
        "\n",
        "`Deep dive`\n",
        "\n",
        "https://www.youtube.com/watch?v=SsHUNfhF32s\n",
        "\n",
        "`Slides`\n",
        "\n",
        "https://docs.google.com/presentation/d/1mJUiPBdtf58NfuSEQ7pVSEQ2Oqmek7F1i4gBwR6JDss/edit#slide=id.g26c0cb8dc66_0_0"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}