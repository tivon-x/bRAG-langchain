{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d2441db-0ad8-40f2-b042-6bb96f184711",
   "metadata": {},
   "source": [
    "# bRAG: Query Transformations with Multi-Query\n",
    "\n",
    "Query transformations are a set of approaches focused on re-writing and / or modifying questions for retrieval.\n",
    "\n",
    "![overview](./image/query-overview.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9833575c",
   "metadata": {},
   "source": [
    "## Pre-requisites (optional but recommended)\n",
    "\n",
    "### Only do the first step if you have never created a virtual environment for this repository. Otherwise, make sure that the Python Kernel that you selected is from your `venv/` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "db495606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create virtual environment\n",
    "! python3 -m venv ../venv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "ef9f01b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activate virtual Python environment\n",
    "! source ../venv/bin/activate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "91091b10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/taha/Desktop/bRAGAI/code/gh/bRAG-langchain/venv/bin/python\n"
     ]
    }
   ],
   "source": [
    "# If your Python is not from your venv path, ensure that your IDE's kernel selection (on the top right corner) is set to the correct path \n",
    "# (your path output should contain \"...venv/bin/python\")\n",
    "\n",
    "! which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "1459b1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install all packages\n",
    "! pip3 install -r ../requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7ad3c7",
   "metadata": {},
   "source": [
    "### * If you choose to skip the pre-requisites and install only the packages specific to this notebook using your global Python path environment, execute the command below; otherwise, proceed to the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "b66a58b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip3 install --quiet pinecone-client python-dotenv langchain langchain-community langchain-core langchain-openai beautifulsoup4 tiktoken pypdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d972e08",
   "metadata": {},
   "source": [
    "## Environment\n",
    "\n",
    "`(1) Packages`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "33bd79d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load all environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Access the environment variables\n",
    "langchain_tracing_v2 = os.getenv('LANGCHAIN_TRACING_V2')\n",
    "langchain_endpoint = os.getenv('LANGCHAIN_ENDPOINT')\n",
    "langchain_api_key = os.getenv('LANGCHAIN_API_KEY')\n",
    "\n",
    "## LLM\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "## Pinecone Vector Database\n",
    "pinecone_api_key = os.getenv('PINECONE_API_KEY')\n",
    "pinecone_api_host = os.getenv('PINECONE_API_HOST')\n",
    "index_name = os.getenv('PINECONE_INDEX_NAME')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e08bb78-7e80-4d95-a124-33b695bf5e6a",
   "metadata": {},
   "source": [
    "`(2) LangSmith`\n",
    "\n",
    "https://docs.smith.langchain.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5258de38-0cc0-4d9d-a5ca-6e750ebe6976",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['LANGCHAIN_TRACING_V2'] = langchain_tracing_v2\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = langchain_endpoint\n",
    "os.environ['LANGCHAIN_API_KEY'] = langchain_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feaccdca-1ab0-43b1-82c2-22e9cd27675b",
   "metadata": {},
   "source": [
    "`(3) API Keys`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1cd6453b-2721-491c-b979-1860d58d8cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['OPENAI_API_KEY'] = openai_api_key\n",
    "openai_model = \"gpt-3.5-turbo\"\n",
    "\n",
    "#Pinecone keys\n",
    "os.environ['PINECONE_API_KEY'] = pinecone_api_key\n",
    "os.environ['PINECONE_API_HOST'] = pinecone_api_host\n",
    "os.environ['PINECONE_INDEX_NAME'] = index_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6383d63a",
   "metadata": {},
   "source": [
    "`(4) Pinecone Init`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "33aa3854",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone\n",
    "\n",
    "pc = Pinecone(api_key=os.environ['PINECONE_API_KEY'])\n",
    "index = pc.Index(os.environ['PINECONE_INDEX_NAME'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2365b3-b61b-4dbf-ab17-636cbfcaf9e0",
   "metadata": {},
   "source": [
    "## Multi Query RAG Architecture\n",
    "\n",
    "Flow:\n",
    "\n",
    "![multi-query](./image/multi-query.png)\n",
    "\n",
    "Docs:\n",
    "\n",
    "* https://python.langchain.com/docs/how_to/MultiQueryRetriever/\n",
    "\n",
    "### Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9d1b6e2b-dd76-410d-b870-23e02564a665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load blog\n",
    "import bs4\n",
    "from langchain_community.document_loaders import PyPDFLoader, PyPDFDirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Pinecone\n",
    "from pprint import pprint\n",
    "\n",
    "#### INDEXING ####\n",
    "\n",
    "# Load Document (Uploading one file at a time)\n",
    "pdf_file_path = \"../test/langchain_turing.pdf\"\n",
    "loader = PyPDFLoader(pdf_file_path)\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "# Upload muiltiple PDF files from a directory\n",
    "# pdf_file_paths = <enter your path here>\n",
    "# loader = PyPDFDirectoryLoader(pdf_file_paths)\n",
    "\n",
    "# docs_dir = loader.load()\n",
    "\n",
    "# Split\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=2000, \n",
    "    chunk_overlap=500)\n",
    "\n",
    "# Make splits\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# Index\n",
    "vectorstore = Pinecone.from_documents(\n",
    "    documents=splits, \n",
    "    embedding=OpenAIEmbeddings(model=\"text-embedding-3-large\"), \n",
    "    index_name=index_name\n",
    ")\n",
    "\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2754e5bd",
   "metadata": {},
   "source": [
    "Make sure your PineconeDB contains the uploaded file\n",
    "\n",
    "# ![Pinecone](./image/pinecone.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f1b6c5-faa9-404b-90c6-22d3b40169fa",
   "metadata": {},
   "source": [
    "### Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "965de464-0c98-4318-9f9e-f8a597c8d5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# Multi Query: Different Perspectives\n",
    "template = \"\"\"You are an AI language model assistant. Your task is to generate five \n",
    "different versions of the given user question to retrieve relevant documents from a vector \n",
    "database. By generating multiple perspectives on the user question, your goal is to help\n",
    "the user overcome some of the limitations of the distance-based similarity search. \n",
    "Provide these alternative questions separated by newlines. Original question: {question}\"\"\"\n",
    "prompt_perspectives = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "generate_queries = (\n",
    "    prompt_perspectives \n",
    "    | ChatOpenAI(model_name=openai_model, temperature=0.1) \n",
    "    | StrOutputParser() \n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4f253520-386f-434b-8daa-d6dadb89eddb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.load import dumps, loads\n",
    "\n",
    "def get_unique_union(documents: list[list]):\n",
    "    \"\"\" Unique union of retrieved docs \"\"\"\n",
    "    # Flatten list of lists, and convert each Document to string\n",
    "    flattened_docs = [dumps(doc) for sublist in documents for doc in sublist]\n",
    "    # Get unique documents\n",
    "    unique_docs = list(set(flattened_docs))\n",
    "    # Return\n",
    "    return [loads(doc) for doc in unique_docs]\n",
    "\n",
    "# Retrieve\n",
    "question = \"How does LangChain leverage modular components like LangGraph, LangSmith, and LangServe to address challenges in building scalable and secure LLM-powered applications?\"\n",
    "retrieval_chain = generate_queries | retriever.map() | get_unique_union\n",
    "docs = retrieval_chain.invoke({\"question\":question})\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "af6e74e8-ddae-4165-9e4b-0022ac125194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain leverages modular components like LangGraph, LangSmith, and LangServe to address challenges in building scalable and secure LLM-powered applications by providing developers with a comprehensive toolkit for simplifying the complexities of working with LLMs. These components enable developers to configure, extend, and deploy applications tailored to specific needs, facilitating the development of stateful, contextually aware applications. LangGraph aids in stateful process modeling, LangServe allows for scalable API deployment, and LangSmith provides monitoring and evaluation capabilities. Additionally, LangChain's emphasis on security through granular permissions, sandboxing, defense in depth, auditability, and monitoring helps mitigate risks associated with data exposure and dependency vulnerabilities. The framework also supports a wide range of third-party integrations, allowing for custom component development and additional functionality, ensuring flexibility and adaptability for various application requirements. Ultimately, LangChain's modular and flexible architecture enables the rapid deployment of sophisticated LLM solutions across diverse domains while addressing complexities and security concerns associated with large language models.\n"
     ]
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# RAG\n",
    "template = \"\"\"Answer the following question based on this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "llm = ChatOpenAI(model_name=openai_model, temperature=0.1)\n",
    "\n",
    "final_rag_chain = (\n",
    "    {\"context\": retrieval_chain, \n",
    "     \"question\": itemgetter(\"question\")} \n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(final_rag_chain.invoke({\"question\":question}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618afe4a-f1d6-433a-9d28-f1956c2b83ef",
   "metadata": {},
   "source": [
    "## Part 6: RAG-Fusion\n",
    "\n",
    "RAG Fusion is an advanced Retrieval-Augmented Generation approach that combines multiple retrieval sources, each specialized in a unique context, to generate a more accurate, contextually rich response. Unlike Regular Multi-Query, which independently executes and aggregates results without blending, RAG Fusion dynamically selects and integrates information from diverse sources, creating a unified, coherent answer that adapts to complex queries. This fusion of contextually relevant information enhances response robustness and relevance, making RAG Fusion particularly effective for handling multi-faceted information retrieval tasks.\n",
    "\n",
    "Flow:\n",
    "\n",
    "![rag-fusion](./image/rag-fusion.png)\n",
    "\n",
    "Docs:\n",
    "\n",
    "* https://github.com/langchain-ai/langchain/blob/master/cookbook/rag_fusion.ipynb?ref=blog.langchain.dev\n",
    "\n",
    "Blog / repo: \n",
    "\n",
    "* https://medium.com/towards-data-science/forget-rag-the-future-is-rag-fusion-1147298d8ad1\n",
    "\n",
    "### Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "34e7075b-b80d-461d-9e2e-e05e29436f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# RAG-Fusion: Related\n",
    "template = \"\"\"You are a helpful assistant that generates multiple search queries based on a single input query. \\n\n",
    "Generate multiple search queries related to: {question} \\n\n",
    "Output (4 queries):\"\"\"\n",
    "prompt_rag_fusion = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9781b40c-c408-42f4-ae14-cd11be513b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "generate_queries = (\n",
    "    prompt_rag_fusion \n",
    "    | ChatOpenAI(model=openai_model, temperature=0.1)\n",
    "    | StrOutputParser() \n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2b1adff1-e993-4747-b95d-656eaaeccfdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.load import dumps, loads\n",
    "\n",
    "def reciprocal_rank_fusion(results: list[list], k=60):\n",
    "    \"\"\" Reciprocal_rank_fusion that takes multiple lists of ranked documents \n",
    "        and an optional parameter k used in the RRF formula \"\"\"\n",
    "    \n",
    "    # Initialize a dictionary to hold fused scores for each unique document\n",
    "    fused_scores = {}\n",
    "\n",
    "    # Iterate through each list of ranked documents\n",
    "    for docs in results:\n",
    "        # Iterate through each document in the list, with its rank (position in the list)\n",
    "        for rank, doc in enumerate(docs):\n",
    "            # Convert the document to a string format to use as a key (assumes documents can be serialized to JSON)\n",
    "            doc_str = dumps(doc)\n",
    "            # If the document is not yet in the fused_scores dictionary, add it with an initial score of 0\n",
    "            if doc_str not in fused_scores:\n",
    "                fused_scores[doc_str] = 0\n",
    "            # Retrieve the current score of the document, if any\n",
    "            previous_score = fused_scores[doc_str]\n",
    "            # Update the score of the document using the RRF formula: 1 / (rank + k)\n",
    "            fused_scores[doc_str] += 1 / (rank + k)\n",
    "\n",
    "    # Sort the documents based on their fused scores in descending order to get the final reranked results\n",
    "    reranked_results = [\n",
    "        (loads(doc), score)\n",
    "        for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    ]\n",
    "\n",
    "    # Return the reranked results as a list of tuples, each containing the document and its fused score\n",
    "    return reranked_results\n",
    "\n",
    "retrieval_chain_rag_fusion = generate_queries | retriever.map() | reciprocal_rank_fusion\n",
    "docs = retrieval_chain_rag_fusion.invoke({\"question\": question})\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ce2adf2d-3d9f-4d43-afb0-8304edcfb1f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('LangChain leverages modular components like LangGraph, LangSmith, and '\n",
      " 'LangServe to address challenges in building scalable and secure LLM-powered '\n",
      " 'applications by providing developers with a comprehensive toolkit for '\n",
      " 'building, deploying, and monitoring applications. LangGraph enables stateful '\n",
      " 'process modeling, LangServe facilitates the deployment of LLM applications '\n",
      " 'as scalable REST APIs, and LangSmith provides monitoring and evaluation '\n",
      " 'capabilities. These components work together to enable developers to build '\n",
      " 'context-aware applications tailored to specific needs across diverse '\n",
      " 'domains, including NLP, cybersecurity, healthcare, finance, and customer '\n",
      " \"service. Additionally, LangChain's emphasis on flexibility allows for custom \"\n",
      " 'component development and integration with third-party tools, enhancing the '\n",
      " 'functionality and adaptability of applications. The security features in '\n",
      " 'LangChain, such as granular permissions, sandboxing, real-time monitoring, '\n",
      " 'and encryption practices, further contribute to ensuring the security of '\n",
      " 'LLM-driven applications, particularly in sectors with rigorous compliance '\n",
      " 'standards. By balancing robust capabilities with enhancements in usability '\n",
      " 'and security, LangChain serves as a valuable tool for developers seeking to '\n",
      " 'leverage LLMs in building innovative and secure applications.')\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# RAG\n",
    "template = \"\"\"Answer the following question based on this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "final_rag_chain = (\n",
    "    {\"context\": retrieval_chain_rag_fusion, \n",
    "     \"question\": itemgetter(\"question\")} \n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "pprint(final_rag_chain.invoke({\"question\":question}))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "94c812d3-4d91-4634-8301-0b68be88a887",
   "metadata": {},
   "source": [
    "## RAG Decomposition Architecture\n",
    "\n",
    "RAG Decomposition Architecture is a specialized framework within Retrieval-Augmented Generation designed to break down complex queries into simpler, manageable sub-queries. Each sub-query focuses on a specific part of the larger question and is sent to specialized retrievers or databases to gather precise information. These sub-results are then combined and synthesized to form a cohesive, comprehensive answer to the original query. This architecture enhances retrieval accuracy, as each sub-query targets a specific context, reducing noise and improving relevance in the final response. RAG Decomposition is particularly useful for multi-part questions, complex topics, or scenarios requiring in-depth, granular answers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19543d04-ff31-4774-b89c-9d31f5a28fc9",
   "metadata": {},
   "source": [
    "### Answer recursively  \n",
    "\n",
    "![answer-recursively](./image/answer-recursively.png)\n",
    "\n",
    "Papers:\n",
    "\n",
    "* https://arxiv.org/pdf/2205.10625.pdf\n",
    "* https://arxiv.org/abs/2212.10509.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f82fac99-58dc-4bb9-84e6-51180db855ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# Decomposition\n",
    "template = \"\"\"You are a helpful assistant that generates multiple sub-questions related to an input question. \\n\n",
    "The goal is to break down the input into a set of sub-problems / sub-questions that can be answers in isolation. \\n\n",
    "Generate multiple search queries related to: {question} \\n\n",
    "Output (3 queries):\"\"\"\n",
    "prompt_decomposition = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c31eefd9-5598-44a1-b0d6-dd04553a3eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# LLM\n",
    "llm = ChatOpenAI(model=openai_model, temperature=0.1)\n",
    "\n",
    "# Chain\n",
    "generate_queries_decomposition = ( prompt_decomposition | llm | StrOutputParser() | (lambda x: x.split(\"\\n\")))\n",
    "\n",
    "# Run\n",
    "question = \"What role does LangChain's Retrieval-Augmented Generation (RAG) pipeline play in improving the accuracy and relevance of LLM responses?\"\n",
    "questions = generate_queries_decomposition.invoke({\"question\":question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "07191b5c-cf72-4b8f-a225-f57dfdc2fc78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"1. How does LangChain's Retrieval-Augmented Generation (RAG) pipeline incorporate external knowledge sources to enhance the accuracy of LLM responses?\",\n",
       " \"2. What specific techniques are used in LangChain's RAG pipeline to ensure the relevance of LLM responses to the input query?\",\n",
       " \"3. Can LangChain's RAG pipeline be customized or fine-tuned to improve the accuracy and relevance of LLM responses for specific domains or industries?\"]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c72bbd12-f85c-4ed0-9dfa-8503afebfafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt\n",
    "template = \"\"\"Here is the question you need to answer:\n",
    "\n",
    "\\n --- \\n {question} \\n --- \\n\n",
    "\n",
    "Here is any available background question + answer pairs:\n",
    "\n",
    "\\n --- \\n {q_a_pairs} \\n --- \\n\n",
    "\n",
    "Here is additional context relevant to the question: \n",
    "\n",
    "\\n --- \\n {context} \\n --- \\n\n",
    "\n",
    "Use the above context and any background question + answer pairs to answer the question: \\n {question}\n",
    "\"\"\"\n",
    "\n",
    "decomposition_prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a20bf0d4-f567-4451-834d-a07190a3185e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "def format_qa_pair(question, answer):\n",
    "    \"\"\"Format Q and A pair\"\"\"\n",
    "    \n",
    "    formatted_string = \"\"\n",
    "    formatted_string += f\"Question: {question}\\nAnswer: {answer}\\n\\n\"\n",
    "    return formatted_string.strip()\n",
    "\n",
    "# llm\n",
    "llm = ChatOpenAI(model_name=openai_model, temperature=0.1)\n",
    "\n",
    "q_a_pairs = \"\"\n",
    "for q in questions:\n",
    "    \n",
    "    rag_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | retriever, \n",
    "     \"question\": itemgetter(\"question\"),\n",
    "     \"q_a_pairs\": itemgetter(\"q_a_pairs\")} \n",
    "    | decomposition_prompt\n",
    "    | llm\n",
    "    | StrOutputParser())\n",
    "\n",
    "    answer = rag_chain.invoke({\"question\":q,\"q_a_pairs\":q_a_pairs})\n",
    "    q_a_pair = format_qa_pair(q,answer)\n",
    "    q_a_pairs = q_a_pairs + \"\\n---\\n\"+  q_a_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e6070fea-ffcf-49ca-ac99-7d7ed2744d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"Yes, LangChain's RAG pipeline can be customized or fine-tuned to improve the \"\n",
      " 'accuracy and relevance of Large Language Model (LLM) responses for specific '\n",
      " 'domains or industries. The RAG pipeline incorporates techniques such as '\n",
      " 'Document Loaders and Text Splitters, Embedding Models and Vector Stores, and '\n",
      " 'Retrievers and RAG Chains to ensure relevance and accuracy in responses. By '\n",
      " 'customizing the preprocessing of documents, embedding models, and retrieval '\n",
      " 'of external data sources, developers can tailor the RAG pipeline to specific '\n",
      " 'domains or industries, enhancing the quality of generated answers. '\n",
      " \"Additionally, LangChain's architecture supports custom component development \"\n",
      " 'and integrations, allowing for flexibility and adaptability to meet the '\n",
      " 'requirements of different application scenarios.')\n"
     ]
    }
   ],
   "source": [
    "pprint(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0fa2e4-d4f1-42fc-a1ad-8eaeb05a0d3e",
   "metadata": {},
   "source": [
    "### Answer individually\n",
    "\n",
    "Alternatively, we can take the answers from each individual query and pass it on directly to the LLM to generate a final answer given the previous answers as context.\n",
    "\n",
    "![answer-individually](./image/answer-individually.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "297425fa-975b-4599-9b9e-a11139b99140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sq : 1. How does LangChain's Retrieval-Augmented Generation (RAG) pipeline incorporate external knowledge sources to enhance the accuracy of LLM responses?\n",
      "rdocs:  [Document(metadata={'page': 2.0, 'source': '../test/langchain_turing.pdf'}, page_content='LangChain 3\\nneeds, providing a flexible foundation for building scalable, secure, and multi-\\nfunctional applications. Figure 1 illustrates a fundamental LangChain pipeline.\\nIn this architecture, diverse data sources—including documents, text, and im-\\nages—are embedded and stored within a vector store. Upon receiving a user’s\\nquery, the system retrieves the most relevant information from the vector store.\\nThis retrieved context is then provided to the large language model (LLM),\\nenhancing its ability to generate accurate and factually grounded responses.\\nFig. 1.LangChain pipeline architecture showcasing the retrieval-augmented genera-\\ntion process. Documents in various formats (e.g., PDF, text, images) are preloaded\\nand embedded into a vector store. When a user submits a query, the system retrieves\\nthe top-k most relevant documents based on vector similarity. These documents are\\ncombined with the query to provide contextual information to the language model\\n(LLM), which then generates an accurate and contextually enriched answer. This ar-\\nchitecture enhances the model’s ability to produce factually grounded responses by\\nincorporating relevant knowledge from the vector store.\\nThe rest of this section provides an overview of LangChain’s primary com-\\nponents, followed by a brief introduction to its advanced modules–LangSmith,\\nLangGraph and LangServe–which are further discussed in Sections 2, 3, and 4\\nrespectively:\\nLLM Interface: Provides APIs for connecting and querying various large lan-\\nguage models, such as OpenAI’s GPT [1], Google’s Gemini [14], and Llama [16],\\nto facilitate seamless application integration.\\nPromptTemplates:Structuredtemplatesthatstandardizeandformatqueries,\\nensuring consistency and precision in interactions with AI models. These tem-\\nplates help guide the model towards producing reliable and relevant outputs.'), Document(metadata={'page': 2.0, 'source': '../test/langchain_turing.pdf'}, page_content='LangChain 3\\nneeds, providing a flexible foundation for building scalable, secure, and multi-\\nfunctional applications. Figure 1 illustrates a fundamental LangChain pipeline.\\nIn this architecture, diverse data sources—including documents, text, and im-\\nages—are embedded and stored within a vector store. Upon receiving a user’s\\nquery, the system retrieves the most relevant information from the vector store.\\nThis retrieved context is then provided to the large language model (LLM),\\nenhancing its ability to generate accurate and factually grounded responses.\\nFig. 1.LangChain pipeline architecture showcasing the retrieval-augmented genera-\\ntion process. Documents in various formats (e.g., PDF, text, images) are preloaded\\nand embedded into a vector store. When a user submits a query, the system retrieves\\nthe top-k most relevant documents based on vector similarity. These documents are\\ncombined with the query to provide contextual information to the language model\\n(LLM), which then generates an accurate and contextually enriched answer. This ar-\\nchitecture enhances the model’s ability to produce factually grounded responses by\\nincorporating relevant knowledge from the vector store.\\nThe rest of this section provides an overview of LangChain’s primary com-\\nponents, followed by a brief introduction to its advanced modules–LangSmith,\\nLangGraph and LangServe–which are further discussed in Sections 2, 3, and 4\\nrespectively:\\nLLM Interface: Provides APIs for connecting and querying various large lan-\\nguage models, such as OpenAI’s GPT [1], Google’s Gemini [14], and Llama [16],\\nto facilitate seamless application integration.\\nPromptTemplates:Structuredtemplatesthatstandardizeandformatqueries,\\nensuring consistency and precision in interactions with AI models. These tem-\\nplates help guide the model towards producing reliable and relevant outputs.'), Document(metadata={'page': 2.0, 'source': '../test/langchain_turing.pdf'}, page_content='LangChain 3\\nneeds, providing a flexible foundation for building scalable, secure, and multi-\\nfunctional applications. Figure 1 illustrates a fundamental LangChain pipeline.\\nIn this architecture, diverse data sources—including documents, text, and im-\\nages—are embedded and stored within a vector store. Upon receiving a user’s\\nquery, the system retrieves the most relevant information from the vector store.\\nThis retrieved context is then provided to the large language model (LLM),\\nenhancing its ability to generate accurate and factually grounded responses.\\nFig. 1.LangChain pipeline architecture showcasing the retrieval-augmented genera-\\ntion process. Documents in various formats (e.g., PDF, text, images) are preloaded\\nand embedded into a vector store. When a user submits a query, the system retrieves\\nthe top-k most relevant documents based on vector similarity. These documents are\\ncombined with the query to provide contextual information to the language model\\n(LLM), which then generates an accurate and contextually enriched answer. This ar-\\nchitecture enhances the model’s ability to produce factually grounded responses by\\nincorporating relevant knowledge from the vector store.\\nThe rest of this section provides an overview of LangChain’s primary com-\\nponents, followed by a brief introduction to its advanced modules–LangSmith,\\nLangGraph and LangServe–which are further discussed in Sections 2, 3, and 4\\nrespectively:\\nLLM Interface: Provides APIs for connecting and querying various large lan-\\nguage models, such as OpenAI’s GPT [1], Google’s Gemini [14], and Llama [16],\\nto facilitate seamless application integration.\\nPromptTemplates:Structuredtemplatesthatstandardizeandformatqueries,\\nensuring consistency and precision in interactions with AI models. These tem-\\nplates help guide the model towards producing reliable and relevant outputs.'), Document(metadata={'page': 2.0, 'source': '../test/langchain_turing.pdf'}, page_content='LangChain 3\\nneeds, providing a flexible foundation for building scalable, secure, and multi-\\nfunctional applications. Figure 1 illustrates a fundamental LangChain pipeline.\\nIn this architecture, diverse data sources—including documents, text, and im-\\nages—are embedded and stored within a vector store. Upon receiving a user’s\\nquery, the system retrieves the most relevant information from the vector store.\\nThis retrieved context is then provided to the large language model (LLM),\\nenhancing its ability to generate accurate and factually grounded responses.\\nFig. 1.LangChain pipeline architecture showcasing the retrieval-augmented genera-\\ntion process. Documents in various formats (e.g., PDF, text, images) are preloaded\\nand embedded into a vector store. When a user submits a query, the system retrieves\\nthe top-k most relevant documents based on vector similarity. These documents are\\ncombined with the query to provide contextual information to the language model\\n(LLM), which then generates an accurate and contextually enriched answer. This ar-\\nchitecture enhances the model’s ability to produce factually grounded responses by\\nincorporating relevant knowledge from the vector store.\\nThe rest of this section provides an overview of LangChain’s primary com-\\nponents, followed by a brief introduction to its advanced modules–LangSmith,\\nLangGraph and LangServe–which are further discussed in Sections 2, 3, and 4\\nrespectively:\\nLLM Interface: Provides APIs for connecting and querying various large lan-\\nguage models, such as OpenAI’s GPT [1], Google’s Gemini [14], and Llama [16],\\nto facilitate seamless application integration.\\nPromptTemplates:Structuredtemplatesthatstandardizeandformatqueries,\\nensuring consistency and precision in interactions with AI models. These tem-\\nplates help guide the model towards producing reliable and relevant outputs.')]\n",
      "q:  What role does LangChain's Retrieval-Augmented Generation (RAG) pipeline play in improving the accuracy and relevance of LLM responses?\n",
      "a:  LangChain's RAG pipeline embeds diverse data sources into a vector store. When a user submits a query, the system retrieves the most relevant information from this vector store. This retrieved context is then provided to the large language model (LLM), improving its ability to generate accurate and factually grounded responses.\n",
      "sq : 2. What specific techniques does LangChain's RAG pipeline use to ensure the relevance of LLM responses to user queries?\n",
      "rdocs:  [Document(metadata={'page': 4.0, 'source': '../test/langchain_turing.pdf'}, page_content='LangChain 5\\nand relevance. RAG allows models to access up-to-date information, extending\\ntheir capabilities beyond their training data. LangChain’s RAG implementation\\nuses:\\n– Document Loaders and Text Splitters: Preprocess documents for in-\\ndexing and efficient retrieval [6].\\n– Embedding Models and Vector Stores: Enable similarity-based re-\\ntrieval by embedding documents into vector spaces. LangChain integrates\\nwithvectorstoragesolutionslikeChromaandMilvusforoptimizedsearches[3].\\n– Retrievers and RAG Chains: Retrieve and merge external data with\\nmodel responses, enhancing applications such as question answering systems\\nand recommendation engines [4].\\n1.3 Security and Permissions Management\\nSecurity is a critical focus in LangChain’s design, particularly given the potential\\naccess to external data sources. LangChain addresses these security challenges\\nthrough best practices and internal controls [3]:\\n– Granular Permissions: Enforces the principle of least privilege by allowing\\ndevelopers to specify limited permissions, minimizing the risk of unautho-\\nrized actions.\\n– Sandboxing and Defense in Depth: Utilizes sandboxed environments\\nand layered security to protect sensitive data and limit exposure to vulner-\\nabilities [3].\\n– Auditability and Monitoring: LangSmith (see Section 2) provides de-\\ntailed logging and monitoring capabilities, enabling developers to track ap-\\nplication usage and detect anomalies in real time.\\n1.4 Integrations and Extensibility\\nLangChain’s architecture supports a wide range of third-party integrations, al-\\nlowing for custom component development and additional functionality, such as\\nmulti-modal data processing and AI tool integration [3]:\\n– IntegrationPackages:LangChainprovidesdedicatedpackages(e.g.,langchain-\\nopenai, langchain-aws) that simplify connections to external platforms, tai-\\nloring applications to specific needs.\\n– Support for Multi-modal Data: Supports image, text, and audio inputs,\\nallowing for applications like chatbots capable of interpreting diverse data\\ntypes.\\n– CustomComponentDevelopment :Developerscanbuildcustomplugins\\nor extend LangChain components, ensuring flexibility and adaptability for\\na wide range of application requirements.\\nLangChain’s modular and flexible architecture equips developers with a com-\\nprehensive toolkit for building, deploying, and monitoring LLM applications. Its\\nadvanced components—LangGraph, LangServe, and LangSmith—enable sophis-\\nticated functionality for scalable, interactive, and robust applications, meeting\\nthe demands of modern AI use cases.'), Document(metadata={'page': 4.0, 'source': '../test/langchain_turing.pdf'}, page_content='LangChain 5\\nand relevance. RAG allows models to access up-to-date information, extending\\ntheir capabilities beyond their training data. LangChain’s RAG implementation\\nuses:\\n– Document Loaders and Text Splitters: Preprocess documents for in-\\ndexing and efficient retrieval [6].\\n– Embedding Models and Vector Stores: Enable similarity-based re-\\ntrieval by embedding documents into vector spaces. LangChain integrates\\nwithvectorstoragesolutionslikeChromaandMilvusforoptimizedsearches[3].\\n– Retrievers and RAG Chains: Retrieve and merge external data with\\nmodel responses, enhancing applications such as question answering systems\\nand recommendation engines [4].\\n1.3 Security and Permissions Management\\nSecurity is a critical focus in LangChain’s design, particularly given the potential\\naccess to external data sources. LangChain addresses these security challenges\\nthrough best practices and internal controls [3]:\\n– Granular Permissions: Enforces the principle of least privilege by allowing\\ndevelopers to specify limited permissions, minimizing the risk of unautho-\\nrized actions.\\n– Sandboxing and Defense in Depth: Utilizes sandboxed environments\\nand layered security to protect sensitive data and limit exposure to vulner-\\nabilities [3].\\n– Auditability and Monitoring: LangSmith (see Section 2) provides de-\\ntailed logging and monitoring capabilities, enabling developers to track ap-\\nplication usage and detect anomalies in real time.\\n1.4 Integrations and Extensibility\\nLangChain’s architecture supports a wide range of third-party integrations, al-\\nlowing for custom component development and additional functionality, such as\\nmulti-modal data processing and AI tool integration [3]:\\n– IntegrationPackages:LangChainprovidesdedicatedpackages(e.g.,langchain-\\nopenai, langchain-aws) that simplify connections to external platforms, tai-\\nloring applications to specific needs.\\n– Support for Multi-modal Data: Supports image, text, and audio inputs,\\nallowing for applications like chatbots capable of interpreting diverse data\\ntypes.\\n– CustomComponentDevelopment :Developerscanbuildcustomplugins\\nor extend LangChain components, ensuring flexibility and adaptability for\\na wide range of application requirements.\\nLangChain’s modular and flexible architecture equips developers with a com-\\nprehensive toolkit for building, deploying, and monitoring LLM applications. Its\\nadvanced components—LangGraph, LangServe, and LangSmith—enable sophis-\\nticated functionality for scalable, interactive, and robust applications, meeting\\nthe demands of modern AI use cases.'), Document(metadata={'page': 4.0, 'source': '../test/langchain_turing.pdf'}, page_content='LangChain 5\\nand relevance. RAG allows models to access up-to-date information, extending\\ntheir capabilities beyond their training data. LangChain’s RAG implementation\\nuses:\\n– Document Loaders and Text Splitters: Preprocess documents for in-\\ndexing and efficient retrieval [6].\\n– Embedding Models and Vector Stores: Enable similarity-based re-\\ntrieval by embedding documents into vector spaces. LangChain integrates\\nwithvectorstoragesolutionslikeChromaandMilvusforoptimizedsearches[3].\\n– Retrievers and RAG Chains: Retrieve and merge external data with\\nmodel responses, enhancing applications such as question answering systems\\nand recommendation engines [4].\\n1.3 Security and Permissions Management\\nSecurity is a critical focus in LangChain’s design, particularly given the potential\\naccess to external data sources. LangChain addresses these security challenges\\nthrough best practices and internal controls [3]:\\n– Granular Permissions: Enforces the principle of least privilege by allowing\\ndevelopers to specify limited permissions, minimizing the risk of unautho-\\nrized actions.\\n– Sandboxing and Defense in Depth: Utilizes sandboxed environments\\nand layered security to protect sensitive data and limit exposure to vulner-\\nabilities [3].\\n– Auditability and Monitoring: LangSmith (see Section 2) provides de-\\ntailed logging and monitoring capabilities, enabling developers to track ap-\\nplication usage and detect anomalies in real time.\\n1.4 Integrations and Extensibility\\nLangChain’s architecture supports a wide range of third-party integrations, al-\\nlowing for custom component development and additional functionality, such as\\nmulti-modal data processing and AI tool integration [3]:\\n– IntegrationPackages:LangChainprovidesdedicatedpackages(e.g.,langchain-\\nopenai, langchain-aws) that simplify connections to external platforms, tai-\\nloring applications to specific needs.\\n– Support for Multi-modal Data: Supports image, text, and audio inputs,\\nallowing for applications like chatbots capable of interpreting diverse data\\ntypes.\\n– CustomComponentDevelopment :Developerscanbuildcustomplugins\\nor extend LangChain components, ensuring flexibility and adaptability for\\na wide range of application requirements.\\nLangChain’s modular and flexible architecture equips developers with a com-\\nprehensive toolkit for building, deploying, and monitoring LLM applications. Its\\nadvanced components—LangGraph, LangServe, and LangSmith—enable sophis-\\nticated functionality for scalable, interactive, and robust applications, meeting\\nthe demands of modern AI use cases.'), Document(metadata={'page': 4.0, 'source': '../test/langchain_turing.pdf'}, page_content='LangChain 5\\nand relevance. RAG allows models to access up-to-date information, extending\\ntheir capabilities beyond their training data. LangChain’s RAG implementation\\nuses:\\n– Document Loaders and Text Splitters: Preprocess documents for in-\\ndexing and efficient retrieval [6].\\n– Embedding Models and Vector Stores: Enable similarity-based re-\\ntrieval by embedding documents into vector spaces. LangChain integrates\\nwithvectorstoragesolutionslikeChromaandMilvusforoptimizedsearches[3].\\n– Retrievers and RAG Chains: Retrieve and merge external data with\\nmodel responses, enhancing applications such as question answering systems\\nand recommendation engines [4].\\n1.3 Security and Permissions Management\\nSecurity is a critical focus in LangChain’s design, particularly given the potential\\naccess to external data sources. LangChain addresses these security challenges\\nthrough best practices and internal controls [3]:\\n– Granular Permissions: Enforces the principle of least privilege by allowing\\ndevelopers to specify limited permissions, minimizing the risk of unautho-\\nrized actions.\\n– Sandboxing and Defense in Depth: Utilizes sandboxed environments\\nand layered security to protect sensitive data and limit exposure to vulner-\\nabilities [3].\\n– Auditability and Monitoring: LangSmith (see Section 2) provides de-\\ntailed logging and monitoring capabilities, enabling developers to track ap-\\nplication usage and detect anomalies in real time.\\n1.4 Integrations and Extensibility\\nLangChain’s architecture supports a wide range of third-party integrations, al-\\nlowing for custom component development and additional functionality, such as\\nmulti-modal data processing and AI tool integration [3]:\\n– IntegrationPackages:LangChainprovidesdedicatedpackages(e.g.,langchain-\\nopenai, langchain-aws) that simplify connections to external platforms, tai-\\nloring applications to specific needs.\\n– Support for Multi-modal Data: Supports image, text, and audio inputs,\\nallowing for applications like chatbots capable of interpreting diverse data\\ntypes.\\n– CustomComponentDevelopment :Developerscanbuildcustomplugins\\nor extend LangChain components, ensuring flexibility and adaptability for\\na wide range of application requirements.\\nLangChain’s modular and flexible architecture equips developers with a com-\\nprehensive toolkit for building, deploying, and monitoring LLM applications. Its\\nadvanced components—LangGraph, LangServe, and LangSmith—enable sophis-\\nticated functionality for scalable, interactive, and robust applications, meeting\\nthe demands of modern AI use cases.')]\n",
      "q:  What role does LangChain's Retrieval-Augmented Generation (RAG) pipeline play in improving the accuracy and relevance of LLM responses?\n",
      "a:  LangChain's RAG pipeline ensures relevance by using Document Loaders and Text Splitters for preprocessing, Embedding Models and Vector Stores for similarity-based retrieval, and Retrievers and RAG Chains to merge external data with model responses. These techniques enhance applications like question answering systems and recommendation engines.\n",
      "sq : 3. Can LangChain's RAG pipeline be customized or fine-tuned to improve the accuracy and relevance of LLM responses for specific domains or industries?\n",
      "rdocs:  [Document(metadata={'page': 4.0, 'source': '../test/langchain_turing.pdf'}, page_content='LangChain 5\\nand relevance. RAG allows models to access up-to-date information, extending\\ntheir capabilities beyond their training data. LangChain’s RAG implementation\\nuses:\\n– Document Loaders and Text Splitters: Preprocess documents for in-\\ndexing and efficient retrieval [6].\\n– Embedding Models and Vector Stores: Enable similarity-based re-\\ntrieval by embedding documents into vector spaces. LangChain integrates\\nwithvectorstoragesolutionslikeChromaandMilvusforoptimizedsearches[3].\\n– Retrievers and RAG Chains: Retrieve and merge external data with\\nmodel responses, enhancing applications such as question answering systems\\nand recommendation engines [4].\\n1.3 Security and Permissions Management\\nSecurity is a critical focus in LangChain’s design, particularly given the potential\\naccess to external data sources. LangChain addresses these security challenges\\nthrough best practices and internal controls [3]:\\n– Granular Permissions: Enforces the principle of least privilege by allowing\\ndevelopers to specify limited permissions, minimizing the risk of unautho-\\nrized actions.\\n– Sandboxing and Defense in Depth: Utilizes sandboxed environments\\nand layered security to protect sensitive data and limit exposure to vulner-\\nabilities [3].\\n– Auditability and Monitoring: LangSmith (see Section 2) provides de-\\ntailed logging and monitoring capabilities, enabling developers to track ap-\\nplication usage and detect anomalies in real time.\\n1.4 Integrations and Extensibility\\nLangChain’s architecture supports a wide range of third-party integrations, al-\\nlowing for custom component development and additional functionality, such as\\nmulti-modal data processing and AI tool integration [3]:\\n– IntegrationPackages:LangChainprovidesdedicatedpackages(e.g.,langchain-\\nopenai, langchain-aws) that simplify connections to external platforms, tai-\\nloring applications to specific needs.\\n– Support for Multi-modal Data: Supports image, text, and audio inputs,\\nallowing for applications like chatbots capable of interpreting diverse data\\ntypes.\\n– CustomComponentDevelopment :Developerscanbuildcustomplugins\\nor extend LangChain components, ensuring flexibility and adaptability for\\na wide range of application requirements.\\nLangChain’s modular and flexible architecture equips developers with a com-\\nprehensive toolkit for building, deploying, and monitoring LLM applications. Its\\nadvanced components—LangGraph, LangServe, and LangSmith—enable sophis-\\nticated functionality for scalable, interactive, and robust applications, meeting\\nthe demands of modern AI use cases.'), Document(metadata={'page': 4.0, 'source': '../test/langchain_turing.pdf'}, page_content='LangChain 5\\nand relevance. RAG allows models to access up-to-date information, extending\\ntheir capabilities beyond their training data. LangChain’s RAG implementation\\nuses:\\n– Document Loaders and Text Splitters: Preprocess documents for in-\\ndexing and efficient retrieval [6].\\n– Embedding Models and Vector Stores: Enable similarity-based re-\\ntrieval by embedding documents into vector spaces. LangChain integrates\\nwithvectorstoragesolutionslikeChromaandMilvusforoptimizedsearches[3].\\n– Retrievers and RAG Chains: Retrieve and merge external data with\\nmodel responses, enhancing applications such as question answering systems\\nand recommendation engines [4].\\n1.3 Security and Permissions Management\\nSecurity is a critical focus in LangChain’s design, particularly given the potential\\naccess to external data sources. LangChain addresses these security challenges\\nthrough best practices and internal controls [3]:\\n– Granular Permissions: Enforces the principle of least privilege by allowing\\ndevelopers to specify limited permissions, minimizing the risk of unautho-\\nrized actions.\\n– Sandboxing and Defense in Depth: Utilizes sandboxed environments\\nand layered security to protect sensitive data and limit exposure to vulner-\\nabilities [3].\\n– Auditability and Monitoring: LangSmith (see Section 2) provides de-\\ntailed logging and monitoring capabilities, enabling developers to track ap-\\nplication usage and detect anomalies in real time.\\n1.4 Integrations and Extensibility\\nLangChain’s architecture supports a wide range of third-party integrations, al-\\nlowing for custom component development and additional functionality, such as\\nmulti-modal data processing and AI tool integration [3]:\\n– IntegrationPackages:LangChainprovidesdedicatedpackages(e.g.,langchain-\\nopenai, langchain-aws) that simplify connections to external platforms, tai-\\nloring applications to specific needs.\\n– Support for Multi-modal Data: Supports image, text, and audio inputs,\\nallowing for applications like chatbots capable of interpreting diverse data\\ntypes.\\n– CustomComponentDevelopment :Developerscanbuildcustomplugins\\nor extend LangChain components, ensuring flexibility and adaptability for\\na wide range of application requirements.\\nLangChain’s modular and flexible architecture equips developers with a com-\\nprehensive toolkit for building, deploying, and monitoring LLM applications. Its\\nadvanced components—LangGraph, LangServe, and LangSmith—enable sophis-\\nticated functionality for scalable, interactive, and robust applications, meeting\\nthe demands of modern AI use cases.'), Document(metadata={'page': 4.0, 'source': '../test/langchain_turing.pdf'}, page_content='LangChain 5\\nand relevance. RAG allows models to access up-to-date information, extending\\ntheir capabilities beyond their training data. LangChain’s RAG implementation\\nuses:\\n– Document Loaders and Text Splitters: Preprocess documents for in-\\ndexing and efficient retrieval [6].\\n– Embedding Models and Vector Stores: Enable similarity-based re-\\ntrieval by embedding documents into vector spaces. LangChain integrates\\nwithvectorstoragesolutionslikeChromaandMilvusforoptimizedsearches[3].\\n– Retrievers and RAG Chains: Retrieve and merge external data with\\nmodel responses, enhancing applications such as question answering systems\\nand recommendation engines [4].\\n1.3 Security and Permissions Management\\nSecurity is a critical focus in LangChain’s design, particularly given the potential\\naccess to external data sources. LangChain addresses these security challenges\\nthrough best practices and internal controls [3]:\\n– Granular Permissions: Enforces the principle of least privilege by allowing\\ndevelopers to specify limited permissions, minimizing the risk of unautho-\\nrized actions.\\n– Sandboxing and Defense in Depth: Utilizes sandboxed environments\\nand layered security to protect sensitive data and limit exposure to vulner-\\nabilities [3].\\n– Auditability and Monitoring: LangSmith (see Section 2) provides de-\\ntailed logging and monitoring capabilities, enabling developers to track ap-\\nplication usage and detect anomalies in real time.\\n1.4 Integrations and Extensibility\\nLangChain’s architecture supports a wide range of third-party integrations, al-\\nlowing for custom component development and additional functionality, such as\\nmulti-modal data processing and AI tool integration [3]:\\n– IntegrationPackages:LangChainprovidesdedicatedpackages(e.g.,langchain-\\nopenai, langchain-aws) that simplify connections to external platforms, tai-\\nloring applications to specific needs.\\n– Support for Multi-modal Data: Supports image, text, and audio inputs,\\nallowing for applications like chatbots capable of interpreting diverse data\\ntypes.\\n– CustomComponentDevelopment :Developerscanbuildcustomplugins\\nor extend LangChain components, ensuring flexibility and adaptability for\\na wide range of application requirements.\\nLangChain’s modular and flexible architecture equips developers with a com-\\nprehensive toolkit for building, deploying, and monitoring LLM applications. Its\\nadvanced components—LangGraph, LangServe, and LangSmith—enable sophis-\\nticated functionality for scalable, interactive, and robust applications, meeting\\nthe demands of modern AI use cases.'), Document(metadata={'page': 4.0, 'source': '../test/langchain_turing.pdf'}, page_content='LangChain 5\\nand relevance. RAG allows models to access up-to-date information, extending\\ntheir capabilities beyond their training data. LangChain’s RAG implementation\\nuses:\\n– Document Loaders and Text Splitters: Preprocess documents for in-\\ndexing and efficient retrieval [6].\\n– Embedding Models and Vector Stores: Enable similarity-based re-\\ntrieval by embedding documents into vector spaces. LangChain integrates\\nwithvectorstoragesolutionslikeChromaandMilvusforoptimizedsearches[3].\\n– Retrievers and RAG Chains: Retrieve and merge external data with\\nmodel responses, enhancing applications such as question answering systems\\nand recommendation engines [4].\\n1.3 Security and Permissions Management\\nSecurity is a critical focus in LangChain’s design, particularly given the potential\\naccess to external data sources. LangChain addresses these security challenges\\nthrough best practices and internal controls [3]:\\n– Granular Permissions: Enforces the principle of least privilege by allowing\\ndevelopers to specify limited permissions, minimizing the risk of unautho-\\nrized actions.\\n– Sandboxing and Defense in Depth: Utilizes sandboxed environments\\nand layered security to protect sensitive data and limit exposure to vulner-\\nabilities [3].\\n– Auditability and Monitoring: LangSmith (see Section 2) provides de-\\ntailed logging and monitoring capabilities, enabling developers to track ap-\\nplication usage and detect anomalies in real time.\\n1.4 Integrations and Extensibility\\nLangChain’s architecture supports a wide range of third-party integrations, al-\\nlowing for custom component development and additional functionality, such as\\nmulti-modal data processing and AI tool integration [3]:\\n– IntegrationPackages:LangChainprovidesdedicatedpackages(e.g.,langchain-\\nopenai, langchain-aws) that simplify connections to external platforms, tai-\\nloring applications to specific needs.\\n– Support for Multi-modal Data: Supports image, text, and audio inputs,\\nallowing for applications like chatbots capable of interpreting diverse data\\ntypes.\\n– CustomComponentDevelopment :Developerscanbuildcustomplugins\\nor extend LangChain components, ensuring flexibility and adaptability for\\na wide range of application requirements.\\nLangChain’s modular and flexible architecture equips developers with a com-\\nprehensive toolkit for building, deploying, and monitoring LLM applications. Its\\nadvanced components—LangGraph, LangServe, and LangSmith—enable sophis-\\nticated functionality for scalable, interactive, and robust applications, meeting\\nthe demands of modern AI use cases.')]\n",
      "q:  What role does LangChain's Retrieval-Augmented Generation (RAG) pipeline play in improving the accuracy and relevance of LLM responses?\n",
      "a:  LangChain's RAG pipeline can be customized or fine-tuned to improve the accuracy and relevance of LLM responses for specific domains or industries through integrations and extensibility features. Developers can build custom plugins or extend LangChain components to tailor applications to specific needs, ensuring flexibility and adaptability. The architecture supports third-party integrations, multi-modal data processing, and AI tool integration, providing a comprehensive toolkit for building, deploying, and monitoring applications.\n"
     ]
    }
   ],
   "source": [
    "# Answer each sub-question individually \n",
    "from langchain import hub\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# RAG prompt\n",
    "\n",
    "template = \"\"\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "Context: {context}\n",
    "Question: {question} \n",
    "Answer: \n",
    "\"\"\"\n",
    "\n",
    "prompt_rag = ChatPromptTemplate.from_template(template)\n",
    "# prompt_rag = hub.pull(\"rlm/rag-prompt\")\n",
    "# template = \" {question} \"\n",
    "# prompt_rag = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "def retrieve_and_rag(question,prompt_rag,sub_question_generator_chain):\n",
    "    \"\"\"RAG on each sub-question\"\"\"\n",
    "    retriever = vectorstore.as_retriever()\n",
    "    \n",
    "    # Use our decomposition / \n",
    "    sub_questions = sub_question_generator_chain.invoke({\"question\":question})\n",
    "    \n",
    "    # Initialize a list to hold RAG chain results\n",
    "    rag_results = []\n",
    "    \n",
    "    for sub_question in sub_questions:\n",
    "        \n",
    "        print(\"sq :\", sub_question)\n",
    "        \n",
    "        # Retrieve documents for each sub-question\n",
    "        retrieved_docs = retriever.invoke(sub_question)\n",
    "        print(\"rdocs: \", retrieved_docs)\n",
    "        \n",
    "        # Use retrieved documents and sub-question in RAG chain\n",
    "        answer = (prompt_rag | llm | StrOutputParser()).invoke({\"context\": retrieved_docs, \"question\": sub_question})\n",
    "        \n",
    "        print(\"q: \", question)\n",
    "        print(\"a: \", answer)\n",
    "        \n",
    "        rag_results.append(answer)\n",
    "    \n",
    "    return rag_results,sub_questions\n",
    "\n",
    "# Wrap the retrieval and RAG process in a RunnableLambda for integration into a chain\n",
    "answers, questions = retrieve_and_rag(question, prompt_rag, generate_queries_decomposition)\n",
    "\n",
    "# print(\"qqq: \", questions, \"\\naaa: \", answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b8631dda-bbcd-437c-81b3-5db7abb831f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Question:\n",
      " What role does LangChain's Retrieval-Augmented Generation (RAG) pipeline play in improving the accuracy and relevance of LLM responses?\n",
      "\n",
      "Context (including 3 question:answer pairs from modified query ):\n",
      " Question 1: 1. How does LangChain's Retrieval-Augmented Generation (RAG) pipeline incorporate external knowledge sources to enhance the accuracy of LLM responses?\n",
      "Answer 1: LangChain's RAG pipeline embeds diverse data sources into a vector store. When a user submits a query, the system retrieves the most relevant information from this vector store. This retrieved context is then provided to the large language model (LLM), improving its ability to generate accurate and factually grounded responses.\n",
      "\n",
      "Question 2: 2. What specific techniques does LangChain's RAG pipeline use to ensure the relevance of LLM responses to user queries?\n",
      "Answer 2: LangChain's RAG pipeline ensures relevance by using Document Loaders and Text Splitters for preprocessing, Embedding Models and Vector Stores for similarity-based retrieval, and Retrievers and RAG Chains to merge external data with model responses. These techniques enhance applications like question answering systems and recommendation engines.\n",
      "\n",
      "Question 3: 3. Can LangChain's RAG pipeline be customized or fine-tuned to improve the accuracy and relevance of LLM responses for specific domains or industries?\n",
      "Answer 3: LangChain's RAG pipeline can be customized or fine-tuned to improve the accuracy and relevance of LLM responses for specific domains or industries through integrations and extensibility features. Developers can build custom plugins or extend LangChain components to tailor applications to specific needs, ensuring flexibility and adaptability. The architecture supports third-party integrations, multi-modal data processing, and AI tool integration, providing a comprehensive toolkit for building, deploying, and monitoring applications.\n",
      "\n",
      "Final Answer:\n",
      " LangChain's Retrieval-Augmented Generation (RAG) pipeline plays a crucial role in enhancing the accuracy and relevance of Large Language Model (LLM) responses by incorporating external knowledge sources. By embedding diverse data sources into a vector store and utilizing techniques such as Document Loaders, Embedding Models, and Retrievers, the RAG pipeline ensures that the LLM generates factually grounded and contextually relevant responses to user queries. Additionally, the pipeline can be customized and fine-tuned for specific domains or industries, allowing developers to tailor applications to meet specific needs and improve the overall performance of the system. Overall, LangChain's RAG pipeline serves as a powerful tool for improving the quality of LLM responses and enhancing the user experience in applications such as question answering systems and recommendation engines.\n"
     ]
    }
   ],
   "source": [
    "def format_qa_pairs(questions, answers):\n",
    "    \"\"\"Format Q and A pairs\"\"\"\n",
    "    \n",
    "    formatted_string = \"\"\n",
    "    for i, (question, answer) in enumerate(zip(questions, answers), start=1):\n",
    "        formatted_string += f\"Question {i}: {question}\\nAnswer {i}: {answer}\\n\\n\"\n",
    "    return formatted_string.strip()\n",
    "\n",
    "context = format_qa_pairs(questions, answers)\n",
    "\n",
    "# Prompt\n",
    "template = \"\"\"Here is a set of Q+A pairs:\n",
    "\n",
    "{context}\n",
    "\n",
    "Use these to synthesize an answer to the question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "final_rag_chain = (\n",
    "    prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# question2 = \"What do you know?\"\n",
    "\n",
    "print(\"Original Question:\\n\", question)\n",
    "print(\"\\nContext (including 3 question:answer pairs from modified query ):\\n\", context)\n",
    "\n",
    "print(\"\\nFinal Answer:\\n\", final_rag_chain.invoke({\"context\":context,\"question\":question}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17698863-e683-48f4-b50e-adaa2bdee55d",
   "metadata": {},
   "source": [
    "<!-- Trace:\n",
    "\n",
    "https://smith.langchain.com/public/ed1cabf5-dea0-478b-8088-f7323d938a9b/r -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6126bebb-94e5-48ef-9a17-6a315ed0a596",
   "metadata": {},
   "source": [
    "## Part 8: Step Back\n",
    "\n",
    "The step-back method is a problem-solving and information retrieval technique that involves generating more abstract or higher-level questions rather than directly addressing the original query. This approach, known as \"stepback prompting,\" emphasizes understanding broader contexts and underlying concepts by posing general questions that provide a bigger picture. The method includes using examples to guide the formulation of these abstract questions and allows for independent retrieval of information relating to both the original and step-back questions. This dual retrieval process can enhance understanding and yield more robust answers, making it particularly useful in domains with substantial conceptual knowledge, such as technical documentation and textbooks, by separately addressing high-level concepts and their detailed implementations.\n",
    "\n",
    "![step-back](./image/step-back.png)\n",
    "\n",
    "Paper: \n",
    "\n",
    "* https://arxiv.org/pdf/2310.06117.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "1d74f9f2-543d-4e41-b90b-7bb527eca1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Few Shot Examples\n",
    "from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "examples = [\n",
    "    {\n",
    "        \"input\": \"Could the members of The Police perform lawful arrests?\",\n",
    "        \"output\": \"what can the members of The Police do?\",\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Jan Sindel’s was born in what country?\",\n",
    "        \"output\": \"what is Jan Sindel’s personal history?\",\n",
    "    },\n",
    "]\n",
    "# We now transform these to example messages\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"ai\", \"{output}\"),\n",
    "    ]\n",
    ")\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    ")\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"You are an expert at world knowledge. Your task is to step back and paraphrase a question to a more generic step-back question, which is easier to answer. Here are a few examples:\"\"\",\n",
    "        ),\n",
    "        # Few shot examples\n",
    "        few_shot_prompt,\n",
    "        # New question\n",
    "        (\"user\", \"{question}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "5cba100d-167f-4392-8f58-88729d3e4ce9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'How does LangChain manage security in integrating external services in LLM applications?'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_queries_step_back = prompt | ChatOpenAI(model=openai_model, temperature=0.1) | StrOutputParser()\n",
    "question = \"How does LangChain ensure security when integrating external services like vector databases and API providers in LLM applications?\"\n",
    "generate_queries_step_back.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "999445b0-d8a0-4208-9bb6-38610667a00b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain ensures security when integrating external services like vector databases and API providers in LLM applications through a combination of best practices and internal controls. \n",
      "\n",
      "1. Granular Permissions: LangChain enforces the principle of least privilege by allowing developers to specify limited permissions. This minimizes the risk of unauthorized actions by ensuring that only necessary permissions are granted, reducing the potential attack surface.\n",
      "\n",
      "2. Sandboxing and Defense in Depth: LangChain utilizes sandboxed environments and layered security measures to protect sensitive data and limit exposure to vulnerabilities. By isolating external services and implementing multiple layers of security, LangChain mitigates the risk of data breaches and unauthorized access.\n",
      "\n",
      "3. Auditability and Monitoring: LangSmith, a component of LangChain, provides detailed logging and monitoring capabilities. This enables developers to track application usage in real-time and detect anomalies, allowing for proactive security measures to be taken.\n",
      "\n",
      "By implementing these security measures, LangChain addresses the challenges associated with integrating external services in LLM applications, ensuring that sensitive data is protected and security risks are minimized.\n"
     ]
    }
   ],
   "source": [
    "# Response prompt \n",
    "response_prompt_template = \"\"\"You are an expert of world knowledge. I am going to ask you a question. Your response should be comprehensive and not contradicted with the following context if they are relevant. Otherwise, ignore them if they are not relevant.\n",
    "\n",
    "# {normal_context}\n",
    "# {step_back_context}\n",
    "\n",
    "# Original Question: {question}\n",
    "# Answer:\"\"\"\n",
    "response_prompt = ChatPromptTemplate.from_template(response_prompt_template)\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        # Retrieve context using the normal question\n",
    "        \"normal_context\": RunnableLambda(lambda x: x[\"question\"]) | retriever,\n",
    "        # Retrieve context using the step-back question\n",
    "        \"step_back_context\": generate_queries_step_back | retriever,\n",
    "        # Pass on the question\n",
    "        \"question\": lambda x: x[\"question\"],\n",
    "    }\n",
    "    | response_prompt\n",
    "    | ChatOpenAI(model=openai_model, temperature=0.1)\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "ans = chain.invoke({\"question\": question})\n",
    "\n",
    "print(ans.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d0e558-4abe-42e4-a33a-2b93692f5fab",
   "metadata": {},
   "source": [
    "## Part 9: HyDE\n",
    "\n",
    "![hyde](./image/hyde.png)\n",
    "\n",
    "Docs: \n",
    "\n",
    "* https://github.com/langchain-ai/langchain/blob/master/cookbook/hypothetical_document_embeddings.ipynb\n",
    "\n",
    "Paper:\n",
    "\n",
    "* https://arxiv.org/abs/2212.10496"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "c2902575-bbbb-41a9-835b-9a24dc08261b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"LangChain's memory module can be utilized to maintain context across multi-turn conversations in a chatbot application by storing and retrieving relevant information from previous interactions. This module can store key information such as user preferences, past queries, and any other relevant data that can help the chatbot maintain a coherent conversation flow. By leveraging this memory module, the chatbot can provide more personalized responses and anticipate user needs based on past interactions. Additionally, the memory module can also help the chatbot track the progression of the conversation and ensure that the context is maintained throughout the dialogue. This can lead to a more seamless and engaging user experience, as the chatbot can reference previous interactions and provide more relevant and accurate responses. Overall, the memory module in LangChain can significantly enhance the conversational capabilities of a chatbot application and improve the overall user experience.\""
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# HyDE document genration\n",
    "template = \"\"\"Please write a scientific paper passage to answer the question\n",
    "Question: {question}\n",
    "Passage:\"\"\"\n",
    "prompt_hyde = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "generate_docs_for_retrieval = (\n",
    "    prompt_hyde | ChatOpenAI(model=openai_model, temperature=0.1) | StrOutputParser() \n",
    ")\n",
    "\n",
    "# Run\n",
    "question = \"How can LangChain's memory module be utilized to maintain context across multi-turn conversations in a chatbot application?\"\n",
    "generate_docs_for_retrieval.invoke({\"question\":question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "d47587bb-23db-42a0-b087-beef9e95308b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'page': 3.0, 'source': '../test/langchain_turing.pdf'}, page_content='4 Vasilios Mavroudis\\nMemory: Enables applications to retain information from past interactions,\\nsupporting both basic and advanced memory structures. This component is crit-\\nical for maintaining context across sessions and delivering contextually aware\\nresponses.\\nIndexes: Serve as structured databases that organize and store information,\\nallowing for efficient data retrieval when processing language queries.\\nRetrievers: Designed to work alongside indexes, retrievers fetch relevant data\\nbased on query inputs, ensuring that the generated responses are well-informed\\nand accurate.\\nVector Store: Manages the embedding of words or phrases as numerical vec-\\ntors, a core step in capturing semantic meaning and supporting tasks involving\\nlanguage understanding and similarity searches.\\nOutput Parsers: Components that refine and structure the generated language\\noutputs for specific tasks, ensuring usability and relevance for the application’s\\ngoals.\\nAgents: Custom chains that prompt the language model to identify and execute\\nthe most effective sequence of actions for a given query, enabling adaptive and\\ndynamic decision-making.\\nCallbacks:Functionsthatlog,monitor,andstreamspecificeventswithinLangChain\\nworkflows, simplifying tracking and debugging processes.\\n1.1 Chat Models and Message Handling\\nLangChain supports chat models that manage complex, multi-turn conversa-\\ntions. These models use structured message sequences, allowing developers to\\ncontrol conversation flow and maintain state over time. The structured message\\nhandling system enables robust interactions with users by storing and retrieving\\nconversation history as needed [6]. Their key features include:\\n– Multi-turn Interactions: LangChain maintains state across conversation\\nturns, making it suitable for prolonged, context-dependent conversations.\\n– Structured Output: Supports structured responses like JSON, allowing\\neasy integration with downstream applications.\\n– Conversation Memory: Maintains continuity by storing conversation his-\\ntory, ideal for applications requiring persistent context, such as customer\\nsupport [4].\\n1.2 Retrieval-Augmented Generation (RAG)\\nLangChain supports Retrieval-Augmented Generation (RAG), which integrates\\nlanguage models with external knowledge bases to enhance response accuracy'),\n",
       " Document(metadata={'page': 3.0, 'source': '../test/langchain_turing.pdf'}, page_content='4 Vasilios Mavroudis\\nMemory: Enables applications to retain information from past interactions,\\nsupporting both basic and advanced memory structures. This component is crit-\\nical for maintaining context across sessions and delivering contextually aware\\nresponses.\\nIndexes: Serve as structured databases that organize and store information,\\nallowing for efficient data retrieval when processing language queries.\\nRetrievers: Designed to work alongside indexes, retrievers fetch relevant data\\nbased on query inputs, ensuring that the generated responses are well-informed\\nand accurate.\\nVector Store: Manages the embedding of words or phrases as numerical vec-\\ntors, a core step in capturing semantic meaning and supporting tasks involving\\nlanguage understanding and similarity searches.\\nOutput Parsers: Components that refine and structure the generated language\\noutputs for specific tasks, ensuring usability and relevance for the application’s\\ngoals.\\nAgents: Custom chains that prompt the language model to identify and execute\\nthe most effective sequence of actions for a given query, enabling adaptive and\\ndynamic decision-making.\\nCallbacks:Functionsthatlog,monitor,andstreamspecificeventswithinLangChain\\nworkflows, simplifying tracking and debugging processes.\\n1.1 Chat Models and Message Handling\\nLangChain supports chat models that manage complex, multi-turn conversa-\\ntions. These models use structured message sequences, allowing developers to\\ncontrol conversation flow and maintain state over time. The structured message\\nhandling system enables robust interactions with users by storing and retrieving\\nconversation history as needed [6]. Their key features include:\\n– Multi-turn Interactions: LangChain maintains state across conversation\\nturns, making it suitable for prolonged, context-dependent conversations.\\n– Structured Output: Supports structured responses like JSON, allowing\\neasy integration with downstream applications.\\n– Conversation Memory: Maintains continuity by storing conversation his-\\ntory, ideal for applications requiring persistent context, such as customer\\nsupport [4].\\n1.2 Retrieval-Augmented Generation (RAG)\\nLangChain supports Retrieval-Augmented Generation (RAG), which integrates\\nlanguage models with external knowledge bases to enhance response accuracy'),\n",
       " Document(metadata={'page': 3.0, 'source': '../test/langchain_turing.pdf'}, page_content='4 Vasilios Mavroudis\\nMemory: Enables applications to retain information from past interactions,\\nsupporting both basic and advanced memory structures. This component is crit-\\nical for maintaining context across sessions and delivering contextually aware\\nresponses.\\nIndexes: Serve as structured databases that organize and store information,\\nallowing for efficient data retrieval when processing language queries.\\nRetrievers: Designed to work alongside indexes, retrievers fetch relevant data\\nbased on query inputs, ensuring that the generated responses are well-informed\\nand accurate.\\nVector Store: Manages the embedding of words or phrases as numerical vec-\\ntors, a core step in capturing semantic meaning and supporting tasks involving\\nlanguage understanding and similarity searches.\\nOutput Parsers: Components that refine and structure the generated language\\noutputs for specific tasks, ensuring usability and relevance for the application’s\\ngoals.\\nAgents: Custom chains that prompt the language model to identify and execute\\nthe most effective sequence of actions for a given query, enabling adaptive and\\ndynamic decision-making.\\nCallbacks:Functionsthatlog,monitor,andstreamspecificeventswithinLangChain\\nworkflows, simplifying tracking and debugging processes.\\n1.1 Chat Models and Message Handling\\nLangChain supports chat models that manage complex, multi-turn conversa-\\ntions. These models use structured message sequences, allowing developers to\\ncontrol conversation flow and maintain state over time. The structured message\\nhandling system enables robust interactions with users by storing and retrieving\\nconversation history as needed [6]. Their key features include:\\n– Multi-turn Interactions: LangChain maintains state across conversation\\nturns, making it suitable for prolonged, context-dependent conversations.\\n– Structured Output: Supports structured responses like JSON, allowing\\neasy integration with downstream applications.\\n– Conversation Memory: Maintains continuity by storing conversation his-\\ntory, ideal for applications requiring persistent context, such as customer\\nsupport [4].\\n1.2 Retrieval-Augmented Generation (RAG)\\nLangChain supports Retrieval-Augmented Generation (RAG), which integrates\\nlanguage models with external knowledge bases to enhance response accuracy'),\n",
       " Document(metadata={'page': 3.0, 'source': '../test/langchain_turing.pdf'}, page_content='4 Vasilios Mavroudis\\nMemory: Enables applications to retain information from past interactions,\\nsupporting both basic and advanced memory structures. This component is crit-\\nical for maintaining context across sessions and delivering contextually aware\\nresponses.\\nIndexes: Serve as structured databases that organize and store information,\\nallowing for efficient data retrieval when processing language queries.\\nRetrievers: Designed to work alongside indexes, retrievers fetch relevant data\\nbased on query inputs, ensuring that the generated responses are well-informed\\nand accurate.\\nVector Store: Manages the embedding of words or phrases as numerical vec-\\ntors, a core step in capturing semantic meaning and supporting tasks involving\\nlanguage understanding and similarity searches.\\nOutput Parsers: Components that refine and structure the generated language\\noutputs for specific tasks, ensuring usability and relevance for the application’s\\ngoals.\\nAgents: Custom chains that prompt the language model to identify and execute\\nthe most effective sequence of actions for a given query, enabling adaptive and\\ndynamic decision-making.\\nCallbacks:Functionsthatlog,monitor,andstreamspecificeventswithinLangChain\\nworkflows, simplifying tracking and debugging processes.\\n1.1 Chat Models and Message Handling\\nLangChain supports chat models that manage complex, multi-turn conversa-\\ntions. These models use structured message sequences, allowing developers to\\ncontrol conversation flow and maintain state over time. The structured message\\nhandling system enables robust interactions with users by storing and retrieving\\nconversation history as needed [6]. Their key features include:\\n– Multi-turn Interactions: LangChain maintains state across conversation\\nturns, making it suitable for prolonged, context-dependent conversations.\\n– Structured Output: Supports structured responses like JSON, allowing\\neasy integration with downstream applications.\\n– Conversation Memory: Maintains continuity by storing conversation his-\\ntory, ideal for applications requiring persistent context, such as customer\\nsupport [4].\\n1.2 Retrieval-Augmented Generation (RAG)\\nLangChain supports Retrieval-Augmented Generation (RAG), which integrates\\nlanguage models with external knowledge bases to enhance response accuracy')]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieve\n",
    "retrieval_chain = generate_docs_for_retrieval | retriever \n",
    "retireved_docs = retrieval_chain.invoke({\"question\":question})\n",
    "retireved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "604fcc36-a1d7-4096-99b5-50db30950fc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"LangChain's memory module enables applications to retain information from past interactions, supporting both basic and advanced memory structures. This component is critical for maintaining context across sessions and delivering contextually aware responses in a chatbot application. By storing and retrieving conversation history as needed, the memory module ensures continuity and persistence of context, making it suitable for prolonged, context-dependent conversations in multi-turn interactions.\""
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RAG\n",
    "template = \"\"\"Answer the following question based on this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "final_rag_chain = (\n",
    "    prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "final_rag_chain.invoke({\"context\":retireved_docs,\"question\":question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60db3d8",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook provides a comprehensive examination of Retrieval-Augmented Generation (RAG) techniques, with a particular emphasis on multi-query architectures and their practical applications.\n",
    "\n",
    "1. **Environment Setup**: Detailed instructions for the creation of a virtual environment and the installation of requisite packages are provided.\n",
    "2. **Data Loading and Indexing**: Methodologies for the effective loading of documents and their subsequent indexing using Pinecone are discussed.\n",
    "3. **Multi-Query RAG**: Techniques for generating multiple perspectives on user queries are explored to enhance retrieval accuracy.\n",
    "4. **RAG Fusion and Decomposition**: Advanced methodologies for the integration and breakdown of queries are presented to improve the quality of responses.\n",
    "5. **Practical Implementations**: Concrete examples illustrating the implementation of these techniques utilizing LangChain and OpenAI models are included.\n",
    "\n",
    "In summary, this notebook serves as a valuable resource for the implementation of RAG methodologies across various applications, underscoring the significance of context and query diversity in the realm of information retrieval tasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
