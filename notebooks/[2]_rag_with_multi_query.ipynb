{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8d2441db-0ad8-40f2-b042-6bb96f184711",
      "metadata": {
        "id": "8d2441db-0ad8-40f2-b042-6bb96f184711"
      },
      "source": [
        "# 基于multi-query的查询转换\n",
        "\n",
        "查询转换是一组专注于重写或修改问题以进行检索的方法。\n",
        "\n",
        "![overview](https://github.com/tivon-x/bRAG-langchain/blob/main/notebooks/image/query-overview.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9833575c",
      "metadata": {
        "id": "9833575c"
      },
      "source": [
        "## 依赖"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "b66a58b1",
      "metadata": {
        "id": "b66a58b1",
        "outputId": "54b56b8b-811c-4343-edc2-66b9f881d545",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m438.1/438.1 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.3/65.3 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m304.2/304.2 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.4/152.4 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.3/46.3 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.2/44.2 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.0/50.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.0/363.0 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m516.3/516.3 kB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.5/216.5 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.0/240.0 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.2/52.2 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "! pip3 install --quiet python-dotenv langchain langchain-community langchain-core langchain-openai beautifulsoup4 tiktoken pypdf langgraph langchain-pinecone"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install --upgrade --quiet  dashscope"
      ],
      "metadata": {
        "id": "G6s6d4BBas4L",
        "outputId": "da96b66a-cffc-4ad7-de66-5cd8b8b05cc8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "G6s6d4BBas4L",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.3 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.1/1.3 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d972e08",
      "metadata": {
        "id": "1d972e08"
      },
      "source": [
        "## Environment\n",
        "\n",
        "`(1) Packages`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33bd79d8",
      "metadata": {
        "id": "33bd79d8"
      },
      "outputs": [],
      "source": [
        "# 非colab环境\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# 从 .env 文件加载所有环境变量\n",
        "load_dotenv()\n",
        "\n",
        "# LangSmith\n",
        "langsmith_tracing = os.getenv('LANGSMITH_TRACING')\n",
        "langsmith_endpoint = os.getenv('LANGSMITH_ENDPOINT')\n",
        "langsmith_api_key = os.getenv('LANGSMITH_API_KEY')\n",
        "\n",
        "## LLM\n",
        "dashscope_api_key = os.getenv('DASHSCOPE_API_KEY')\n",
        "\n",
        "## Pinecone 向量数据库\n",
        "pinecone_api_key = os.getenv('PINECONE_API_KEY')\n",
        "pinecone_api_host = os.getenv('PINECONE_API_HOST')\n",
        "index_name = os.getenv('PINECONE_INDEX_NAME')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Colab环境\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "langsmith_tracing = userdata.get('LANGSMITH_TRACING')\n",
        "langsmith_endpoint = userdata.get('LANGSMITH_ENDPOINT')\n",
        "langsmith_api_key = userdata.get('LANGSMITH_API_KEY')\n",
        "\n",
        "dashscope_api_key = userdata.get(\"DASHSCOPE_API_KEY\")\n",
        "\n",
        "pinecone_api_key = userdata.get('PINECONE_API_KEY')\n",
        "pinecone_api_host = userdata.get('PINECONE_API_HOST')\n",
        "index_name = userdata.get('PINECONE_INDEX_NAME')"
      ],
      "metadata": {
        "id": "M8Xt3Hpwa39V"
      },
      "id": "M8Xt3Hpwa39V",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "3e08bb78-7e80-4d95-a124-33b695bf5e6a",
      "metadata": {
        "id": "3e08bb78-7e80-4d95-a124-33b695bf5e6a"
      },
      "source": [
        "`(2) LangSmith`\n",
        "\n",
        "https://docs.smith.langchain.com/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "5258de38-0cc0-4d9d-a5ca-6e750ebe6976",
      "metadata": {
        "id": "5258de38-0cc0-4d9d-a5ca-6e750ebe6976"
      },
      "outputs": [],
      "source": [
        "os.environ['LANGSMITH_TRACING'] = langsmith_tracing\n",
        "os.environ['LANGSMITH_ENDPOINT'] = langsmith_endpoint\n",
        "os.environ['LANGSMITH_API_KEY'] = langsmith_api_key"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "feaccdca-1ab0-43b1-82c2-22e9cd27675b",
      "metadata": {
        "id": "feaccdca-1ab0-43b1-82c2-22e9cd27675b"
      },
      "source": [
        "`(3) API Keys`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "1cd6453b-2721-491c-b979-1860d58d8cf5",
      "metadata": {
        "id": "1cd6453b-2721-491c-b979-1860d58d8cf5"
      },
      "outputs": [],
      "source": [
        "# 使用阿里云百炼平台\n",
        "os.environ['DASHSCOPE_API_KEY'] = dashscope_api_key\n",
        "dashscope_model = \"qwen-plus-latest\"\n",
        "\n",
        "#Pinecone keys\n",
        "os.environ['PINECONE_API_KEY'] = pinecone_api_key\n",
        "os.environ['PINECONE_API_HOST'] = pinecone_api_host\n",
        "os.environ['PINECONE_INDEX_NAME'] = index_name"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# langchain的webbaseloader需要\n",
        "os.environ[\"USER_AGENT\"] = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\""
      ],
      "metadata": {
        "id": "F-cGH7GbbK-H"
      },
      "id": "F-cGH7GbbK-H",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "4f2365b3-b61b-4dbf-ab17-636cbfcaf9e0",
      "metadata": {
        "id": "4f2365b3-b61b-4dbf-ab17-636cbfcaf9e0"
      },
      "source": [
        "## Multi Query RAG 架构\n",
        "\n",
        "Flow:\n",
        "\n",
        "![multi-query](https://github.com/tivon-x/bRAG-langchain/blob/main/notebooks/image/multi-query.png?raw=1)\n",
        "\n",
        "文档:\n",
        "\n",
        "* https://python.langchain.com/docs/how_to/MultiQueryRetriever/\n",
        "\n",
        "基于距离的[向量数据库](https://python.langchain.ac.cn/docs/concepts/vectorstores/)检索 [嵌入](https://python.langchain.ac.cn/docs/concepts/embedding_models/)（表示）多维空间中的查询，并基于距离度量找到相似的嵌入文档。但是，如果查询措辞略有变化，或者嵌入未能很好地捕捉数据语义，检索可能会产生不同的结果。有时会进行提示工程/调整来手动解决这些问题，但这可能很繁琐。\n",
        "\n",
        "Multi Query RAG 架构通过使用 LLM 从不同角度为给定的用户输入查询生成多个查询，从而自动执行提示调整过程。对于每个查询，它检索一组相关文档，并取所有查询的唯一并集，以获得更大的潜在相关文档集。通过对同一问题生成多个角度，这种方法可以减轻基于距离的检索的一些限制，并获得更丰富的结果。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. 构建索引"
      ],
      "metadata": {
        "id": "lWB7DlgKoAgl"
      },
      "id": "lWB7DlgKoAgl"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "9d1b6e2b-dd76-410d-b870-23e02564a665",
      "metadata": {
        "id": "9d1b6e2b-dd76-410d-b870-23e02564a665"
      },
      "outputs": [],
      "source": [
        "# Load blog\n",
        "import bs4\n",
        "from langchain_community.document_loaders import PyPDFLoader, PyPDFDirectoryLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.embeddings import DashScopeEmbeddings\n",
        "from langchain_pinecone import PineconeVectorStore\n",
        "from pprint import pprint\n",
        "\n",
        "#### 索引 ####\n",
        "\n",
        "# Load Document (Uploading one file at a time)\n",
        "pdf_file_path = \"../langchain_turing.pdf\" # or ../test/langchain_turing.pdf\n",
        "loader = PyPDFLoader(pdf_file_path)\n",
        "\n",
        "docs = loader.load()\n",
        "\n",
        "# 上传来自一个文件的多个pdf文档\n",
        "# pdf_file_paths = <enter your path here>\n",
        "# loader = PyPDFDirectoryLoader(pdf_file_paths)\n",
        "\n",
        "# docs_dir = loader.load()\n",
        "\n",
        "# Splitter\n",
        "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "    chunk_size=2000,\n",
        "    chunk_overlap=500)\n",
        "\n",
        "# split\n",
        "splits = text_splitter.split_documents(docs)\n",
        "\n",
        "# 嵌入、索引\n",
        "def batch_read(lst, batch_size=10):\n",
        "    for i in range(0, len(lst), batch_size):\n",
        "        yield lst[i:i + batch_size]\n",
        "\n",
        "# 向量数据库用来存储嵌入向量和执行相似度搜索\n",
        "embeddings=DashScopeEmbeddings(model=\"text-embedding-v4\")\n",
        "vectorstore = PineconeVectorStore(index_name=index_name, embedding=embeddings)\n",
        "\n",
        "for batch in batch_read(splits):\n",
        "  vectorstore.add_documents(batch)\n",
        "\n",
        "retriever = vectorstore.as_retriever()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2754e5bd",
      "metadata": {
        "id": "2754e5bd"
      },
      "source": [
        "确保Pincone有上传的文件\n",
        "\n",
        " ![Pinecone](https://github.com/tivon-x/bRAG-langchain/blob/main/notebooks/image/pinecone.png?raw=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76f1b6c5-faa9-404b-90c6-22d3b40169fa",
      "metadata": {
        "id": "76f1b6c5-faa9-404b-90c6-22d3b40169fa"
      },
      "source": [
        "### 2. 生成多个query"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.1 MultiQueryRetriever\n",
        "MultiQueryRetriever 通过使用 LLM 从不同角度为给定的用户输入查询生成多个查询，从而自动化提示调整过程。对于每个查询，它检索一组相关文档，并取所有查询的唯一并集，以获得更大的潜在相关文档集。通过对同一问题生成多个角度，MultiQueryRetriever 可以减轻基于距离的检索的一些限制，并获得更丰富的结果。"
      ],
      "metadata": {
        "id": "bv2s1779mZHW"
      },
      "id": "bv2s1779mZHW"
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
        "from langchain_community.chat_models.tongyi import ChatTongyi\n",
        "\n",
        "question = \"What are the approaches to Task Decomposition?\"\n",
        "query_llm = ChatTongyi(model=dashscope_model, temperature=0.1)\n",
        "retriever_from_llm = MultiQueryRetriever.from_llm(\n",
        "    retriever=retriever, llm=query_llm\n",
        ")"
      ],
      "metadata": {
        "id": "gtOZKL9DkEjn"
      },
      "id": "gtOZKL9DkEjn",
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 通过logging获取查询生成结果\n",
        "import logging\n",
        "\n",
        "logging.basicConfig()\n",
        "# 在info级别\n",
        "logging.getLogger(\"langchain.retrievers.multi_query\").setLevel(logging.INFO)"
      ],
      "metadata": {
        "id": "Jzo62fzKkraF"
      },
      "id": "Jzo62fzKkraF",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique_docs = retriever_from_llm.invoke(question)\n",
        "len(unique_docs)"
      ],
      "metadata": {
        "id": "IvMnvh47k30U",
        "outputId": "fa2c555f-e50f-4f8b-8ada-5d64dbbcf61b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "IvMnvh47k30U",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:langchain.retrievers.multi_query:Generated queries: ['What methods or strategies are commonly used for breaking down complex tasks into smaller components?', 'How can a large task be systematically divided into manageable subtasks?', 'What are the different techniques for decomposing a task in project management or problem-solving processes?']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.2 手动构建\n",
        "在底层，MultiQueryRetriever 使用特定的 [提示](https://python.langchain.ac.cn/api_reference/langchain/retrievers/langchain.retrievers.multi_query.MultiQueryRetriever.html)生成查询。要自定义此提示\n",
        "\n",
        "1. 创建一个 [PromptTemplate](https://python.langchain.ac.cn/api_reference/core/prompts/langchain_core.prompts.prompt.PromptTemplate.html)，其中包含问题的输入变量；\n",
        "2. 实现一个如下所示的[输出解析器](https://python.langchain.ac.cn/docs/concepts/output_parsers/)，将结果拆分为查询列表。\n",
        "\n",
        "提示和输出解析器必须共同支持生成查询列表。"
      ],
      "metadata": {
        "id": "41beKzKclA7X"
      },
      "id": "41beKzKclA7X"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "965de464-0c98-4318-9f9e-f8a597c8d5d6",
      "metadata": {
        "id": "965de464-0c98-4318-9f9e-f8a597c8d5d6"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import BaseOutputParser\n",
        "\n",
        "# Multi Query prompt\n",
        "template = \"\"\"You are an AI language model assistant. Your task is to generate five\n",
        "different versions of the given user question to retrieve relevant documents from a vector\n",
        "database. By generating multiple perspectives on the user question, your goal is to help\n",
        "the user overcome some of the limitations of the distance-based similarity search.\n",
        "Provide these alternative questions separated by newlines. Original question: {question}\"\"\"\n",
        "prompt_perspectives = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "# Output parser will split the LLM result into a list of queries\n",
        "# 输出解析器会切分LLM的结果文本为查询列表\n",
        "class LineListOutputParser(BaseOutputParser[list[str]]):\n",
        "    \"\"\"Output parser for a list of lines.\"\"\"\n",
        "\n",
        "    def parse(self, text: str) -> list[str]:\n",
        "        lines = text.strip().split(\"\\n\")\n",
        "        return list(filter(None, lines))  # 去除空行\n",
        "\n",
        "output_parser = LineListOutputParser()\n",
        "\n",
        "generate_queries = (\n",
        "    prompt_perspectives\n",
        "    | ChatTongyi(model=dashscope_model, temperature=0.1)\n",
        "    | output_parser\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"How does LangChain leverage modular components like LangGraph, LangSmith, and LangServe to address challenges in building scalable and secure LLM-powered applications?\"\n",
        "generate_queries.invoke({\"question\":question})"
      ],
      "metadata": {
        "id": "qTEC-3Lrhs1p",
        "outputId": "7c8a8360-7740-4cf4-92d9-fc2a994a74a8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "qTEC-3Lrhs1p",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['What are the roles of LangGraph, LangSmith, and LangServe in enhancing scalability and security within LLM-powered applications using LangChain?  ',\n",
              " 'How do modular components such as LangGraph, LangSmith, and LangServe integrate with LangChain to overcome challenges in developing secure and scalable LLM applications?  ',\n",
              " 'In what ways does LangChain utilize tools like LangGraph, LangSmith, and LangServe to ensure robustness and scalability in LLM-based systems?  ',\n",
              " \"How can LangChain's modular architecture, including LangGraph, LangSmith, and LangServe, help developers build more secure and scalable applications powered by large language models?  \",\n",
              " 'What benefits do LangGraph, LangSmith, and LangServe bring to LangChain in terms of addressing scalability and security concerns in LLM application development?']"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "4f253520-386f-434b-8daa-d6dadb89eddb",
      "metadata": {
        "id": "4f253520-386f-434b-8daa-d6dadb89eddb",
        "outputId": "e7ee246b-db65-401c-c6f4-2dcee8ccfe86",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "from langchain.load import dumps, loads\n",
        "\n",
        "def get_unique_union(documents: list[list]):\n",
        "    \"\"\" Unique union of retrieved docs \"\"\"\n",
        "    # Flatten list of lists, and convert each Document to string\n",
        "    # 展开列表的列表，并将每个文档转换为字符串\n",
        "    flattened_docs = [dumps(doc) for sublist in documents for doc in sublist]\n",
        "    # Get unique documents\n",
        "    # 获取唯一的文档\n",
        "    unique_docs = list(set(flattened_docs))\n",
        "    # Return\n",
        "    return [loads(doc) for doc in unique_docs]\n",
        "\n",
        "# 检索\n",
        "question = \"How does LangChain leverage modular components like LangGraph, LangSmith, and LangServe to address challenges in building scalable and secure LLM-powered applications?\"\n",
        "\n",
        "# map 返回 一个新的 Runnable 对象，它会将输入列表映射到输出列表\n",
        "# 对每个输入调用invoke\n",
        "map_retriver = retriever.map()\n",
        "\n",
        "retrieval_chain = generate_queries | map_retriver | get_unique_union\n",
        "docs = retrieval_chain.invoke({\"question\":question})\n",
        "len(docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. 生成"
      ],
      "metadata": {
        "id": "jPmUz90vmf_V"
      },
      "id": "jPmUz90vmf_V"
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "af6e74e8-ddae-4165-9e4b-0022ac125194",
      "metadata": {
        "id": "af6e74e8-ddae-4165-9e4b-0022ac125194"
      },
      "outputs": [],
      "source": [
        "from operator import itemgetter\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# 生成 prompt\n",
        "template = \"\"\"Answer the following question based on this context:\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "llm = ChatTongyi(model=dashscope_model, temperature=0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####3.1 MultiQueryRetriever"
      ],
      "metadata": {
        "id": "GwDg5utHnVde"
      },
      "id": "GwDg5utHnVde"
    },
    {
      "cell_type": "code",
      "source": [
        "final_rag_chain = (\n",
        "    {\"context\": retriever_from_llm,\n",
        "     \"question\": itemgetter(\"question\")}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "print(final_rag_chain.invoke({\"question\":question}))"
      ],
      "metadata": {
        "id": "OMEtTrYWnFIZ",
        "outputId": "7e6877be-4537-46be-bc6c-8a5b478b52c9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "OMEtTrYWnFIZ",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:langchain.retrievers.multi_query:Generated queries: ['How does LangChain utilize tools such as LangGraph, LangSmith, and LangServe to enhance scalability and security in LLM-based applications?  ', 'What role do modular components like LangGraph, LangSmith, and LangServe play in LangChain when it comes to developing robust and secure applications powered by large language models?  ', \"In what ways do LangChain's modules—LangGraph, LangSmith, and LangServe—help overcome common challenges in deploying scalable and secure LLM-powered systems?\"]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LangChain leverages modular components like **LangGraph**, **LangSmith**, and **LangServe** to address challenges in building scalable and secure large language model (LLM)-powered applications as follows:\n",
            "\n",
            "1. **LangGraph for Stateful Process Modeling**:  \n",
            "   - LangGraph enables developers to design complex, stateful workflows by representing processes as graphs, where nodes represent actions or decisions and edges define transitions.\n",
            "   - This modular approach allows developers to build contextually aware and dynamic applications that can maintain and manage state across interactions.\n",
            "   - It integrates seamlessly with the broader LangChain ecosystem, allowing for incorporation of external tools and APIs as graph nodes while supporting independent operation when needed.\n",
            "\n",
            "2. **LangSmith for Monitoring and Evaluation**:  \n",
            "   - LangSmith provides tracing capabilities to monitor application performance, capturing detailed logs of agent interactions and node execution within LangGraph.\n",
            "   - It aids in debugging, optimizing, and evaluating LLM-driven workflows, ensuring high reliability and transparency.\n",
            "   - By enabling real-time monitoring and analytics, LangSmith enhances the ability to identify inefficiencies and improve application behavior over time.\n",
            "\n",
            "3. **LangServe for Scalable API Deployment**:  \n",
            "   - LangServe simplifies the deployment of LangChain applications as RESTful APIs, making them accessible to external systems and clients.\n",
            "   - It supports load balancing and auto-scaling, which are crucial for maintaining performance under variable traffic loads, making it suitable for production-grade deployments.\n",
            "   - The framework includes customizable routing, request handling, and response formatting, enabling secure and efficient API management tailored to specific use cases.\n",
            "\n",
            "Together, these components form a cohesive yet flexible toolkit that addresses key challenges such as **state management**, **scalability**, **monitoring**, and **secure deployment** of LLM-powered applications. While this modular design introduces some complexity and potential security considerations due to reliance on external integrations, LangChain's architecture empowers developers to create robust, scalable, and secure applications across diverse domains.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.2 手动构建"
      ],
      "metadata": {
        "id": "ScfV-zGInbLa"
      },
      "id": "ScfV-zGInbLa"
    },
    {
      "cell_type": "code",
      "source": [
        "final_rag_chain = (\n",
        "    {\"context\": retrieval_chain,\n",
        "     \"question\": itemgetter(\"question\")}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "print(final_rag_chain.invoke({\"question\":question}))"
      ],
      "metadata": {
        "id": "y4z9eMDFnBqm",
        "outputId": "ca85fac4-588d-4640-93db-eb2f35940a10",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "y4z9eMDFnBqm",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LangChain leverages modular components like **LangGraph**, **LangSmith**, and **LangServe** to address challenges in building scalable and secure applications powered by large language models (LLMs) in the following ways:\n",
            "\n",
            "1. **LangGraph for Stateful Process Modeling**:  \n",
            "   - LangGraph allows developers to structure complex workflows using nodes and edges, enabling branching logic and multi-agent interactions.  \n",
            "   - It integrates with LangChain and LangSmith to provide tracing capabilities through LangSmith, allowing detailed monitoring of each node’s performance and logging of agent interactions.  \n",
            "   - This enhances scalability by supporting dynamic, stateful processes while improving transparency and debugging capabilities, which are essential for maintaining security and reliability.\n",
            "\n",
            "2. **LangSmith for Monitoring and Evaluation**:  \n",
            "   - LangSmith provides tools for real-time performance monitoring, error tracking, and version control, helping developers iteratively optimize their applications.  \n",
            "   - Its **tracing** feature captures inputs, outputs, and metadata from LLM interactions, offering visibility into application behavior for debugging and identifying bottlenecks.  \n",
            "   - **Performance testing** features allow developers to validate applications under real-world conditions, ensuring reliability and uncovering potential security vulnerabilities before deployment.\n",
            "\n",
            "3. **LangServe for Scalable API Deployment**:  \n",
            "   - LangServe enables developers to deploy LangChain applications as REST APIs, making them accessible to external systems and users in real time.  \n",
            "   - It supports **scalability and load balancing**, handling high traffic volumes by distributing loads across instances and enabling auto-scaling based on demand.  \n",
            "   - This ensures consistent performance even during traffic spikes, making it suitable for production environments where both scalability and low response times are critical.  \n",
            "\n",
            "### Addressing Challenges:  \n",
            "- **Scalability**: By leveraging LangServe's REST API deployment and load-balancing features, LangChain ensures that applications can handle high traffic and scale effectively.  \n",
            "- **Security**: LangSmith improves observability and error tracking, helping identify risks early, while LangChain’s framework includes granular permissions, sandboxing, and real-time monitoring to enhance data security.  \n",
            "- **Complexity Management**: The modular design allows developers to build tailored applications using individual components without being overwhelmed by unnecessary complexity upfront.  \n",
            "\n",
            "In summary, LangChain uses these modular components synergistically to simplify development, improve scalability, and strengthen security, enabling developers to build robust and context-aware LLM-powered applications across diverse domains.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. LangGraph"
      ],
      "metadata": {
        "id": "j08yVmqcn6HA"
      },
      "id": "j08yVmqcn6HA"
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import START, StateGraph\n",
        "from typing import List, TypedDict, Annotated\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "retriever_from_llm = MultiQueryRetriever.from_llm(\n",
        "    retriever=retriever, llm=ChatTongyi(model=dashscope_model, temperature=0.1)\n",
        "  )\n",
        "\n",
        "# 定义应用的状态\n",
        "class State(TypedDict):\n",
        "    question: str\n",
        "    context: List[Document]\n",
        "    answer: str\n",
        "\n",
        "# 定义检索节点\n",
        "def retrieve(state: State):\n",
        "  retrieved_docs = retriever_from_llm.invoke(state[\"question\"])\n",
        "  return {\"context\": retrieved_docs}\n",
        "\n",
        "# 定义应用的生成节点\n",
        "def generate(state: State):\n",
        "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
        "    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})\n",
        "    response = llm.invoke(messages)\n",
        "    return {\"answer\": response.content}\n",
        "\n",
        "# 构建图\n",
        "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
        "graph_builder.add_edge(START, \"retrieve\")\n",
        "graph = graph_builder.compile()\n"
      ],
      "metadata": {
        "id": "JANFbYFyn5XS"
      },
      "id": "JANFbYFyn5XS",
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 可视化\n",
        "from IPython.display import Image, display\n",
        "\n",
        "display(Image(graph.get_graph().draw_mermaid_png()))"
      ],
      "metadata": {
        "id": "zlp5WxWgrCnd",
        "outputId": "3245a9b6-17b9-40ef-b996-b6282dc41217",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        }
      },
      "id": "zlp5WxWgrCnd",
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = graph.invoke({\"question\": \"What is Task Decomposition?\"})\n",
        "print(response[\"answer\"])"
      ],
      "metadata": {
        "id": "_zM0fZDBrD0M",
        "outputId": "15538278-8abc-4101-ee11-15bbeae0a96a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "_zM0fZDBrD0M",
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:langchain.retrievers.multi_query:Generated queries: ['What is the concept of Task Decomposition in project management?  ', 'How does Task Decomposition help in breaking down complex tasks?  ', 'What are the key principles and applications of Task Decomposition?']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Based on the provided context, **Task Decomposition** refers to the process of breaking down complex workflows or applications into smaller, manageable components or nodes that can be executed independently or in sequence. This concept is central to LangGraph’s functionality within the LangChain ecosystem.\n",
            "\n",
            "In the context of **LangGraph**, Task Decomposition involves:\n",
            "\n",
            "1. **Defining Nodes and State**: Developers initialize individual functions (nodes) such as calling an LLM, invoking a tool, or accessing external data. They also define the state schema to manage the application's context.\n",
            "\n",
            "2. **Setting Entry Points and Edges**: Nodes are connected by edges that determine the flow of execution based on input conditions and application state. These transitions can be conditional or sequential.\n",
            "\n",
            "3. **Compiling and Executing the Graph**: Once nodes and edges are defined, the graph is compiled into a runnable format. Functions like `invoke()` and `stream()` allow for execution and real-time updates, enabling iterative processing of tasks.\n",
            "\n",
            "By decomposing tasks in this structured way, LangGraph supports advanced agent workflows with features like:\n",
            "- **Branching and Cycles**: Allowing for conditional logic and repeated execution until certain conditions are met.\n",
            "- **Persistence and State Management**: Maintaining continuity across sessions by saving and retrieving application state.\n",
            "- **Human-in-the-loop Interactions**: Enabling manual intervention at specific stages for approval or modification.\n",
            "- **Streaming Outputs**: Supporting real-time updates during task execution.\n",
            "\n",
            "Thus, **Task Decomposition** in LangGraph enables developers to model complex, multi-step, and potentially long-running processes in a modular and可控 manner.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "618afe4a-f1d6-433a-9d28-f1956c2b83ef",
      "metadata": {
        "id": "618afe4a-f1d6-433a-9d28-f1956c2b83ef"
      },
      "source": [
        "## Part 6: RAG-Fusion\n",
        "\n",
        "RAG Fusion是一种先进的检索增强生成方法，它结合了多个检索源，每个检索源都专用于一个独特的上下文，以生成更准确、上下文丰富的响应。与独立执行和聚合结果而不混合的常规的multi-query不同，RAG Fusion动态选择和整合来自不同来源的信息，创建一个统一、连贯的答案，以适应复杂的查询。这种上下文相关信息的融合增强了响应的鲁棒性和相关性，使RAG fusion在处理多方面信息检索任务方面特别有效。\n",
        "\n",
        "Flow:\n",
        "\n",
        "![rag-fusion](https://github.com/tivon-x/bRAG-langchain/blob/main/notebooks/image/rag-fusion.png?raw=1)\n",
        "\n",
        "Docs:\n",
        "\n",
        "* https://github.com/langchain-ai/langchain/blob/master/cookbook/rag_fusion.ipynb?ref=blog.langchain.dev\n",
        "\n",
        "Blog / repo:\n",
        "\n",
        "* https://medium.com/towards-data-science/forget-rag-the-future-is-rag-fusion-1147298d8ad1\n",
        "\n",
        "RAG-Fusion：\n",
        "\n",
        "Query Duplication with a Twist: 用 LLM 根据用户的 Query 生成几个相关的但不同的 Queries\n",
        "Vector Search Unleashed：对原 Query 和生成的 Queries 都进行向量（或者其他方式的）搜索\n",
        "Intelligent Reranking：使用 Reciprocal Rank Fusion 对搜索结果进行重新排序，挑选前 N 个。\n",
        "Eloquent Finale: 把挑选的结果和问题一起，发给 LLM 让他生成最终的结果\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RAG Chain"
      ],
      "metadata": {
        "id": "wqUdnhoOw0Rn"
      },
      "id": "wqUdnhoOw0Rn"
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "34e7075b-b80d-461d-9e2e-e05e29436f3e",
      "metadata": {
        "id": "34e7075b-b80d-461d-9e2e-e05e29436f3e"
      },
      "outputs": [],
      "source": [
        "# RAG-Fusion: Related\n",
        "template = \"\"\"You are a helpful assistant that generates multiple search queries based on a single input query. \\n\n",
        "Generate multiple search queries related to: {question} \\n\n",
        "Output (4 queries):\"\"\"\n",
        "prompt_rag_fusion = ChatPromptTemplate.from_template(template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "9781b40c-c408-42f4-ae14-cd11be513b63",
      "metadata": {
        "id": "9781b40c-c408-42f4-ae14-cd11be513b63"
      },
      "outputs": [],
      "source": [
        "generate_queries = (\n",
        "    prompt_rag_fusion\n",
        "    | ChatTongyi(model=dashscope_model, temperature=0.1)\n",
        "    | output_parser\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "2b1adff1-e993-4747-b95d-656eaaeccfdd",
      "metadata": {
        "id": "2b1adff1-e993-4747-b95d-656eaaeccfdd",
        "outputId": "061118ae-75fc-4f43-f63a-a8f5bb941548",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "from langchain.load import dumps, loads\n",
        "\n",
        "def reciprocal_rank_fusion(results: list[list], k=60, n=7):\n",
        "    \"\"\" Reciprocal_rank_fusion that takes multiple lists of ranked documents\n",
        "        and an optional parameter k used in the RRF formula\n",
        "        接受多个排名文档列表、RRF公式中使用的可选参数k、返回的文档数量\n",
        "    \"\"\"\n",
        "\n",
        "    # 初始化字典以保存每个唯一文档的融合分数\n",
        "    fused_scores = {}\n",
        "\n",
        "    # 遍历每个排名文档列表\n",
        "    for docs in results:\n",
        "        # 遍历列表中的每个文档及其排名（列表中的位置）\n",
        "        for rank, doc in enumerate(docs):\n",
        "            # 将文档转换为字符串格式以用作key（假设文档可以序列化为JSON）\n",
        "            doc_str = dumps(doc)\n",
        "            # 如果文档尚未在fused_scores字典中，请将其初始分数添加为0\n",
        "            if doc_str not in fused_scores:\n",
        "                fused_scores[doc_str] = 0\n",
        "            # 检索文档的当前分数（如果有的话）\n",
        "            previous_score = fused_scores[doc_str]\n",
        "            # 使用RRF公式更新文档的分数：1/（rank+k）\n",
        "            fused_scores[doc_str] += 1 / (rank + k)\n",
        "\n",
        "    # 根据fusion分数按降序对文档进行排序，以获得最终的重新排序结果\n",
        "    reranked_results = [\n",
        "        (loads(doc), score)\n",
        "        for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
        "    ]\n",
        "\n",
        "    # 将重新排序的结果作为元组列表返回，每个元组包含文档及其fusion分数\n",
        "    return reranked_results[:n]\n",
        "\n",
        "retrieval_chain_rag_fusion = generate_queries | retriever.map() | reciprocal_rank_fusion\n",
        "docs = retrieval_chain_rag_fusion.invoke({\"question\": question})\n",
        "len(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "ce2adf2d-3d9f-4d43-afb0-8304edcfb1f1",
      "metadata": {
        "id": "ce2adf2d-3d9f-4d43-afb0-8304edcfb1f1",
        "outputId": "f10c0498-e196-4cd3-e178-1c3dcf756c7e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('LangChain leverages modular components like **LangGraph**, **LangSmith**, '\n",
            " 'and **LangServe** to address challenges in building scalable and secure '\n",
            " 'large language model (LLM)-powered applications by providing specialized '\n",
            " 'tools that streamline development, deployment, monitoring, and security. '\n",
            " \"Here's how each component contributes:\\n\"\n",
            " '\\n'\n",
            " '1. **LangGraph for Stateful Process Modeling:**\\n'\n",
            " '   - LangGraph enables developers to build complex, stateful, and '\n",
            " 'multi-agent workflows with fine-grained control over application logic.\\n'\n",
            " '   - It supports cycles, branching, and persistence, which are essential for '\n",
            " 'iterative or conditional processes in agentic architectures.\\n'\n",
            " '   - Built-in **persistence** ensures continuity across sessions, crucial '\n",
            " 'for applications like customer service agents or educational tools.\\n'\n",
            " '   - Provides **human-in-the-loop** support, allowing manual intervention '\n",
            " 'for oversight—particularly important in high-stakes domains such as '\n",
            " 'healthcare or legal services.\\n'\n",
            " '   - Enables real-time **streaming outputs**, enhancing user experience in '\n",
            " 'interactive systems like chatbots.\\n'\n",
            " '\\n'\n",
            " '2. **LangSmith for Monitoring and Evaluation:**\\n'\n",
            " '   - LangSmith offers comprehensive **tracing capabilities**, enabling '\n",
            " 'detailed logging of agent interactions and performance metrics.\\n'\n",
            " '   - Facilitates **evaluation and testing** through structured datasets and '\n",
            " 'both built-in and custom evaluators (e.g., exact match scoring, cosine '\n",
            " 'similarity), ensuring accuracy and reliability.\\n'\n",
            " '   - Supports **dataset management** and version control, allowing '\n",
            " 'developers to maintain consistency and track improvements as applications '\n",
            " 'evolve.\\n'\n",
            " '   - Integrates seamlessly with **LangChain and LangServe**, offering '\n",
            " 'end-to-end observability, including monitoring API usage patterns, request '\n",
            " 'latencies, and system bottlenecks.\\n'\n",
            " '\\n'\n",
            " '3. **LangServe for Scalable API Deployment:**\\n'\n",
            " '   - LangServe simplifies the deployment of LangChain applications as **REST '\n",
            " 'APIs**, making LLM-powered systems accessible to external clients and '\n",
            " 'services.\\n'\n",
            " '   - Offers robust **scalability and load balancing**, ensuring consistent '\n",
            " 'performance under high traffic volumes and unpredictable usage patterns.\\n'\n",
            " '   - Provides tools for **API endpoint customization**, including routing, '\n",
            " 'request handling, and response formatting, supporting a wide range of use '\n",
            " 'cases.\\n'\n",
            " '   - Works closely with **LangSmith** to provide real-time monitoring and '\n",
            " 'insights into deployed applications.\\n'\n",
            " '\\n'\n",
            " '### Addressing Challenges:\\n'\n",
            " '- **Scalability:** Through LangServe’s REST API framework and auto-scaling '\n",
            " 'capabilities, applications can handle variable loads efficiently.\\n'\n",
            " '- **State Management:** LangGraph’s persistent state management and memory '\n",
            " 'tracking ensure continuity and context-awareness in long-running '\n",
            " 'applications.\\n'\n",
            " '- **Security:** While not explicitly detailed, LangChain includes features '\n",
            " 'like granular permissions, sandboxing, and monitoring, with ongoing efforts '\n",
            " 'toward dynamic permission adjustment, advanced encryption, and proactive '\n",
            " 'analytics to mitigate risks from third-party integrations.\\n'\n",
            " '- **Complexity Management:** The modular design allows developers to '\n",
            " 'selectively use components based on application needs, though this '\n",
            " 'flexibility introduces a learning curve.\\n'\n",
            " '\\n'\n",
            " 'By integrating these components, LangChain provides a **comprehensive '\n",
            " 'toolkit** that empowers developers to build scalable, stateful, and secure '\n",
            " 'LLM applications while addressing key challenges in usability, performance, '\n",
            " 'and security.')\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "# RAG\n",
        "template = \"\"\"Answer the following question based on this context:\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "final_rag_chain = (\n",
        "    {\"context\": retrieval_chain_rag_fusion,\n",
        "     \"question\": itemgetter(\"question\")}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "pprint(final_rag_chain.invoke({\"question\":question}))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "94c812d3-4d91-4634-8301-0b68be88a887",
      "metadata": {
        "id": "94c812d3-4d91-4634-8301-0b68be88a887"
      },
      "source": [
        "## RAG Decomposition 架构\n",
        "\n",
        "RAG分解（decomposition）架构是检索增强生成中的一个专门框架，旨在将复杂查询分解为更简单、可管理的子查询。每个子查询专注于大问题的一个特定部分，并发送到专门的检索器或数据库中以获取精确信息。这些子结果随后被整合和综合，形成对原始查询的连贯、全面的回答。这种架构提高了检索准确性，因为每个子查询针对特定上下文，减少了噪音，提高了最终响应的相关性。RAG分解特别适用于多部分问题、复杂主题或需要深入、细致回答的场景。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19543d04-ff31-4774-b89c-9d31f5a28fc9",
      "metadata": {
        "id": "19543d04-ff31-4774-b89c-9d31f5a28fc9"
      },
      "source": [
        "### Answer recursively  \n",
        "\n",
        "![answer-recursively](https://github.com/tivon-x/bRAG-langchain/blob/main/notebooks/image/answer-recursively.png?raw=1)\n",
        "\n",
        "Papers:\n",
        "\n",
        "* https://arxiv.org/pdf/2205.10625.pdf\n",
        "* https://arxiv.org/abs/2212.10509.pdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "f82fac99-58dc-4bb9-84e6-51180db855ad",
      "metadata": {
        "id": "f82fac99-58dc-4bb9-84e6-51180db855ad"
      },
      "outputs": [],
      "source": [
        "# Decomposition prompt\n",
        "template = \"\"\"You are a helpful assistant that generates multiple sub-questions related to an input question. \\n\n",
        "The goal is to break down the input into a set of sub-problems / sub-questions that can be answers in isolation. \\n\n",
        "Generate multiple search queries related to: {question} \\n\n",
        "Output (3 queries):\"\"\"\n",
        "prompt_decomposition = ChatPromptTemplate.from_template(template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "c31eefd9-5598-44a1-b0d6-dd04553a3eb4",
      "metadata": {
        "id": "c31eefd9-5598-44a1-b0d6-dd04553a3eb4"
      },
      "outputs": [],
      "source": [
        "# Chain\n",
        "generate_queries_decomposition = ( prompt_decomposition | query_llm | output_parser)\n",
        "\n",
        "# Run\n",
        "question = \"What role does LangChain's Retrieval-Augmented Generation (RAG) pipeline play in improving the accuracy and relevance of LLM responses?\"\n",
        "questions = generate_queries_decomposition.invoke({\"question\":question})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "07191b5c-cf72-4b8f-a225-f57dfdc2fc78",
      "metadata": {
        "id": "07191b5c-cf72-4b8f-a225-f57dfdc2fc78",
        "outputId": "e2c60283-53dc-4f97-d13b-5104c07d06ba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"1. How does LangChain's Retrieval-Augmented Generation (RAG) pipeline enhance the accuracy of responses from large language models?  \",\n",
              " '2. What is the role of external data retrieval in improving the relevance of LLM-generated answers within the RAG framework?  ',\n",
              " \"3. How does the integration of retrieval and generation phases in LangChain's RAG pipeline impact overall model performance?\"]"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "source": [
        "questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "c72bbd12-f85c-4ed0-9dfa-8503afebfafa",
      "metadata": {
        "id": "c72bbd12-f85c-4ed0-9dfa-8503afebfafa"
      },
      "outputs": [],
      "source": [
        "# Prompt\n",
        "template = \"\"\"Here is the question you need to answer:\n",
        "\n",
        "\\n --- \\n {question} \\n --- \\n\n",
        "\n",
        "Here is any available background question + answer pairs:\n",
        "\n",
        "\\n --- \\n {q_a_pairs} \\n --- \\n\n",
        "\n",
        "Here is additional context relevant to the question:\n",
        "\n",
        "\\n --- \\n {context} \\n --- \\n\n",
        "\n",
        "Use the above context and any background question + answer pairs to answer the question: \\n {question}\n",
        "\"\"\"\n",
        "\n",
        "decomposition_prompt = ChatPromptTemplate.from_template(template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "a20bf0d4-f567-4451-834d-a07190a3185e",
      "metadata": {
        "id": "a20bf0d4-f567-4451-834d-a07190a3185e"
      },
      "outputs": [],
      "source": [
        "from operator import itemgetter\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "def format_qa_pair(question, answer):\n",
        "    \"\"\"Format Q and A pair\"\"\"\n",
        "\n",
        "    formatted_string = \"\"\n",
        "    formatted_string += f\"Question: {question}\\nAnswer: {answer}\\n\\n\"\n",
        "    return formatted_string.strip()\n",
        "\n",
        "q_a_pairs = \"\"\n",
        "for q in questions:\n",
        "\n",
        "    # itemgetter获取对应字段的值\n",
        "    rag_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | retriever,\n",
        "     \"question\": itemgetter(\"question\"),\n",
        "     \"q_a_pairs\": itemgetter(\"q_a_pairs\")}\n",
        "    | decomposition_prompt\n",
        "    | llm\n",
        "    | StrOutputParser())\n",
        "\n",
        "    answer = rag_chain.invoke({\"question\":q,\"q_a_pairs\":q_a_pairs})\n",
        "    q_a_pair = format_qa_pair(q,answer)\n",
        "    q_a_pairs = q_a_pairs + \"\\n---\\n\"+  q_a_pair"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pprint(q_a_pairs)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "5Rl1bW6Dyfey",
        "outputId": "6971c97c-576a-44ca-a074-6b7fe26428f5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "5Rl1bW6Dyfey",
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('\\n'\n",
            " '---\\n'\n",
            " \"Question: 1. How does LangChain's Retrieval-Augmented Generation (RAG) \"\n",
            " 'pipeline enhance the accuracy of responses from large language models?  \\n'\n",
            " \"Answer: LangChain's Retrieval-Augmented Generation (RAG) pipeline enhances \"\n",
            " 'the accuracy of responses from large language models (LLMs) by integrating '\n",
            " \"external knowledge sources into the model's response generation process. \"\n",
            " 'Here’s how it works:\\n'\n",
            " '\\n'\n",
            " '1. **Access to External Knowledge**: The RAG pipeline allows LLMs to access '\n",
            " 'up-to-date and domain-specific information beyond their training data. '\n",
            " 'Documents in various formats (e.g., PDF, text, images) are preloaded, '\n",
            " 'embedded, and stored in a vector store. This ensures that relevant '\n",
            " 'contextual data can be retrieved and used during query processing.\\n'\n",
            " '\\n'\n",
            " '2. **Vector Embedding and Similarity Search**: Documents are transformed '\n",
            " 'into numerical vector representations using embedding models. When a user '\n",
            " 'submits a query, the system performs a similarity search in the vector store '\n",
            " 'to identify the most relevant documents or snippets. This step ensures that '\n",
            " 'only contextually relevant information is retrieved for the query.\\n'\n",
            " '\\n'\n",
            " '3. **Contextual Enrichment of Responses**: The retrieved documents are '\n",
            " \"combined with the user's query and provided as input to the LLM. By \"\n",
            " 'incorporating this additional context, the LLM generates more factually '\n",
            " 'grounded and accurate responses. This helps mitigate issues like '\n",
            " 'hallucinations or outdated information that may arise when the model relies '\n",
            " 'solely on its internal knowledge.\\n'\n",
            " '\\n'\n",
            " '4. **Components Supporting RAG**:\\n'\n",
            " '   - **Document Loaders and Text Splitters**: These preprocess documents to '\n",
            " 'ensure efficient indexing and retrieval.\\n'\n",
            " '   - **Retrievers**: They fetch relevant data based on query inputs, '\n",
            " 'ensuring the generated responses are well-informed.\\n'\n",
            " '   - **RAG Chains**: These components merge the retrieved external data with '\n",
            " \"the model's output, refining the final response.\\n\"\n",
            " '\\n'\n",
            " \"By leveraging these techniques, LangChain's RAG pipeline enables LLMs to \"\n",
            " 'produce responses that are not only contextually enriched but also more '\n",
            " 'accurate and aligned with the latest available information.\\n'\n",
            " '---\\n'\n",
            " 'Question: 2. What is the role of external data retrieval in improving the '\n",
            " 'relevance of LLM-generated answers within the RAG framework?  \\n'\n",
            " 'Answer: In the Retrieval-Augmented Generation (RAG) framework, **external '\n",
            " 'data retrieval plays a crucial role in improving the relevance of Large '\n",
            " 'Language Model (LLM)-generated answers** by incorporating up-to-date, '\n",
            " 'context-specific, and factual information from external knowledge sources. '\n",
            " \"Here's how it enhances relevance:\\n\"\n",
            " '\\n'\n",
            " '1. **Access to Current and Domain-Specific Knowledge**:  \\n'\n",
            " '   LLMs are trained on static datasets and may lack access to real-time or '\n",
            " 'domain-specific information. External data retrieval enables the model to '\n",
            " 'pull relevant details from dynamically updated databases, documents, or '\n",
            " 'knowledge bases, ensuring that responses are accurate and timely.\\n'\n",
            " '\\n'\n",
            " '2. **Contextual Relevance through Similarity Search**:  \\n'\n",
            " '   Documents are embedded into vector representations and stored in a '\n",
            " '**vector store**. When a query is made, the system performs a similarity '\n",
            " 'search to find the most relevant document snippets. These top-k results '\n",
            " 'provide contextual background tailored specifically to the user’s question, '\n",
            " 'allowing the LLM to generate more focused and precise responses.\\n'\n",
            " '\\n'\n",
            " '3. **Reduction of Hallucinations and Inaccuracies**:  \\n'\n",
            " '   By grounding responses in verified external sources rather than relying '\n",
            " 'solely on internal knowledge, RAG significantly reduces the risk of '\n",
            " 'hallucinations—false or fabricated information. This ensures that the output '\n",
            " 'aligns with known facts and improves trustworthiness.\\n'\n",
            " '\\n'\n",
            " '4. **Personalization and Customization**:  \\n'\n",
            " '   The ability to retrieve data from specific documents or datasets allows '\n",
            " \"for personalized responses based on an organization's internal knowledge, \"\n",
            " 'such as product manuals, customer records, or research findings. This '\n",
            " \"customization increases the relevance of the response to the user's unique \"\n",
            " 'needs.\\n'\n",
            " '\\n'\n",
            " '5. **Integration with Application Workflow**:  \\n'\n",
            " '   Components like **retrievers** and **RAG chains** ensure that retrieved '\n",
            " 'content is effectively merged with the query before being passed to the LLM. '\n",
            " 'This integration enriches the prompt with relevant context, guiding the '\n",
            " 'model toward generating more meaningful and targeted outputs.\\n'\n",
            " '\\n'\n",
            " '### Summary:\\n'\n",
            " 'External data retrieval in the RAG framework acts as a bridge between the '\n",
            " 'general knowledge of an LLM and the specific, current, or proprietary '\n",
            " 'knowledge required for a task. It ensures that the generated responses are '\n",
            " 'not only linguistically coherent but also factually grounded, contextually '\n",
            " 'appropriate, and highly relevant to the query at hand.\\n'\n",
            " '---\\n'\n",
            " 'Question: 3. How does the integration of retrieval and generation phases in '\n",
            " \"LangChain's RAG pipeline impact overall model performance?\\n\"\n",
            " 'Answer: The integration of **retrieval and generation phases** in '\n",
            " \"LangChain's Retrieval-Augmented Generation (RAG) pipeline has a significant \"\n",
            " 'positive impact on the **overall performance of large language models '\n",
            " '(LLMs)**. This integration enhances model behavior across several key '\n",
            " 'dimensions, including accuracy, relevance, factual grounding, and contextual '\n",
            " 'understanding.\\n'\n",
            " '\\n'\n",
            " '### Key Impacts on Model Performance:\\n'\n",
            " '\\n'\n",
            " '#### 1. **Improved Accuracy and Factual Consistency**\\n'\n",
            " '- The retrieval phase fetches relevant, up-to-date, or domain-specific '\n",
            " 'information from external knowledge sources (e.g., document databases, '\n",
            " 'vector stores).\\n'\n",
            " '- This retrieved context is then fed into the LLM during the generation '\n",
            " 'phase, allowing it to ground its output in verified facts rather than '\n",
            " 'relying solely on its pre-training data.\\n'\n",
            " '- As a result, the model produces responses that are more factually accurate '\n",
            " 'and consistent with current or specialized knowledge.\\n'\n",
            " '\\n'\n",
            " '#### 2. **Reduction in Hallucinations**\\n'\n",
            " '- One of the well-known challenges with LLMs is their tendency to generate '\n",
            " 'plausible-sounding but incorrect or fabricated information when they lack '\n",
            " 'sufficient internal knowledge about a topic.\\n'\n",
            " '- By integrating retrieval-augmented context, the RAG pipeline provides the '\n",
            " 'LLM with explicit supporting evidence for generating answers, significantly '\n",
            " 'reducing hallucinations.\\n'\n",
            " '\\n'\n",
            " '#### 3. **Enhanced Contextual Relevance**\\n'\n",
            " '- During retrieval, documents or snippets most semantically similar to the '\n",
            " 'user’s query are selected using **vector embeddings and similarity search** '\n",
            " 'techniques.\\n'\n",
            " '- This ensures that only the most contextually relevant information is used '\n",
            " 'in the generation process, leading to responses that are tightly aligned '\n",
            " 'with the intent and specifics of the query.\\n'\n",
            " '\\n'\n",
            " '#### 4. **Support for Real-Time and Dynamic Knowledge**\\n'\n",
            " '- LLMs typically have static training data cutoffs, limiting access to '\n",
            " 'real-time or frequently updated information.\\n'\n",
            " '- The RAG pipeline enables dynamic access to external knowledge bases, '\n",
            " 'allowing the model to provide timely and evolving responses—critical for '\n",
            " 'applications like news summarization, legal research, or customer support.\\n'\n",
            " '\\n'\n",
            " '#### 5. **Increased Customizability and Domain Adaptability**\\n'\n",
            " '- Developers can tailor the retrieval component to pull from proprietary or '\n",
            " 'niche datasets (e.g., medical journals, technical manuals, enterprise '\n",
            " 'databases), enabling the LLM to function effectively in domain-specific '\n",
            " 'scenarios.\\n'\n",
            " '- This makes the model more adaptable and effective across diverse use cases '\n",
            " 'such as healthcare diagnostics, financial reporting, and legal advice.\\n'\n",
            " '\\n'\n",
            " '#### 6. **Efficient Use of Model Capabilities**\\n'\n",
            " '- The separation and coordination of retrieval and generation allow for an '\n",
            " 'efficient division of labor: the **retriever** handles information '\n",
            " 'discovery, while the **generator** focuses on synthesizing and articulating '\n",
            " 'responses.\\n'\n",
            " '- This modular design optimizes computational resources and improves '\n",
            " 'inference efficiency compared to training or fine-tuning a new LLM for each '\n",
            " 'new knowledge update.\\n'\n",
            " '\\n'\n",
            " '---\\n'\n",
            " '\\n'\n",
            " '### Summary:\\n'\n",
            " 'By tightly integrating retrieval and generation phases, LangChain’s RAG '\n",
            " 'pipeline empowers LLMs to deliver responses that are **more accurate, '\n",
            " 'factually grounded, contextually relevant, and adaptable to dynamic or '\n",
            " 'domain-specific knowledge**. This leads to improved overall performance in '\n",
            " 'terms of both **quality of output** and **user trust**, making RAG a '\n",
            " 'powerful approach for deploying LLMs in production environments where '\n",
            " 'reliability and precision are critical.')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "e6070fea-ffcf-49ca-ac99-7d7ed2744d40",
      "metadata": {
        "collapsed": true,
        "id": "e6070fea-ffcf-49ca-ac99-7d7ed2744d40",
        "outputId": "24aadd85-be80-467a-9bec-fa57a7af48da",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(\"The integration of **retrieval and generation phases** in LangChain's \"\n",
            " 'Retrieval-Augmented Generation (RAG) pipeline has a significant positive '\n",
            " 'impact on the **overall performance of large language models (LLMs)**. This '\n",
            " 'integration enhances model behavior across several key dimensions, including '\n",
            " 'accuracy, relevance, factual grounding, and contextual understanding.\\n'\n",
            " '\\n'\n",
            " '### Key Impacts on Model Performance:\\n'\n",
            " '\\n'\n",
            " '#### 1. **Improved Accuracy and Factual Consistency**\\n'\n",
            " '- The retrieval phase fetches relevant, up-to-date, or domain-specific '\n",
            " 'information from external knowledge sources (e.g., document databases, '\n",
            " 'vector stores).\\n'\n",
            " '- This retrieved context is then fed into the LLM during the generation '\n",
            " 'phase, allowing it to ground its output in verified facts rather than '\n",
            " 'relying solely on its pre-training data.\\n'\n",
            " '- As a result, the model produces responses that are more factually accurate '\n",
            " 'and consistent with current or specialized knowledge.\\n'\n",
            " '\\n'\n",
            " '#### 2. **Reduction in Hallucinations**\\n'\n",
            " '- One of the well-known challenges with LLMs is their tendency to generate '\n",
            " 'plausible-sounding but incorrect or fabricated information when they lack '\n",
            " 'sufficient internal knowledge about a topic.\\n'\n",
            " '- By integrating retrieval-augmented context, the RAG pipeline provides the '\n",
            " 'LLM with explicit supporting evidence for generating answers, significantly '\n",
            " 'reducing hallucinations.\\n'\n",
            " '\\n'\n",
            " '#### 3. **Enhanced Contextual Relevance**\\n'\n",
            " '- During retrieval, documents or snippets most semantically similar to the '\n",
            " 'user’s query are selected using **vector embeddings and similarity search** '\n",
            " 'techniques.\\n'\n",
            " '- This ensures that only the most contextually relevant information is used '\n",
            " 'in the generation process, leading to responses that are tightly aligned '\n",
            " 'with the intent and specifics of the query.\\n'\n",
            " '\\n'\n",
            " '#### 4. **Support for Real-Time and Dynamic Knowledge**\\n'\n",
            " '- LLMs typically have static training data cutoffs, limiting access to '\n",
            " 'real-time or frequently updated information.\\n'\n",
            " '- The RAG pipeline enables dynamic access to external knowledge bases, '\n",
            " 'allowing the model to provide timely and evolving responses—critical for '\n",
            " 'applications like news summarization, legal research, or customer support.\\n'\n",
            " '\\n'\n",
            " '#### 5. **Increased Customizability and Domain Adaptability**\\n'\n",
            " '- Developers can tailor the retrieval component to pull from proprietary or '\n",
            " 'niche datasets (e.g., medical journals, technical manuals, enterprise '\n",
            " 'databases), enabling the LLM to function effectively in domain-specific '\n",
            " 'scenarios.\\n'\n",
            " '- This makes the model more adaptable and effective across diverse use cases '\n",
            " 'such as healthcare diagnostics, financial reporting, and legal advice.\\n'\n",
            " '\\n'\n",
            " '#### 6. **Efficient Use of Model Capabilities**\\n'\n",
            " '- The separation and coordination of retrieval and generation allow for an '\n",
            " 'efficient division of labor: the **retriever** handles information '\n",
            " 'discovery, while the **generator** focuses on synthesizing and articulating '\n",
            " 'responses.\\n'\n",
            " '- This modular design optimizes computational resources and improves '\n",
            " 'inference efficiency compared to training or fine-tuning a new LLM for each '\n",
            " 'new knowledge update.\\n'\n",
            " '\\n'\n",
            " '---\\n'\n",
            " '\\n'\n",
            " '### Summary:\\n'\n",
            " 'By tightly integrating retrieval and generation phases, LangChain’s RAG '\n",
            " 'pipeline empowers LLMs to deliver responses that are **more accurate, '\n",
            " 'factually grounded, contextually relevant, and adaptable to dynamic or '\n",
            " 'domain-specific knowledge**. This leads to improved overall performance in '\n",
            " 'terms of both **quality of output** and **user trust**, making RAG a '\n",
            " 'powerful approach for deploying LLMs in production environments where '\n",
            " 'reliability and precision are critical.')\n"
          ]
        }
      ],
      "source": [
        "pprint(answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb0fa2e4-d4f1-42fc-a1ad-8eaeb05a0d3e",
      "metadata": {
        "id": "eb0fa2e4-d4f1-42fc-a1ad-8eaeb05a0d3e"
      },
      "source": [
        "### Answer individually\n",
        "\n",
        "或者，我们可以从每个单独的查询中获取答案，并将其直接传递给LLM，以根据之前的答案作为上下文生成最终答案。\n",
        "\n",
        "![answer-individually](https://github.com/tivon-x/bRAG-langchain/blob/main/notebooks/image/answer-individually.png?raw=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "297425fa-975b-4599-9b9e-a11139b99140",
      "metadata": {
        "id": "297425fa-975b-4599-9b9e-a11139b99140",
        "outputId": "572360d2-cb87-458f-cb4f-aecf5cc227aa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sq : 1. How does LangChain's Retrieval-Augmented Generation (RAG) pipeline enhance the accuracy of responses from large language models?  \n",
            "rdocs:  [Document(id='e58c9738-6d46-409e-8f9c-f29b625f6525', metadata={'author': '', 'creationdate': '2024-11-06T10:08:55+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-11-06T10:08:55+00:00', 'page': 2.0, 'page_label': '3', 'producer': 'pdfTeX-1.40.26', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'source': '../langchain_turing.pdf', 'subject': '', 'title': '', 'total_pages': 14.0, 'trapped': '/False'}, page_content='LangChain 3\\nneeds, providing a flexible foundation for building scalable, secure, and multi-\\nfunctional applications. Figure 1 illustrates a fundamental LangChain pipeline.\\nIn this architecture, diverse data sources—including documents, text, and im-\\nages—are embedded and stored within a vector store. Upon receiving a user’s\\nquery, the system retrieves the most relevant information from the vector store.\\nThis retrieved context is then provided to the large language model (LLM),\\nenhancing its ability to generate accurate and factually grounded responses.\\nFig. 1.LangChain pipeline architecture showcasing the retrieval-augmented genera-\\ntion process. Documents in various formats (e.g., PDF, text, images) are preloaded\\nand embedded into a vector store. When a user submits a query, the system retrieves\\nthe top-k most relevant documents based on vector similarity. These documents are\\ncombined with the query to provide contextual information to the language model\\n(LLM), which then generates an accurate and contextually enriched answer. This ar-\\nchitecture enhances the model’s ability to produce factually grounded responses by\\nincorporating relevant knowledge from the vector store.\\nThe rest of this section provides an overview of LangChain’s primary com-\\nponents, followed by a brief introduction to its advanced modules–LangSmith,\\nLangGraph and LangServe–which are further discussed in Sections 2, 3, and 4\\nrespectively:\\nLLM Interface: Provides APIs for connecting and querying various large lan-\\nguage models, such as OpenAI’s GPT [1], Google’s Gemini [14], and Llama [16],\\nto facilitate seamless application integration.\\nPromptTemplates:Structuredtemplatesthatstandardizeandformatqueries,\\nensuring consistency and precision in interactions with AI models. These tem-\\nplates help guide the model towards producing reliable and relevant outputs.'), Document(id='1f5db6db-18f6-431c-9adf-559b539f334c', metadata={'author': '', 'creationdate': '2024-11-06T10:08:55+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-11-06T10:08:55+00:00', 'page': 4.0, 'page_label': '5', 'producer': 'pdfTeX-1.40.26', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'source': '../langchain_turing.pdf', 'subject': '', 'title': '', 'total_pages': 14.0, 'trapped': '/False'}, page_content='LangChain 5\\nand relevance. RAG allows models to access up-to-date information, extending\\ntheir capabilities beyond their training data. LangChain’s RAG implementation\\nuses:\\n– Document Loaders and Text Splitters: Preprocess documents for in-\\ndexing and efficient retrieval [6].\\n– Embedding Models and Vector Stores: Enable similarity-based re-\\ntrieval by embedding documents into vector spaces. LangChain integrates\\nwithvectorstoragesolutionslikeChromaandMilvusforoptimizedsearches[3].\\n– Retrievers and RAG Chains: Retrieve and merge external data with\\nmodel responses, enhancing applications such as question answering systems\\nand recommendation engines [4].\\n1.3 Security and Permissions Management\\nSecurity is a critical focus in LangChain’s design, particularly given the potential\\naccess to external data sources. LangChain addresses these security challenges\\nthrough best practices and internal controls [3]:\\n– Granular Permissions: Enforces the principle of least privilege by allowing\\ndevelopers to specify limited permissions, minimizing the risk of unautho-\\nrized actions.\\n– Sandboxing and Defense in Depth: Utilizes sandboxed environments\\nand layered security to protect sensitive data and limit exposure to vulner-\\nabilities [3].\\n– Auditability and Monitoring: LangSmith (see Section 2) provides de-\\ntailed logging and monitoring capabilities, enabling developers to track ap-\\nplication usage and detect anomalies in real time.\\n1.4 Integrations and Extensibility\\nLangChain’s architecture supports a wide range of third-party integrations, al-\\nlowing for custom component development and additional functionality, such as\\nmulti-modal data processing and AI tool integration [3]:\\n– IntegrationPackages:LangChainprovidesdedicatedpackages(e.g.,langchain-\\nopenai, langchain-aws) that simplify connections to external platforms, tai-\\nloring applications to specific needs.\\n– Support for Multi-modal Data: Supports image, text, and audio inputs,\\nallowing for applications like chatbots capable of interpreting diverse data\\ntypes.\\n– CustomComponentDevelopment :Developerscanbuildcustomplugins\\nor extend LangChain components, ensuring flexibility and adaptability for\\na wide range of application requirements.\\nLangChain’s modular and flexible architecture equips developers with a com-\\nprehensive toolkit for building, deploying, and monitoring LLM applications. Its\\nadvanced components—LangGraph, LangServe, and LangSmith—enable sophis-\\nticated functionality for scalable, interactive, and robust applications, meeting\\nthe demands of modern AI use cases.'), Document(id='2220bd38-5524-4e38-9664-0097fd41be52', metadata={'author': '', 'creationdate': '2024-11-06T10:08:55+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-11-06T10:08:55+00:00', 'page': 3.0, 'page_label': '4', 'producer': 'pdfTeX-1.40.26', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'source': '../langchain_turing.pdf', 'subject': '', 'title': '', 'total_pages': 14.0, 'trapped': '/False'}, page_content='4 Vasilios Mavroudis\\nMemory: Enables applications to retain information from past interactions,\\nsupporting both basic and advanced memory structures. This component is crit-\\nical for maintaining context across sessions and delivering contextually aware\\nresponses.\\nIndexes: Serve as structured databases that organize and store information,\\nallowing for efficient data retrieval when processing language queries.\\nRetrievers: Designed to work alongside indexes, retrievers fetch relevant data\\nbased on query inputs, ensuring that the generated responses are well-informed\\nand accurate.\\nVector Store: Manages the embedding of words or phrases as numerical vec-\\ntors, a core step in capturing semantic meaning and supporting tasks involving\\nlanguage understanding and similarity searches.\\nOutput Parsers: Components that refine and structure the generated language\\noutputs for specific tasks, ensuring usability and relevance for the application’s\\ngoals.\\nAgents: Custom chains that prompt the language model to identify and execute\\nthe most effective sequence of actions for a given query, enabling adaptive and\\ndynamic decision-making.\\nCallbacks:Functionsthatlog,monitor,andstreamspecificeventswithinLangChain\\nworkflows, simplifying tracking and debugging processes.\\n1.1 Chat Models and Message Handling\\nLangChain supports chat models that manage complex, multi-turn conversa-\\ntions. These models use structured message sequences, allowing developers to\\ncontrol conversation flow and maintain state over time. The structured message\\nhandling system enables robust interactions with users by storing and retrieving\\nconversation history as needed [6]. Their key features include:\\n– Multi-turn Interactions: LangChain maintains state across conversation\\nturns, making it suitable for prolonged, context-dependent conversations.\\n– Structured Output: Supports structured responses like JSON, allowing\\neasy integration with downstream applications.\\n– Conversation Memory: Maintains continuity by storing conversation his-\\ntory, ideal for applications requiring persistent context, such as customer\\nsupport [4].\\n1.2 Retrieval-Augmented Generation (RAG)\\nLangChain supports Retrieval-Augmented Generation (RAG), which integrates\\nlanguage models with external knowledge bases to enhance response accuracy'), Document(id='4aa1e75e-2831-4b6f-9fe9-a3dbae0a2cf7', metadata={'author': '', 'creationdate': '2024-11-06T10:08:55+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-11-06T10:08:55+00:00', 'page': 12.0, 'page_label': '13', 'producer': 'pdfTeX-1.40.26', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'source': '../langchain_turing.pdf', 'subject': '', 'title': '', 'total_pages': 14.0, 'trapped': '/False'}, page_content='LangChain 13\\nLangChain’s security model addresses many of these concerns, yet challenges\\npersist, particularly in sectors with rigorous compliance standards, such as fi-\\nnance and healthcare. Key areas for ongoing improvement include:\\n– DynamicPermissionAdjustment :CurrentpermissionsettingsinLangChain\\nare defined at deployment, but in dynamic applications, permissions may\\nneed to adapt based on user interactions. Implementing adaptive permis-\\nsions responsive to application state or user roles could enhance security.\\n– Advanced Encryption Standards: For applications processing highly\\nsensitive data, adopting advanced encryption practices—such as end-to-end\\nor field-level encryption—could bolster data security within even trusted\\nenvironments.\\n– Proactive Security Analytics: Integrating predictive analytics to pre-\\nemptively identify risks could further secure applications. Machine learning\\nmodels analyzing application logs could flag anomalous patterns indicative\\nof potential breaches or misuse.\\nIn summary, LangChain’s security framework includes robust features such\\nas granular permissions, sandboxing, and real-time monitoring. While these mea-\\nsures provide a solid foundation, the ongoing challenge of securing LLM-driven\\napplications—particularly those relying on external providers—demands contin-\\nued advancements in security practices.\\n6 Conclusion\\nLangChain significantly advances the development of applications powered by\\nlarge language models (LLMs). Its modular framework—including components\\nlike LangGraph for stateful process modeling, LangServe for scalable API de-\\nployment, and LangSmith for monitoring and evaluation—enables developers to\\nbuild scalable, context-aware applications tailored to specific needs across di-\\nverse domains, including NLP, cybersecurity, healthcare, finance, and customer\\nservice.\\nWhile its versatility extends beyond NLP, allowing for applications in fields\\nlike cybersecurity (e.g., threat detection and automated incident response), the\\nframework’s emphasis on flexibility introduces complexities that may present a\\nlearning curve for developers new to LangChain. Additionally, reliance on exter-\\nnal integrations raises important security considerations, such as data exposure\\nand dependency vulnerabilities, which are critical in sensitive areas where data\\nintegrity and privacy are paramount.\\nIn summary, LangChain’s transformative potential lies in bridging the gap\\nbetween the power of large language models and practical application develop-\\nment across multiple fields. By balancing its robust capabilities with enhance-\\nments in usability and security, LangChain can continue to serve as a valuable\\ntool for developers seeking to leverage LLMs in building innovative and secure\\napplications. As industries increasingly adopt AI technologies, frameworks like\\nLangChain are poised to play a pivotal role in shaping the next generation of\\nintelligent, scalable, and secure solutions across various sectors.')]\n",
            "q:  What role does LangChain's Retrieval-Augmented Generation (RAG) pipeline play in improving the accuracy and relevance of LLM responses?\n",
            "a:  LangChain's Retrieval-Augmented Generation (RAG) pipeline enhances the accuracy of responses by integrating external knowledge sources with large language models (LLMs). When a user submits a query, the system retrieves the most relevant information from a vector store and combines it with the query to provide contextual information to the LLM. This ensures that the generated response is factually grounded and enriched with up-to-date or domain-specific knowledge beyond the model’s training data.\n",
            "sq : 2. What is the role of external data retrieval in improving the relevance of LLM-generated answers within the RAG framework?  \n",
            "rdocs:  [Document(id='1f5db6db-18f6-431c-9adf-559b539f334c', metadata={'author': '', 'creationdate': '2024-11-06T10:08:55+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-11-06T10:08:55+00:00', 'page': 4.0, 'page_label': '5', 'producer': 'pdfTeX-1.40.26', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'source': '../langchain_turing.pdf', 'subject': '', 'title': '', 'total_pages': 14.0, 'trapped': '/False'}, page_content='LangChain 5\\nand relevance. RAG allows models to access up-to-date information, extending\\ntheir capabilities beyond their training data. LangChain’s RAG implementation\\nuses:\\n– Document Loaders and Text Splitters: Preprocess documents for in-\\ndexing and efficient retrieval [6].\\n– Embedding Models and Vector Stores: Enable similarity-based re-\\ntrieval by embedding documents into vector spaces. LangChain integrates\\nwithvectorstoragesolutionslikeChromaandMilvusforoptimizedsearches[3].\\n– Retrievers and RAG Chains: Retrieve and merge external data with\\nmodel responses, enhancing applications such as question answering systems\\nand recommendation engines [4].\\n1.3 Security and Permissions Management\\nSecurity is a critical focus in LangChain’s design, particularly given the potential\\naccess to external data sources. LangChain addresses these security challenges\\nthrough best practices and internal controls [3]:\\n– Granular Permissions: Enforces the principle of least privilege by allowing\\ndevelopers to specify limited permissions, minimizing the risk of unautho-\\nrized actions.\\n– Sandboxing and Defense in Depth: Utilizes sandboxed environments\\nand layered security to protect sensitive data and limit exposure to vulner-\\nabilities [3].\\n– Auditability and Monitoring: LangSmith (see Section 2) provides de-\\ntailed logging and monitoring capabilities, enabling developers to track ap-\\nplication usage and detect anomalies in real time.\\n1.4 Integrations and Extensibility\\nLangChain’s architecture supports a wide range of third-party integrations, al-\\nlowing for custom component development and additional functionality, such as\\nmulti-modal data processing and AI tool integration [3]:\\n– IntegrationPackages:LangChainprovidesdedicatedpackages(e.g.,langchain-\\nopenai, langchain-aws) that simplify connections to external platforms, tai-\\nloring applications to specific needs.\\n– Support for Multi-modal Data: Supports image, text, and audio inputs,\\nallowing for applications like chatbots capable of interpreting diverse data\\ntypes.\\n– CustomComponentDevelopment :Developerscanbuildcustomplugins\\nor extend LangChain components, ensuring flexibility and adaptability for\\na wide range of application requirements.\\nLangChain’s modular and flexible architecture equips developers with a com-\\nprehensive toolkit for building, deploying, and monitoring LLM applications. Its\\nadvanced components—LangGraph, LangServe, and LangSmith—enable sophis-\\nticated functionality for scalable, interactive, and robust applications, meeting\\nthe demands of modern AI use cases.'), Document(id='e58c9738-6d46-409e-8f9c-f29b625f6525', metadata={'author': '', 'creationdate': '2024-11-06T10:08:55+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-11-06T10:08:55+00:00', 'page': 2.0, 'page_label': '3', 'producer': 'pdfTeX-1.40.26', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'source': '../langchain_turing.pdf', 'subject': '', 'title': '', 'total_pages': 14.0, 'trapped': '/False'}, page_content='LangChain 3\\nneeds, providing a flexible foundation for building scalable, secure, and multi-\\nfunctional applications. Figure 1 illustrates a fundamental LangChain pipeline.\\nIn this architecture, diverse data sources—including documents, text, and im-\\nages—are embedded and stored within a vector store. Upon receiving a user’s\\nquery, the system retrieves the most relevant information from the vector store.\\nThis retrieved context is then provided to the large language model (LLM),\\nenhancing its ability to generate accurate and factually grounded responses.\\nFig. 1.LangChain pipeline architecture showcasing the retrieval-augmented genera-\\ntion process. Documents in various formats (e.g., PDF, text, images) are preloaded\\nand embedded into a vector store. When a user submits a query, the system retrieves\\nthe top-k most relevant documents based on vector similarity. These documents are\\ncombined with the query to provide contextual information to the language model\\n(LLM), which then generates an accurate and contextually enriched answer. This ar-\\nchitecture enhances the model’s ability to produce factually grounded responses by\\nincorporating relevant knowledge from the vector store.\\nThe rest of this section provides an overview of LangChain’s primary com-\\nponents, followed by a brief introduction to its advanced modules–LangSmith,\\nLangGraph and LangServe–which are further discussed in Sections 2, 3, and 4\\nrespectively:\\nLLM Interface: Provides APIs for connecting and querying various large lan-\\nguage models, such as OpenAI’s GPT [1], Google’s Gemini [14], and Llama [16],\\nto facilitate seamless application integration.\\nPromptTemplates:Structuredtemplatesthatstandardizeandformatqueries,\\nensuring consistency and precision in interactions with AI models. These tem-\\nplates help guide the model towards producing reliable and relevant outputs.'), Document(id='2220bd38-5524-4e38-9664-0097fd41be52', metadata={'author': '', 'creationdate': '2024-11-06T10:08:55+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-11-06T10:08:55+00:00', 'page': 3.0, 'page_label': '4', 'producer': 'pdfTeX-1.40.26', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'source': '../langchain_turing.pdf', 'subject': '', 'title': '', 'total_pages': 14.0, 'trapped': '/False'}, page_content='4 Vasilios Mavroudis\\nMemory: Enables applications to retain information from past interactions,\\nsupporting both basic and advanced memory structures. This component is crit-\\nical for maintaining context across sessions and delivering contextually aware\\nresponses.\\nIndexes: Serve as structured databases that organize and store information,\\nallowing for efficient data retrieval when processing language queries.\\nRetrievers: Designed to work alongside indexes, retrievers fetch relevant data\\nbased on query inputs, ensuring that the generated responses are well-informed\\nand accurate.\\nVector Store: Manages the embedding of words or phrases as numerical vec-\\ntors, a core step in capturing semantic meaning and supporting tasks involving\\nlanguage understanding and similarity searches.\\nOutput Parsers: Components that refine and structure the generated language\\noutputs for specific tasks, ensuring usability and relevance for the application’s\\ngoals.\\nAgents: Custom chains that prompt the language model to identify and execute\\nthe most effective sequence of actions for a given query, enabling adaptive and\\ndynamic decision-making.\\nCallbacks:Functionsthatlog,monitor,andstreamspecificeventswithinLangChain\\nworkflows, simplifying tracking and debugging processes.\\n1.1 Chat Models and Message Handling\\nLangChain supports chat models that manage complex, multi-turn conversa-\\ntions. These models use structured message sequences, allowing developers to\\ncontrol conversation flow and maintain state over time. The structured message\\nhandling system enables robust interactions with users by storing and retrieving\\nconversation history as needed [6]. Their key features include:\\n– Multi-turn Interactions: LangChain maintains state across conversation\\nturns, making it suitable for prolonged, context-dependent conversations.\\n– Structured Output: Supports structured responses like JSON, allowing\\neasy integration with downstream applications.\\n– Conversation Memory: Maintains continuity by storing conversation his-\\ntory, ideal for applications requiring persistent context, such as customer\\nsupport [4].\\n1.2 Retrieval-Augmented Generation (RAG)\\nLangChain supports Retrieval-Augmented Generation (RAG), which integrates\\nlanguage models with external knowledge bases to enhance response accuracy'), Document(id='dacade29-2e41-4332-8077-f2d2fc4b3e0d', metadata={'author': '', 'creationdate': '2024-11-06T10:08:55+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-11-06T10:08:55+00:00', 'page': 13.0, 'page_label': '14', 'producer': 'pdfTeX-1.40.26', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'source': '../langchain_turing.pdf', 'subject': '', 'title': '', 'total_pages': 14.0, 'trapped': '/False'}, page_content='14 Vasilios Mavroudis\\nReferences\\n1. Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Flo-\\nrencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal\\nAnadkat, et al. GPT-4 Technical Report.arXiv preprint arXiv:2303.08774, 2023.\\n2. Harrison Chase. LangChain, Oct 2022. Available at https://github.com/\\nlangchain-ai/langchain.\\n3. LangChain, Inc. LangChain Documentation: Integration Providers. LangChain,\\nInc.,SanFrancisco,CA,2024. Availableat https://python.langchain.com/docs/\\nintegrations/providers/.\\n4. LangChain, Inc. LangChain Documentation: Key Concepts. LangChain, Inc.,\\nSan Francisco, CA, 2024. Available at https://python.langchain.com/docs/\\nconcepts/.\\n5. LangChain, Inc. LangChain Documentation: LangServe. LangChain, Inc.,\\nSan Francisco, CA, 2024. Available at https://python.langchain.com/docs/\\nlangserve/.\\n6. LangChain, Inc. LangChain Documentation: Security Best Practices. LangChain,\\nInc.,SanFrancisco,CA,2024. Availableat https://python.langchain.com/docs/\\nsecurity/.\\n7. LangChain, Inc.LangGraph: Building Language Agents as Graphs, 2024. Accessed:\\n2024-11-04.\\n8. LangChain, Inc.LangGraph Platform Documentation, 2024. Accessed: 2024-11-04.\\n9. LangChain, Inc. LangSmith: A Developer Platform for LLM Applications, 2024.\\nAccessed: 2024-11-04.\\n10. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir\\nKarpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock-\\ntäschel, et al. Retrieval-augmented generation for knowledge-intensive nlp tasks.\\nAdvances in Neural Information Processing Systems, 33:9459–9474, 2020.\\n11. Grzegorz Malewicz, Matthew H Austern, Aart JC Bik, James C Dehnert, Ilan\\nHorn, Naty Leiser, and Grzegorz Czajkowski. Pregel: A System for Large-Scale\\nGraph Processing. InProceedings of the 2010 ACM SIGMOD International Con-\\nference on Management of Data, pages 135–146, 2010.\\n12. OpenAI. Hello GPT-4O, 05 2024.\\n13. OpenAI. Introducing OpenAI O1-Preview, 09 2024.\\n14. Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui\\nYu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, Katie Millican,\\net al. Gemini: A Family of Highly Capable Multimodal Models.arXiv preprint\\narXiv:2312.11805, 2023.\\n15. The Apache Software Foundation.Apache Beam: An Advanced Unified Program-\\nming Model, 2024. Accessed: 2024-11-04.\\n16. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne\\nLachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal\\nAzhar, et al. LLaMA: Open and Efficient Foundation Language Models.arXiv\\npreprint arXiv:2302.13971, 2023.')]\n",
            "q:  What role does LangChain's Retrieval-Augmented Generation (RAG) pipeline play in improving the accuracy and relevance of LLM responses?\n",
            "a:  External data retrieval in the RAG framework enhances the relevance of LLM-generated answers by providing up-to-date and contextually specific information beyond the model's training data. It allows the model to access a vector store of embedded documents, retrieving the most relevant content based on similarity searches. This retrieved context is then combined with user queries to generate more accurate and factually grounded responses.\n",
            "sq : 3. How does the integration of retrieval and generation phases in LangChain’s RAG pipeline impact overall model performance?\n",
            "rdocs:  [Document(id='e58c9738-6d46-409e-8f9c-f29b625f6525', metadata={'author': '', 'creationdate': '2024-11-06T10:08:55+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-11-06T10:08:55+00:00', 'page': 2.0, 'page_label': '3', 'producer': 'pdfTeX-1.40.26', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'source': '../langchain_turing.pdf', 'subject': '', 'title': '', 'total_pages': 14.0, 'trapped': '/False'}, page_content='LangChain 3\\nneeds, providing a flexible foundation for building scalable, secure, and multi-\\nfunctional applications. Figure 1 illustrates a fundamental LangChain pipeline.\\nIn this architecture, diverse data sources—including documents, text, and im-\\nages—are embedded and stored within a vector store. Upon receiving a user’s\\nquery, the system retrieves the most relevant information from the vector store.\\nThis retrieved context is then provided to the large language model (LLM),\\nenhancing its ability to generate accurate and factually grounded responses.\\nFig. 1.LangChain pipeline architecture showcasing the retrieval-augmented genera-\\ntion process. Documents in various formats (e.g., PDF, text, images) are preloaded\\nand embedded into a vector store. When a user submits a query, the system retrieves\\nthe top-k most relevant documents based on vector similarity. These documents are\\ncombined with the query to provide contextual information to the language model\\n(LLM), which then generates an accurate and contextually enriched answer. This ar-\\nchitecture enhances the model’s ability to produce factually grounded responses by\\nincorporating relevant knowledge from the vector store.\\nThe rest of this section provides an overview of LangChain’s primary com-\\nponents, followed by a brief introduction to its advanced modules–LangSmith,\\nLangGraph and LangServe–which are further discussed in Sections 2, 3, and 4\\nrespectively:\\nLLM Interface: Provides APIs for connecting and querying various large lan-\\nguage models, such as OpenAI’s GPT [1], Google’s Gemini [14], and Llama [16],\\nto facilitate seamless application integration.\\nPromptTemplates:Structuredtemplatesthatstandardizeandformatqueries,\\nensuring consistency and precision in interactions with AI models. These tem-\\nplates help guide the model towards producing reliable and relevant outputs.'), Document(id='1f5db6db-18f6-431c-9adf-559b539f334c', metadata={'author': '', 'creationdate': '2024-11-06T10:08:55+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-11-06T10:08:55+00:00', 'page': 4.0, 'page_label': '5', 'producer': 'pdfTeX-1.40.26', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'source': '../langchain_turing.pdf', 'subject': '', 'title': '', 'total_pages': 14.0, 'trapped': '/False'}, page_content='LangChain 5\\nand relevance. RAG allows models to access up-to-date information, extending\\ntheir capabilities beyond their training data. LangChain’s RAG implementation\\nuses:\\n– Document Loaders and Text Splitters: Preprocess documents for in-\\ndexing and efficient retrieval [6].\\n– Embedding Models and Vector Stores: Enable similarity-based re-\\ntrieval by embedding documents into vector spaces. LangChain integrates\\nwithvectorstoragesolutionslikeChromaandMilvusforoptimizedsearches[3].\\n– Retrievers and RAG Chains: Retrieve and merge external data with\\nmodel responses, enhancing applications such as question answering systems\\nand recommendation engines [4].\\n1.3 Security and Permissions Management\\nSecurity is a critical focus in LangChain’s design, particularly given the potential\\naccess to external data sources. LangChain addresses these security challenges\\nthrough best practices and internal controls [3]:\\n– Granular Permissions: Enforces the principle of least privilege by allowing\\ndevelopers to specify limited permissions, minimizing the risk of unautho-\\nrized actions.\\n– Sandboxing and Defense in Depth: Utilizes sandboxed environments\\nand layered security to protect sensitive data and limit exposure to vulner-\\nabilities [3].\\n– Auditability and Monitoring: LangSmith (see Section 2) provides de-\\ntailed logging and monitoring capabilities, enabling developers to track ap-\\nplication usage and detect anomalies in real time.\\n1.4 Integrations and Extensibility\\nLangChain’s architecture supports a wide range of third-party integrations, al-\\nlowing for custom component development and additional functionality, such as\\nmulti-modal data processing and AI tool integration [3]:\\n– IntegrationPackages:LangChainprovidesdedicatedpackages(e.g.,langchain-\\nopenai, langchain-aws) that simplify connections to external platforms, tai-\\nloring applications to specific needs.\\n– Support for Multi-modal Data: Supports image, text, and audio inputs,\\nallowing for applications like chatbots capable of interpreting diverse data\\ntypes.\\n– CustomComponentDevelopment :Developerscanbuildcustomplugins\\nor extend LangChain components, ensuring flexibility and adaptability for\\na wide range of application requirements.\\nLangChain’s modular and flexible architecture equips developers with a com-\\nprehensive toolkit for building, deploying, and monitoring LLM applications. Its\\nadvanced components—LangGraph, LangServe, and LangSmith—enable sophis-\\nticated functionality for scalable, interactive, and robust applications, meeting\\nthe demands of modern AI use cases.'), Document(id='2220bd38-5524-4e38-9664-0097fd41be52', metadata={'author': '', 'creationdate': '2024-11-06T10:08:55+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-11-06T10:08:55+00:00', 'page': 3.0, 'page_label': '4', 'producer': 'pdfTeX-1.40.26', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'source': '../langchain_turing.pdf', 'subject': '', 'title': '', 'total_pages': 14.0, 'trapped': '/False'}, page_content='4 Vasilios Mavroudis\\nMemory: Enables applications to retain information from past interactions,\\nsupporting both basic and advanced memory structures. This component is crit-\\nical for maintaining context across sessions and delivering contextually aware\\nresponses.\\nIndexes: Serve as structured databases that organize and store information,\\nallowing for efficient data retrieval when processing language queries.\\nRetrievers: Designed to work alongside indexes, retrievers fetch relevant data\\nbased on query inputs, ensuring that the generated responses are well-informed\\nand accurate.\\nVector Store: Manages the embedding of words or phrases as numerical vec-\\ntors, a core step in capturing semantic meaning and supporting tasks involving\\nlanguage understanding and similarity searches.\\nOutput Parsers: Components that refine and structure the generated language\\noutputs for specific tasks, ensuring usability and relevance for the application’s\\ngoals.\\nAgents: Custom chains that prompt the language model to identify and execute\\nthe most effective sequence of actions for a given query, enabling adaptive and\\ndynamic decision-making.\\nCallbacks:Functionsthatlog,monitor,andstreamspecificeventswithinLangChain\\nworkflows, simplifying tracking and debugging processes.\\n1.1 Chat Models and Message Handling\\nLangChain supports chat models that manage complex, multi-turn conversa-\\ntions. These models use structured message sequences, allowing developers to\\ncontrol conversation flow and maintain state over time. The structured message\\nhandling system enables robust interactions with users by storing and retrieving\\nconversation history as needed [6]. Their key features include:\\n– Multi-turn Interactions: LangChain maintains state across conversation\\nturns, making it suitable for prolonged, context-dependent conversations.\\n– Structured Output: Supports structured responses like JSON, allowing\\neasy integration with downstream applications.\\n– Conversation Memory: Maintains continuity by storing conversation his-\\ntory, ideal for applications requiring persistent context, such as customer\\nsupport [4].\\n1.2 Retrieval-Augmented Generation (RAG)\\nLangChain supports Retrieval-Augmented Generation (RAG), which integrates\\nlanguage models with external knowledge bases to enhance response accuracy'), Document(id='7b4659b1-fbf7-4574-b131-e7390e2ceca0', metadata={'author': '', 'creationdate': '2024-11-06T10:08:55+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-11-06T10:08:55+00:00', 'page': 1.0, 'page_label': '2', 'producer': 'pdfTeX-1.40.26', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'source': '../langchain_turing.pdf', 'subject': '', 'title': '', 'total_pages': 14.0, 'trapped': '/False'}, page_content='2 Vasilios Mavroudis\\nstateful, and contextually aware applications with ease. Its suite of compo-\\nnents—including LangGraph for stateful process modeling, LangServe for scal-\\nableAPIdeployment,andLangSmithformonitoringandevaluation—collectively\\nform a comprehensive toolkit for leveraging LLMs effectively [3].\\nLangChain facilitates the integration of LLMs into a wide array of applica-\\ntions, empowering developers to create solutions that are not only functional\\nbut also efficient and secure. Its support for features like chat models, retrieval-\\naugmented generation (RAG) [10], and secure API interactions allows for the\\nrapid deployment of sophisticated language model solutions across diverse do-\\nmains such as healthcare, customer service, finance, and mental health.\\nDespite its strengths, LangChain’s emphasis on flexibility through modular-\\nity introduces certain complexities. Developers may encounter a steep learning\\ncurve when navigating its extensive components and integrations. Moreover, the\\nreliance on external integrations and third-party providers necessitates a careful\\nexamination of security practices to mitigate risks associated with data exposure\\nand dependency vulnerabilities.\\nThis paper provides a comprehensive analysis of LangChain, delving into its\\narchitecture, core components, and the interplay between its modules. We ex-\\nplore how LangChain facilitates the development of LLM applications by exam-\\nining each component’s functionality and their synergistic contributions to the\\nframework. Furthermore, we critically evaluate the limitations and criticisms of\\nLangChain, focusing on the complexities introduced by its modular design and\\nthe security implications of its extensive integrations.\\nBy offering valuable insights into both the capabilities and challenges of\\nLangChain, this paper aims to serve as a key resource for developers and re-\\nsearchers interested in LLM application development. We seek to illuminate\\nthe transformative potential of LangChain in advancing NLP applications while\\nproviding a nuanced understanding of its practical boundaries. Ultimately, this\\nanalysis guides users in effectively harnessing LangChain to build innovative and\\nsecure LLM-powered applications tailored to their specific needs.\\nThe remainder of this paper is organized as follows: Section 1 delves into\\nthe core architecture of LangChain, detailing its primary components and their\\nfunctionalities. Section 2 examines LangSmith and its role in monitoring and\\nevaluation of LLM applications. In Section 3, we explore LangGraph’s capabili-\\nties in stateful process modeling. Section 4 discusses LangServe for scalable API\\ndeployment of LangChain applications. Finally, section 5 addresses the limita-\\ntions and criticisms of LangChain, particularly focusing on the complexities and\\nsecurity concerns associated with its modular design and external integrations.\\n1 Architecture\\nLangChain is built with a modular architecture, designed to simplify the life-\\ncycle of applications powered by large language models (LLMs), from initial\\ndevelopment through to deployment and monitoring [3]. This modularity al-\\nlows developers to configure, extend, and deploy applications tailored to specific')]\n",
            "q:  What role does LangChain's Retrieval-Augmented Generation (RAG) pipeline play in improving the accuracy and relevance of LLM responses?\n",
            "a:  The integration of retrieval and generation phases in LangChain’s RAG pipeline enhances model performance by providing up-to-date, relevant context from external knowledge sources, which improves the accuracy and factual grounding of responses. This approach allows models to go beyond their training data, incorporating real-world information for more informed outputs. Additionally, it supports dynamic and context-aware applications such as question answering systems and recommendation engines.\n"
          ]
        }
      ],
      "source": [
        "# Answer each sub-question individually\n",
        "from langchain import hub\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# RAG prompt\n",
        "\n",
        "template = \"\"\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
        "Context: {context}\n",
        "Question: {question}\n",
        "Answer:\n",
        "\"\"\"\n",
        "\n",
        "prompt_rag = ChatPromptTemplate.from_template(template)\n",
        "# prompt_rag = hub.pull(\"rlm/rag-prompt\")\n",
        "# template = \" {question} \"\n",
        "# prompt_rag = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "def retrieve_and_rag(question,prompt_rag,sub_question_generator_chain):\n",
        "    \"\"\"RAG on each sub-question\"\"\"\n",
        "    retriever = vectorstore.as_retriever()\n",
        "\n",
        "    # Use our decomposition /\n",
        "    sub_questions = sub_question_generator_chain.invoke({\"question\":question})\n",
        "\n",
        "    # Initialize a list to hold RAG chain results\n",
        "    rag_results = []\n",
        "\n",
        "    for sub_question in sub_questions:\n",
        "\n",
        "        print(\"sq :\", sub_question)\n",
        "\n",
        "        # Retrieve documents for each sub-question\n",
        "        retrieved_docs = retriever.invoke(sub_question)\n",
        "        print(\"rdocs: \", retrieved_docs)\n",
        "\n",
        "        # Use retrieved documents and sub-question in RAG chain\n",
        "        answer = (prompt_rag | llm | StrOutputParser()).invoke({\"context\": retrieved_docs, \"question\": sub_question})\n",
        "\n",
        "        print(\"q: \", question)\n",
        "        print(\"a: \", answer)\n",
        "\n",
        "        rag_results.append(answer)\n",
        "\n",
        "    return rag_results,sub_questions\n",
        "\n",
        "# Wrap the retrieval and RAG process in a RunnableLambda for integration into a chain\n",
        "answers, questions = retrieve_and_rag(question, prompt_rag, generate_queries_decomposition)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"qqq: \", questions, \"\\naaa: \", answers)"
      ],
      "metadata": {
        "id": "5fLtH8GWzIxV",
        "outputId": "8792d56e-4439-4d87-cb31-db4016485de5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "5fLtH8GWzIxV",
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "qqq:  [\"1. How does LangChain's Retrieval-Augmented Generation (RAG) pipeline enhance the accuracy of responses from large language models?  \", '2. What is the role of external data retrieval in improving the relevance of LLM-generated answers within the RAG framework?  ', '3. How does the integration of retrieval and generation phases in LangChain’s RAG pipeline impact overall model performance?'] \n",
            "aaa:  [\"LangChain's Retrieval-Augmented Generation (RAG) pipeline enhances the accuracy of responses by integrating external knowledge sources with large language models (LLMs). When a user submits a query, the system retrieves the most relevant information from a vector store and combines it with the query to provide contextual information to the LLM. This ensures that the generated response is factually grounded and enriched with up-to-date or domain-specific knowledge beyond the model’s training data.\", \"External data retrieval in the RAG framework enhances the relevance of LLM-generated answers by providing up-to-date and contextually specific information beyond the model's training data. It allows the model to access a vector store of embedded documents, retrieving the most relevant content based on similarity searches. This retrieved context is then combined with user queries to generate more accurate and factually grounded responses.\", 'The integration of retrieval and generation phases in LangChain’s RAG pipeline enhances model performance by providing up-to-date, relevant context from external knowledge sources, which improves the accuracy and factual grounding of responses. This approach allows models to go beyond their training data, incorporating real-world information for more informed outputs. Additionally, it supports dynamic and context-aware applications such as question answering systems and recommendation engines.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "b8631dda-bbcd-437c-81b3-5db7abb831f9",
      "metadata": {
        "id": "b8631dda-bbcd-437c-81b3-5db7abb831f9",
        "outputId": "908d0207-fcad-477c-8410-c3a2d2f3276d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Question:\n",
            " What role does LangChain's Retrieval-Augmented Generation (RAG) pipeline play in improving the accuracy and relevance of LLM responses?\n",
            "\n",
            "Context (including 3 question:answer pairs from modified query ):\n",
            " Question 1: 1. How does LangChain's Retrieval-Augmented Generation (RAG) pipeline enhance the accuracy of responses from large language models?  \n",
            "Answer 1: LangChain's Retrieval-Augmented Generation (RAG) pipeline enhances the accuracy of responses by integrating external knowledge sources with large language models (LLMs). When a user submits a query, the system retrieves the most relevant information from a vector store and combines it with the query to provide contextual information to the LLM. This ensures that the generated response is factually grounded and enriched with up-to-date or domain-specific knowledge beyond the model’s training data.\n",
            "\n",
            "Question 2: 2. What is the role of external data retrieval in improving the relevance of LLM-generated answers within the RAG framework?  \n",
            "Answer 2: External data retrieval in the RAG framework enhances the relevance of LLM-generated answers by providing up-to-date and contextually specific information beyond the model's training data. It allows the model to access a vector store of embedded documents, retrieving the most relevant content based on similarity searches. This retrieved context is then combined with user queries to generate more accurate and factually grounded responses.\n",
            "\n",
            "Question 3: 3. How does the integration of retrieval and generation phases in LangChain’s RAG pipeline impact overall model performance?\n",
            "Answer 3: The integration of retrieval and generation phases in LangChain’s RAG pipeline enhances model performance by providing up-to-date, relevant context from external knowledge sources, which improves the accuracy and factual grounding of responses. This approach allows models to go beyond their training data, incorporating real-world information for more informed outputs. Additionally, it supports dynamic and context-aware applications such as question answering systems and recommendation engines.\n",
            "\n",
            "Final Answer:\n",
            " LangChain's Retrieval-Augmented Generation (RAG) pipeline plays a crucial role in improving the **accuracy and relevance** of Large Language Model (LLM) responses by combining external knowledge retrieval with language generation. When answering a query, the RAG pipeline first retrieves the most relevant information from a vector store—typically composed of domain-specific or up-to-date documents—and then uses that context alongside the original query to guide the LLM’s response generation.\n",
            "\n",
            "This dual-phase approach ensures that responses are not only grounded in factual, current, or specialized knowledge beyond the model’s training data but also tailored to the specific context of the user's question. As a result, the integration of retrieval and generation enhances the model's ability to deliver precise, context-aware, and factually supported answers, making it especially valuable for applications like question-answering systems, recommendation engines, and other knowledge-intensive tasks.\n"
          ]
        }
      ],
      "source": [
        "def format_qa_pairs(questions, answers):\n",
        "    \"\"\"Format Q and A pairs\"\"\"\n",
        "\n",
        "    formatted_string = \"\"\n",
        "    for i, (question, answer) in enumerate(zip(questions, answers), start=1):\n",
        "        formatted_string += f\"Question {i}: {question}\\nAnswer {i}: {answer}\\n\\n\"\n",
        "    return formatted_string.strip()\n",
        "\n",
        "context = format_qa_pairs(questions, answers)\n",
        "\n",
        "# Prompt\n",
        "template = \"\"\"Here is a set of Q+A pairs:\n",
        "\n",
        "{context}\n",
        "\n",
        "Use these to synthesize an answer to the question: {question}\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "final_rag_chain = (\n",
        "    prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "\n",
        "print(\"Original Question:\\n\", question)\n",
        "print(\"\\nContext (including 3 question:answer pairs from modified query ):\\n\", context)\n",
        "\n",
        "print(\"\\nFinal Answer:\\n\", final_rag_chain.invoke({\"context\":context,\"question\":question}))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17698863-e683-48f4-b50e-adaa2bdee55d",
      "metadata": {
        "id": "17698863-e683-48f4-b50e-adaa2bdee55d"
      },
      "source": [
        "<!-- Trace:\n",
        "\n",
        "https://smith.langchain.com/public/ed1cabf5-dea0-478b-8088-f7323d938a9b/r -->"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6126bebb-94e5-48ef-9a17-6a315ed0a596",
      "metadata": {
        "id": "6126bebb-94e5-48ef-9a17-6a315ed0a596"
      },
      "source": [
        "## Part 8: Step Back\n",
        "\n",
        "后退法是一种解决问题和信息检索的技术，它通过生成更抽象或更高层次的问题，而不是直接回答原问题。这种方法被称为“后退提示”，强调通过提出一般性问题来理解更广泛的背景和基本概念，从而提供更大的视角。该方法包括使用例子来引导抽象问题的形成，并允许独立检索与原问题和后退问题相关的信息。这种双重检索过程可以提升理解并生成更全面的回答，特别适用于技术文档和教科书等需要大量概念知识的领域，通过分别处理高层次概念及其详细实现来增强实用性。\n",
        "\n",
        "![step-back](https://github.com/tivon-x/bRAG-langchain/blob/main/notebooks/image/step-back.png?raw=1)\n",
        "\n",
        "Paper:\n",
        "\n",
        "* https://arxiv.org/pdf/2310.06117.pdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "1d74f9f2-543d-4e41-b90b-7bb527eca1d9",
      "metadata": {
        "id": "1d74f9f2-543d-4e41-b90b-7bb527eca1d9"
      },
      "outputs": [],
      "source": [
        "# Few Shot 样本\n",
        "from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
        "examples = [\n",
        "    {\n",
        "        \"input\": \"Could the members of The Police perform lawful arrests?\",\n",
        "        \"output\": \"what can the members of The Police do?\",\n",
        "    },\n",
        "    {\n",
        "        \"input\": \"Jan Sindel’s was born in what country?\",\n",
        "        \"output\": \"what is Jan Sindel’s personal history?\",\n",
        "    },\n",
        "]\n",
        "# 转换成相应的消息\n",
        "example_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"human\", \"{input}\"),\n",
        "        (\"ai\", \"{output}\"),\n",
        "    ]\n",
        ")\n",
        "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
        "    example_prompt=example_prompt,\n",
        "    examples=examples,\n",
        ")\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"\"\"You are an expert at world knowledge. Your task is to step back and paraphrase a question to a more generic step-back question, which is easier to answer. Here are a few examples:\"\"\",\n",
        "        ),\n",
        "        # Few shot 样本\n",
        "        few_shot_prompt,\n",
        "        # 新问题\n",
        "        (\"user\", \"{question}\"),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "5cba100d-167f-4392-8f58-88729d3e4ce9",
      "metadata": {
        "id": "5cba100d-167f-4392-8f58-88729d3e4ce9",
        "outputId": "4ac9e21f-f6e8-47e0-c546-d14188df5094",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'What security measures does LangChain implement when connecting to external services in LLM applications?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 53
        }
      ],
      "source": [
        "generate_queries_step_back = prompt | llm | StrOutputParser()\n",
        "question = \"How does LangChain ensure security when integrating external services like vector databases and API providers in LLM applications?\"\n",
        "generate_queries_step_back.invoke({\"question\": question})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "999445b0-d8a0-4208-9bb6-38610667a00b",
      "metadata": {
        "id": "999445b0-d8a0-4208-9bb6-38610667a00b",
        "outputId": "e4f29f85-f1d3-41fa-a759-c5a5ec596361",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LangChain ensures security when integrating external services like vector databases and API providers in LLM applications through a combination of best practices, internal controls, and advanced security features. These measures are designed to address the risks associated with data exposure and third-party dependencies, particularly for applications handling sensitive information.\n",
            "\n",
            "### 1. **Granular Permissions**\n",
            "LangChain enforces the principle of least privilege by allowing developers to define fine-grained permission settings. This minimizes the risk of unauthorized actions by restricting access to specific resources or operations based on user roles or application requirements. While current permissions are typically defined at deployment, future improvements could include dynamic adjustments based on real-time interactions or application states.\n",
            "\n",
            "### 2. **Sandboxing and Defense-in-Depth**\n",
            "To protect against potential vulnerabilities introduced by external integrations, LangChain employs sandboxed environments and layered security mechanisms. Sandboxing isolates components and restricts their access to system resources, reducing the attack surface. The defense-in-depth approach adds multiple layers of protection—such as network isolation, runtime monitoring, and secure coding practices—to mitigate risks from compromised external services.\n",
            "\n",
            "### 3. **Data Encryption and Secure Communication**\n",
            "Although not explicitly detailed in the provided context, the discussion around securing highly sensitive data implies the importance of encryption practices. For critical applications, LangChain can be extended to support advanced encryption standards such as:\n",
            "- **End-to-end encryption** to protect data in transit between the application and external services.\n",
            "- **Field-level encryption** or encryption at rest for data stored in vector databases or cloud storage platforms, ensuring that even if data is accessed, it remains unreadable without proper decryption keys.\n",
            "\n",
            "### 4. **Auditability and Monitoring via LangSmith**\n",
            "LangSmith plays a crucial role in maintaining security by providing comprehensive logging and monitoring capabilities. It enables developers to:\n",
            "- Track application usage patterns.\n",
            "- Detect anomalies in real time.\n",
            "- Investigate potential breaches or misuse of resources.\n",
            "\n",
            "This audit trail supports accountability and helps identify any suspicious activity stemming from external service integrations.\n",
            "\n",
            "### 5. **Vetting and Monitoring of External Providers**\n",
            "Since LangChain integrates with numerous third-party services—such as vector databases (e.g., Chroma, Milvus), API providers, and cloud platforms—it emphasizes the need for thorough vetting of these providers' security protocols. Developers are encouraged to:\n",
            "- Evaluate the provider's compliance with industry standards (e.g., GDPR, HIPAA).\n",
            "- Monitor the provider’s infrastructure for known vulnerabilities or incidents.\n",
            "- Implement fallback strategies or redundancy to minimize service disruptions caused by third-party compromises.\n",
            "\n",
            "### 6. **Proactive Security Analytics**\n",
            "The framework acknowledges the value of predictive analytics in preemptively identifying risks. By analyzing logs and user behavior, machine learning models can detect anomalous patterns that may indicate breaches, misconfigurations, or unauthorized access attempts. This proactive approach enhances overall security resilience.\n",
            "\n",
            "### Conclusion\n",
            "LangChain incorporates robust security features such as granular permissions, sandboxing, and real-time monitoring to safeguard LLM applications that integrate external services. However, given the complexity of securing applications reliant on external providers—especially in regulated industries like finance and healthcare—continued advancements in encryption, adaptive permissions, and proactive analytics are essential to address evolving threats. Developers must also remain vigilant in selecting and managing third-party integrations to ensure end-to-end security.\n"
          ]
        }
      ],
      "source": [
        "# Response prompt\n",
        "response_prompt_template = \"\"\"You are an expert of world knowledge. I am going to ask you a question. Your response should be comprehensive and not contradicted with the following context if they are relevant. Otherwise, ignore them if they are not relevant.\n",
        "\n",
        "# {normal_context}\n",
        "# {step_back_context}\n",
        "\n",
        "# Original Question: {question}\n",
        "# Answer:\"\"\"\n",
        "response_prompt = ChatPromptTemplate.from_template(response_prompt_template)\n",
        "\n",
        "chain = (\n",
        "    {\n",
        "        # 使用普通的问题检索上下文\n",
        "        \"normal_context\": RunnableLambda(lambda x: x[\"question\"]) | retriever,\n",
        "        # 使用step-back问题检索上下文\n",
        "        \"step_back_context\": generate_queries_step_back | retriever,\n",
        "        # 传递question\n",
        "        \"question\": lambda x: x[\"question\"],\n",
        "    } # 并行invoke\n",
        "    | response_prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "ans = chain.invoke({\"question\": question})\n",
        "\n",
        "print(ans.strip())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "63d0e558-4abe-42e4-a33a-2b93692f5fab",
      "metadata": {
        "id": "63d0e558-4abe-42e4-a33a-2b93692f5fab"
      },
      "source": [
        "## Part 9: HyDE\n",
        "\n",
        "![hyde](https://github.com/tivon-x/bRAG-langchain/blob/main/notebooks/image/hyde.png?raw=1)\n",
        "\n",
        "Docs:\n",
        "\n",
        "* https://github.com/langchain-ai/langchain/blob/master/cookbook/hypothetical_document_embeddings.ipynb\n",
        "\n",
        "Paper:\n",
        "\n",
        "* https://arxiv.org/abs/2212.10496"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "c2902575-bbbb-41a9-835b-9a24dc08261b",
      "metadata": {
        "id": "c2902575-bbbb-41a9-835b-9a24dc08261b",
        "outputId": "da94848b-5d84-4b4f-ed7a-01e22edcd11f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"LangChain's memory module plays a critical role in maintaining context across multi-turn conversations in chatbot applications by capturing, storing, and retrieving relevant interaction history. In conversational AI systems, where user inputs are often dependent on prior messages, preserving contextual information is essential for generating coherent and contextually accurate responses. LangChain addresses this need through its modular memory components, such as `ConversationBufferMemory`, `ConversationSummaryMemory`, and `CombinedMemory`, which enable the chatbot to retain varying levels of conversational history.\\n\\nThe `ConversationBufferMemory` stores the raw sequence of messages exchanged between the user and the chatbot, allowing the model to reference exact previous inputs and outputs. While effective for short-term context retention, this approach can become computationally expensive as the conversation lengthens. To mitigate this, `ConversationSummaryMemory` automatically generates concise summaries of past interactions, enabling the model to maintain an abstract understanding of the conversation without retaining all verbatim exchanges. Additionally, developers can implement `CombinedMemory` to leverage both raw message buffers and dynamic summaries, balancing fidelity and efficiency based on application requirements.\\n\\nBy integrating these memory modules into the chatbot’s chain structure, developers ensure that each turn of the conversation includes relevant historical context in the prompt fed to the language model. This integration allows the model to reason over past interactions dynamically, leading to more personalized and context-aware responses. As a result, LangChain's memory module significantly enhances the continuity and coherence of multi-turn dialogues in chatbot applications.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 55
        }
      ],
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "# HyDE 文档生成\n",
        "template = \"\"\"Please write a scientific paper passage to answer the question\n",
        "Question: {question}\n",
        "Passage:\"\"\"\n",
        "prompt_hyde = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "generate_docs_for_retrieval = (\n",
        "    prompt_hyde | llm | StrOutputParser()\n",
        ")\n",
        "\n",
        "# Run\n",
        "question = \"How can LangChain's memory module be utilized to maintain context across multi-turn conversations in a chatbot application?\"\n",
        "generate_docs_for_retrieval.invoke({\"question\":question})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "id": "d47587bb-23db-42a0-b087-beef9e95308b",
      "metadata": {
        "collapsed": true,
        "id": "d47587bb-23db-42a0-b087-beef9e95308b",
        "outputId": "9fc4bb97-7fda-454e-b79e-378626976aba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(id='2220bd38-5524-4e38-9664-0097fd41be52', metadata={'author': '', 'creationdate': '2024-11-06T10:08:55+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-11-06T10:08:55+00:00', 'page': 3.0, 'page_label': '4', 'producer': 'pdfTeX-1.40.26', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'source': '../langchain_turing.pdf', 'subject': '', 'title': '', 'total_pages': 14.0, 'trapped': '/False'}, page_content='4 Vasilios Mavroudis\\nMemory: Enables applications to retain information from past interactions,\\nsupporting both basic and advanced memory structures. This component is crit-\\nical for maintaining context across sessions and delivering contextually aware\\nresponses.\\nIndexes: Serve as structured databases that organize and store information,\\nallowing for efficient data retrieval when processing language queries.\\nRetrievers: Designed to work alongside indexes, retrievers fetch relevant data\\nbased on query inputs, ensuring that the generated responses are well-informed\\nand accurate.\\nVector Store: Manages the embedding of words or phrases as numerical vec-\\ntors, a core step in capturing semantic meaning and supporting tasks involving\\nlanguage understanding and similarity searches.\\nOutput Parsers: Components that refine and structure the generated language\\noutputs for specific tasks, ensuring usability and relevance for the application’s\\ngoals.\\nAgents: Custom chains that prompt the language model to identify and execute\\nthe most effective sequence of actions for a given query, enabling adaptive and\\ndynamic decision-making.\\nCallbacks:Functionsthatlog,monitor,andstreamspecificeventswithinLangChain\\nworkflows, simplifying tracking and debugging processes.\\n1.1 Chat Models and Message Handling\\nLangChain supports chat models that manage complex, multi-turn conversa-\\ntions. These models use structured message sequences, allowing developers to\\ncontrol conversation flow and maintain state over time. The structured message\\nhandling system enables robust interactions with users by storing and retrieving\\nconversation history as needed [6]. Their key features include:\\n– Multi-turn Interactions: LangChain maintains state across conversation\\nturns, making it suitable for prolonged, context-dependent conversations.\\n– Structured Output: Supports structured responses like JSON, allowing\\neasy integration with downstream applications.\\n– Conversation Memory: Maintains continuity by storing conversation his-\\ntory, ideal for applications requiring persistent context, such as customer\\nsupport [4].\\n1.2 Retrieval-Augmented Generation (RAG)\\nLangChain supports Retrieval-Augmented Generation (RAG), which integrates\\nlanguage models with external knowledge bases to enhance response accuracy'),\n",
              " Document(id='e58c9738-6d46-409e-8f9c-f29b625f6525', metadata={'author': '', 'creationdate': '2024-11-06T10:08:55+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-11-06T10:08:55+00:00', 'page': 2.0, 'page_label': '3', 'producer': 'pdfTeX-1.40.26', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'source': '../langchain_turing.pdf', 'subject': '', 'title': '', 'total_pages': 14.0, 'trapped': '/False'}, page_content='LangChain 3\\nneeds, providing a flexible foundation for building scalable, secure, and multi-\\nfunctional applications. Figure 1 illustrates a fundamental LangChain pipeline.\\nIn this architecture, diverse data sources—including documents, text, and im-\\nages—are embedded and stored within a vector store. Upon receiving a user’s\\nquery, the system retrieves the most relevant information from the vector store.\\nThis retrieved context is then provided to the large language model (LLM),\\nenhancing its ability to generate accurate and factually grounded responses.\\nFig. 1.LangChain pipeline architecture showcasing the retrieval-augmented genera-\\ntion process. Documents in various formats (e.g., PDF, text, images) are preloaded\\nand embedded into a vector store. When a user submits a query, the system retrieves\\nthe top-k most relevant documents based on vector similarity. These documents are\\ncombined with the query to provide contextual information to the language model\\n(LLM), which then generates an accurate and contextually enriched answer. This ar-\\nchitecture enhances the model’s ability to produce factually grounded responses by\\nincorporating relevant knowledge from the vector store.\\nThe rest of this section provides an overview of LangChain’s primary com-\\nponents, followed by a brief introduction to its advanced modules–LangSmith,\\nLangGraph and LangServe–which are further discussed in Sections 2, 3, and 4\\nrespectively:\\nLLM Interface: Provides APIs for connecting and querying various large lan-\\nguage models, such as OpenAI’s GPT [1], Google’s Gemini [14], and Llama [16],\\nto facilitate seamless application integration.\\nPromptTemplates:Structuredtemplatesthatstandardizeandformatqueries,\\nensuring consistency and precision in interactions with AI models. These tem-\\nplates help guide the model towards producing reliable and relevant outputs.'),\n",
              " Document(id='1f5db6db-18f6-431c-9adf-559b539f334c', metadata={'author': '', 'creationdate': '2024-11-06T10:08:55+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-11-06T10:08:55+00:00', 'page': 4.0, 'page_label': '5', 'producer': 'pdfTeX-1.40.26', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'source': '../langchain_turing.pdf', 'subject': '', 'title': '', 'total_pages': 14.0, 'trapped': '/False'}, page_content='LangChain 5\\nand relevance. RAG allows models to access up-to-date information, extending\\ntheir capabilities beyond their training data. LangChain’s RAG implementation\\nuses:\\n– Document Loaders and Text Splitters: Preprocess documents for in-\\ndexing and efficient retrieval [6].\\n– Embedding Models and Vector Stores: Enable similarity-based re-\\ntrieval by embedding documents into vector spaces. LangChain integrates\\nwithvectorstoragesolutionslikeChromaandMilvusforoptimizedsearches[3].\\n– Retrievers and RAG Chains: Retrieve and merge external data with\\nmodel responses, enhancing applications such as question answering systems\\nand recommendation engines [4].\\n1.3 Security and Permissions Management\\nSecurity is a critical focus in LangChain’s design, particularly given the potential\\naccess to external data sources. LangChain addresses these security challenges\\nthrough best practices and internal controls [3]:\\n– Granular Permissions: Enforces the principle of least privilege by allowing\\ndevelopers to specify limited permissions, minimizing the risk of unautho-\\nrized actions.\\n– Sandboxing and Defense in Depth: Utilizes sandboxed environments\\nand layered security to protect sensitive data and limit exposure to vulner-\\nabilities [3].\\n– Auditability and Monitoring: LangSmith (see Section 2) provides de-\\ntailed logging and monitoring capabilities, enabling developers to track ap-\\nplication usage and detect anomalies in real time.\\n1.4 Integrations and Extensibility\\nLangChain’s architecture supports a wide range of third-party integrations, al-\\nlowing for custom component development and additional functionality, such as\\nmulti-modal data processing and AI tool integration [3]:\\n– IntegrationPackages:LangChainprovidesdedicatedpackages(e.g.,langchain-\\nopenai, langchain-aws) that simplify connections to external platforms, tai-\\nloring applications to specific needs.\\n– Support for Multi-modal Data: Supports image, text, and audio inputs,\\nallowing for applications like chatbots capable of interpreting diverse data\\ntypes.\\n– CustomComponentDevelopment :Developerscanbuildcustomplugins\\nor extend LangChain components, ensuring flexibility and adaptability for\\na wide range of application requirements.\\nLangChain’s modular and flexible architecture equips developers with a com-\\nprehensive toolkit for building, deploying, and monitoring LLM applications. Its\\nadvanced components—LangGraph, LangServe, and LangSmith—enable sophis-\\nticated functionality for scalable, interactive, and robust applications, meeting\\nthe demands of modern AI use cases.'),\n",
              " Document(id='7b4659b1-fbf7-4574-b131-e7390e2ceca0', metadata={'author': '', 'creationdate': '2024-11-06T10:08:55+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-11-06T10:08:55+00:00', 'page': 1.0, 'page_label': '2', 'producer': 'pdfTeX-1.40.26', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'source': '../langchain_turing.pdf', 'subject': '', 'title': '', 'total_pages': 14.0, 'trapped': '/False'}, page_content='2 Vasilios Mavroudis\\nstateful, and contextually aware applications with ease. Its suite of compo-\\nnents—including LangGraph for stateful process modeling, LangServe for scal-\\nableAPIdeployment,andLangSmithformonitoringandevaluation—collectively\\nform a comprehensive toolkit for leveraging LLMs effectively [3].\\nLangChain facilitates the integration of LLMs into a wide array of applica-\\ntions, empowering developers to create solutions that are not only functional\\nbut also efficient and secure. Its support for features like chat models, retrieval-\\naugmented generation (RAG) [10], and secure API interactions allows for the\\nrapid deployment of sophisticated language model solutions across diverse do-\\nmains such as healthcare, customer service, finance, and mental health.\\nDespite its strengths, LangChain’s emphasis on flexibility through modular-\\nity introduces certain complexities. Developers may encounter a steep learning\\ncurve when navigating its extensive components and integrations. Moreover, the\\nreliance on external integrations and third-party providers necessitates a careful\\nexamination of security practices to mitigate risks associated with data exposure\\nand dependency vulnerabilities.\\nThis paper provides a comprehensive analysis of LangChain, delving into its\\narchitecture, core components, and the interplay between its modules. We ex-\\nplore how LangChain facilitates the development of LLM applications by exam-\\nining each component’s functionality and their synergistic contributions to the\\nframework. Furthermore, we critically evaluate the limitations and criticisms of\\nLangChain, focusing on the complexities introduced by its modular design and\\nthe security implications of its extensive integrations.\\nBy offering valuable insights into both the capabilities and challenges of\\nLangChain, this paper aims to serve as a key resource for developers and re-\\nsearchers interested in LLM application development. We seek to illuminate\\nthe transformative potential of LangChain in advancing NLP applications while\\nproviding a nuanced understanding of its practical boundaries. Ultimately, this\\nanalysis guides users in effectively harnessing LangChain to build innovative and\\nsecure LLM-powered applications tailored to their specific needs.\\nThe remainder of this paper is organized as follows: Section 1 delves into\\nthe core architecture of LangChain, detailing its primary components and their\\nfunctionalities. Section 2 examines LangSmith and its role in monitoring and\\nevaluation of LLM applications. In Section 3, we explore LangGraph’s capabili-\\nties in stateful process modeling. Section 4 discusses LangServe for scalable API\\ndeployment of LangChain applications. Finally, section 5 addresses the limita-\\ntions and criticisms of LangChain, particularly focusing on the complexities and\\nsecurity concerns associated with its modular design and external integrations.\\n1 Architecture\\nLangChain is built with a modular architecture, designed to simplify the life-\\ncycle of applications powered by large language models (LLMs), from initial\\ndevelopment through to deployment and monitoring [3]. This modularity al-\\nlows developers to configure, extend, and deploy applications tailored to specific')]"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ],
      "source": [
        "# Retrieve\n",
        "retrieval_chain = generate_docs_for_retrieval | retriever\n",
        "retireved_docs = retrieval_chain.invoke({\"question\":question})\n",
        "retireved_docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "id": "604fcc36-a1d7-4096-99b5-50db30950fc5",
      "metadata": {
        "id": "604fcc36-a1d7-4096-99b5-50db30950fc5",
        "outputId": "e4ec8a72-3dcf-4dae-9007-1ff999cf4142",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'LangChain\\'s **memory module** can be utilized to maintain context across multi-turn conversations in a chatbot application by storing and retrieving conversation history as needed. Specifically, the memory component enables the application to retain information from past interactions, which is critical for preserving context over time.\\n\\nFor a chatbot, this means:\\n\\n1. **Maintaining State**: The memory module keeps track of prior messages exchanged between the user and the chatbot. This allows the chatbot to reference previous interactions when generating responses, ensuring continuity during prolonged or complex conversations.\\n\\n2. **Contextual Awareness**: By leveraging stored conversation history, the chatbot can deliver responses that are contextually aware and relevant to the ongoing dialogue, rather than treating each query independently.\\n\\n3. **Structured Message Handling**: LangChain’s chat models use structured message sequences (e.g., roles like \"user\" and \"assistant\") that work with the memory module to manage conversation flow effectively.\\n\\nIn summary, the memory module supports multi-turn conversations by retaining historical context and enabling the chatbot to interact more naturally and intelligently throughout the dialogue.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 57
        }
      ],
      "source": [
        "# RAG\n",
        "template = \"\"\"Answer the following question based on this context:\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "final_rag_chain = (\n",
        "    prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "final_rag_chain.invoke({\"context\":retireved_docs,\"question\":question})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c60db3d8",
      "metadata": {
        "id": "c60db3d8"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "本笔记本全面介绍了检索增强生成（RAG）技术，特别强调了multi-query架构及其实际应用。\n",
        "\n",
        "- 环境设置：提供了创建虚拟环境和安装必要软件包的详细说明。\n",
        "- 数据加载和索引：讨论了使用Pinecone有效加载文档及其后续索引的方法。\n",
        "- multi-query RAG：探索了生成用户查询多个视角的技术，以提高检索准确性。\n",
        "- RAG融合和分解：提出了用于集成和分解查询的高级方法，以提高响应质量。\n",
        "- 实际实现：包括使用LangChain和qwen模型实现这些技术的具体示例。\n",
        "\n",
        "总之，本笔记本是跨各种应用程序实施RAG方法的宝贵资源，强调了上下文和查询多样性在信息检索任务领域的重要性。"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WGEJ32Jc4RyS"
      },
      "id": "WGEJ32Jc4RyS",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}